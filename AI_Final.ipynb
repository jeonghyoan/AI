{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "collapsed_sections": [
        "eEqvcYcYh7aY",
        "Rx42YpFW2MO9",
        "-2QP3J_JFjFO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setting"
      ],
      "metadata": {
        "id": "eEqvcYcYh7aY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqQg6Z14qmjb",
        "outputId": "7c755d6d-0ef6-4188-80d4-782de7edb3f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "root = '/content/drive/MyDrive/ITM_AI/CD-DETR'\n",
        "os.chdir(root)"
      ],
      "metadata": {
        "id": "YyrBkKxTuhu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod -R 777 .\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "dIgj8OtnAc07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9243732c-4df7-4c77-f500-6341885949e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (3.0.6)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.0.7)\n",
            "Collecting submitit (from -r requirements.txt (line 3))\n",
            "  Downloading submitit-1.5.1-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.7/74.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.16.0+cu118)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.11.4)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (2.4.0)\n",
            "Collecting addict (from -r requirements.txt (line 8))\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting yapf (from -r requirements.txt (line 9))\n",
            "  Downloading yapf-0.40.2-py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.7/254.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm (from -r requirements.txt (line 10))\n",
            "  Downloading timm-0.9.12-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools->-r requirements.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools->-r requirements.txt (line 2)) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from submitit->-r requirements.txt (line 3)) (2.2.1)\n",
            "Requirement already satisfied: typing_extensions>=3.7.4.2 in /usr/local/lib/python3.10/dist-packages (from submitit->-r requirements.txt (line 3)) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.6.0->-r requirements.txt (line 5)) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.6.0->-r requirements.txt (line 5)) (9.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->-r requirements.txt (line 9)) (7.0.0)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->-r requirements.txt (line 9)) (4.1.0)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->-r requirements.txt (line 9)) (2.0.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 10)) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 10)) (0.19.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 10)) (0.4.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->-r requirements.txt (line 9)) (3.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm->-r requirements.txt (line 10)) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.5.0->-r requirements.txt (line 4)) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.6.0->-r requirements.txt (line 5)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.6.0->-r requirements.txt (line 5)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.6.0->-r requirements.txt (line 5)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.6.0->-r requirements.txt (line 5)) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.5.0->-r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (1.16.0)\n",
            "Installing collected packages: addict, submitit, yapf, timm\n",
            "Successfully installed addict-2.4.0 submitit-1.5.1 timm-0.9.12 yapf-0.40.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Motivation"
      ],
      "metadata": {
        "id": "p5YNn3rP16LJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=148hxhxhibMlL_YW56t35DrmPFOwjYIQn' height=400>\n",
        "\n",
        "### **Project Motivation**\n",
        "* Overcome deep learning's inability to adapt to new, emerging classes not\n",
        "present in the initial dataset.\n",
        "* Enhance real-world utility as data constantly evolves.\n",
        "\n",
        "### **Problem Statement**\n",
        "- Implementing class incremental learning in object detection, a vital part of computer vision, presents unique challenges.\n",
        "- Complexity of identifying multiple classes within a single image in object detection.\n",
        "\n",
        "\n",
        "### **Project Aim**\n",
        "- Introduce a buffer training strategy optimized for object detection tasks.\n",
        "- Improve replay methods to make incremental learning more suitable for real-world applicability.\n"
      ],
      "metadata": {
        "id": "lt17eHI92Jvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A description of the data"
      ],
      "metadata": {
        "id": "murUbGvv1tLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## COCO Dataset (Common Object in Context)\n"
      ],
      "metadata": {
        "id": "Rx42YpFW2MO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We choose the MS COCO 2017 (Microsoft Common Objects in Context) dataset\n",
        "- One of the benchmark in Object Detection\n",
        "   \n",
        "**Why COCO 2017?**\n",
        "- Scale and diversity of categories reflect the complexity of real-world data\n",
        "- Uneven distribution of images across classes mirrors real-world variety and imbalance.\n",
        "   \n",
        "**Dataset Details:**\n",
        "- Consists of over 200,000 images from various everyday scenes\n",
        "- Annotated with object bounding boxes and class labels across 80 diverse object categories"
      ],
      "metadata": {
        "id": "RTUade7VKClR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Details\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1Serx-VakSQVrPEdmddeltIUmKjLQudEs' height=300 >\n",
        "\n",
        "- Quite unbalanced class distribution\n",
        "- This is normal in the context of Object Detection Task!"
      ],
      "metadata": {
        "id": "QmA0b9vK2RN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1bBf8fwdPev2Cl3vizpXmjFeR_FqunWiu' height=300 >\n",
        "\n",
        "- Dataset Sample Image\n",
        "- Configured to 'Common', life-related Objects"
      ],
      "metadata": {
        "id": "X1fwbT8j2S_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1CdTIuloNzSY1J_1cbm8my8n6jFDP2Izm' height = 300>\n",
        "\n",
        "- Annotation format of Object Detection\n",
        "- Object Detection needs class(category) id & bbox coordinations *BOTH*!\n"
      ],
      "metadata": {
        "id": "hHrohgr-2UwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used coco.sh to download the overall dataset in local!\n",
        "> Too large dataset size (~=30GB), We downloaded into local and moved to Google Drive.\n",
        "\n",
        "\n",
        "```\n",
        "mkdir COCODIR\n",
        "cd COCODIR\n",
        "\n",
        "wget -c http://images.cocodataset.org/zips/train2017.zip\n",
        "echo \"Extracting train2017.zip\"\n",
        "unzip -qq train2017.zip\n",
        "rm train2017.zip\n",
        "\n",
        "wget -c http://images.cocodataset.org/zips/val2017.zip\n",
        "...(similar to upon)...\n",
        "\n",
        "wget -c http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "...(similar to upon)...\n",
        "\n",
        "cd annotations\n",
        "find . -type f \\\n",
        "    -not -wholename ./instances_train2017.json \\\n",
        "    -not -wholename ./instances_val2017.json \\\n",
        "    -delete\n",
        "\n",
        "echo \"DONE.\"\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "bPIIRrcoI4uh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# COCO Dataset - benchmark of Object Detection\n",
        "!bash /content/drive/MyDrive/ITM_AI/CD-DETR/coco.sh"
      ],
      "metadata": {
        "id": "flJU1Mb5utIK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6d3efe9-f028-4bcb-8113-6df5c0e140b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘COCODIR’: File exists\n",
            "--2023-12-10 14:20:56--  http://images.cocodataset.org/zips/train2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.12.36, 3.5.28.199, 52.216.221.129, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.12.36|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19336861798 (18G) [application/zip]\n",
            "Saving to: ‘train2017.zip’\n",
            "\n",
            "train2017.zip       100%[===================>]  18.01G  6.59MB/s    in 24m 54s \n",
            "\n",
            "2023-12-10 14:45:50 (12.3 MB/s) - ‘train2017.zip’ saved [19336861798/19336861798]\n",
            "\n",
            "Extracting train2017.zip\n",
            "replace train2017/000000147328.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter and architecture choices that were explored"
      ],
      "metadata": {
        "id": "douD_Goj2aka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selected Baseline Architecture - Deformable DETR\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1PRx1SB55yMnqNmE6GF7BTfFmuyWTJvY1' height=300 >\n",
        "\n",
        "- **Object Detector exploiting *Transformer***\n",
        "  - Previous object detection architecture was usually established on CNN\n",
        "\n",
        "<br/>\n",
        "\n",
        "- **Deformable Attetion**\n",
        "  - Enhancing spatial awareness and improve localization accuracy\n",
        "  - particularly for deformed or irregular object shapes\n",
        "\n",
        "<br/>\n",
        "\n",
        "- DETR(DEtection TRansformer) => Increase **Stability** and **Peformance**!\n",
        "- We selected this due to its *clear structure and stability*."
      ],
      "metadata": {
        "id": "fTZUdWiW3xGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Replay Management - for Incremental Object Detection\n",
        "\n",
        "<br/>\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1mskrvJgBJw4duszykDBgl9tEcJBFRAOc' height=200>\n",
        "\n",
        "**ICaRL**(Rebuffi et al. 2017)\n",
        "- Presented Replay method First\n",
        "- Replay: collect some *sample* of respective class in the *buffer*\n",
        "- Reuse the samples in the buffer when new dataset is trained\n",
        "- Exploited mean-of-feature to collect the Replay Buffer\n",
        "- **Targeted to Classification!**\n",
        "\n",
        "<br/>\n",
        "\n",
        "Then, in Object Detection?\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1d_DwVJAtnc5_6lWeY5XIZREKl_uFQuJc' height=300 >\n",
        "\n",
        "- Newly emerging classes(dataset)\n",
        "- Multiple classes in one image => main difference between classification\n",
        "- How to collect the buffer is main issue.\n"
      ],
      "metadata": {
        "id": "NC7FZRyq39rg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Effective & handling Class imbalance in Buffer Management Strategy\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1nfc66uhmi-bayybVOPxp5Xi7MQvJOCHi' height=300 >\n",
        "\n",
        "To tailoring the Replay method into Object Detection domain, We establisehd *two* criteria.\n",
        "- **Number of unique labels** in a scene\n",
        "- **Train-loss based strategy**; more efficient retrieving\n",
        "\n",
        "Also, Consider the possibility that sparse classes will not be collected in the buffer\n",
        "- **GM(Guarantee Mimimum)**\n",
        "  - Determine the minimum number of images that should be included per class\n"
      ],
      "metadata": {
        "id": "3F2PsIn-QRvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploting Buffer in max!\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1wJMktMN4WgVspSJ3Qhd2CvXRSJ3MPJFp' height=300 >\n",
        "\n",
        "Buffer is relatevely too small than newly emerging dataset, So we devised how to fully use it.\n",
        "\n",
        "- **CER (Circular Experience Replay) Training Strategy**\n",
        "  - Repeatedly train buffer image into new training session.\n",
        "  > We combines Experience Replay (ER) training with circular training, where older samples are replaced over time to avoid fixation on outdated information and overfitting."
      ],
      "metadata": {
        "id": "tgIeZNXFWqOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selected Hyperparameters\n",
        "\n",
        "1. **Task**.   \n",
        "Current dataset + How many newly emerging dataset?\n",
        "  - We selected 2\n",
        "  - Simplest setting\n",
        "\n",
        "2. **Guarantee Minimum.**  \n",
        "Restricting number of images in buffer\n",
        "  - limit image 1200\n",
        "  - least image 12 (1%)\n",
        "\n",
        "3. **Learning rate scheduler.**  \n",
        "For effective learning.  \n",
        "  - Step LR\n",
        "  - lr:2e-4\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1UVa2FmtM_CPU6Tk_-Vdyf-O9DNNNPLIO' height=200 >"
      ],
      "metadata": {
        "id": "AOogaypYTp-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n"
      ],
      "metadata": {
        "id": "G2zbGuJS4D0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BTW, We runned our code in owned server due to memory issue & assining GPU problem in GCP ...🤯"
      ],
      "metadata": {
        "id": "MhP3uKssYbaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We compared our results with\n",
        "\n",
        "* Upper Bound\n",
        "  - maximum potential performance achieved by joint training across all classes.\n",
        "* Under Bound\n",
        "  - Without the any incremental method, which experiences a significant forgetting.\n",
        "* Mosaic\n",
        "  - Random buffer sampling + Buffer Expansion through Mosaic Augmentation.  \n",
        "  <img src='https://drive.google.com/uc?id=1LrPuImSoUpo1hyKLCr6SDKqSGIMJ-31q' height=200 >\n",
        "  - Large buffer capacity more than 10%.\n",
        "\n",
        "* Ours\n",
        "  -  Small buffer limit but still performs well\n",
        "  - 1% Buffer in regards to COCO Dataset (1200 images)\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1HPF7oEBaF75mrYGVCZh7G3oVkeKzh6ps' >\n",
        "\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "### Ours with Ablation Study\n",
        "  - Compare performance by adjusting the number of circular replay and normal replay epochs\n",
        "    - (CER/ER)\n",
        "    - 48/2\n",
        "    - 47/3\n",
        "    - 45/5\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1gY8jIjvWYKDcAIqhQXpPlRnUdllEP9zv' height=300>"
      ],
      "metadata": {
        "id": "fPNJv7zoYw_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our performance + Abiliation"
      ],
      "metadata": {
        "id": "0jsXnhDDF72w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1_nla0DsWRLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py \\\n",
        "    --output_dir exps/deform \\\n",
        "    --coco_path ../COCODIR \\\n",
        "    --batch_size 12 \\\n",
        "    --resume ./pth/cd-detr-v3.pth \\\n",
        "    --with_box_refine \\\n",
        "    --eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JigoFEXddekG",
        "outputId": "4bbc7cca-31b2-444b-a379-35c469a271a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "git:\n",
            "  sha: 11169a60c33333af00a4849f1808023eba96a931, status: has uncommited changes, branch: main\n",
            "\n",
            "Namespace(lr=0.0002, lr_backbone_names=['backbone.0'], lr_backbone=2e-05, lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, batch_size=12, weight_decay=0.0001, epochs=50, lr_drop=40, lr_drop_epochs=None, clip_max_norm=0.1, sgd=False, with_box_refine=True, two_stage=False, frozen_weights=None, backbone='resnet50', dilation=False, position_embedding='sine', position_embedding_scale=6.283185307179586, num_feature_levels=4, enc_layers=6, dec_layers=6, dim_feedforward=1024, hidden_dim=256, dropout=0.1, nheads=8, num_queries=300, dec_n_points=4, enc_n_points=4, masks=False, aux_loss=True, set_cost_class=2, set_cost_bbox=5, set_cost_giou=2, mask_loss_coef=1, dice_loss_coef=1, cls_loss_coef=2, bbox_loss_coef=5, giou_loss_coef=2, focal_alpha=0.25, dataset_file='coco', coco_path='../COCODIR', coco_panoptic_path=None, remove_difficult=False, output_dir='exps/deform', device='cuda', seed=42, resume='./pth/cd-detr-v3.pth', start_epoch=0, eval=True, num_workers=2, cache_mode=False, distributed=False)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "number of params: 40627260\n",
            "loading annotations into memory...\n",
            "Done (t=14.75s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=1.88s)\n",
            "creating index...\n",
            "index created!\n",
            "transformer.level_embed\n",
            "transformer.encoder.layers.0.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.0.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.0.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.0.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.0.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.0.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.0.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.0.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.0.norm1.weight\n",
            "transformer.encoder.layers.0.norm1.bias\n",
            "transformer.encoder.layers.0.linear1.weight\n",
            "transformer.encoder.layers.0.linear1.bias\n",
            "transformer.encoder.layers.0.linear2.weight\n",
            "transformer.encoder.layers.0.linear2.bias\n",
            "transformer.encoder.layers.0.norm2.weight\n",
            "transformer.encoder.layers.0.norm2.bias\n",
            "transformer.encoder.layers.1.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.1.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.1.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.1.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.1.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.1.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.1.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.1.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.1.norm1.weight\n",
            "transformer.encoder.layers.1.norm1.bias\n",
            "transformer.encoder.layers.1.linear1.weight\n",
            "transformer.encoder.layers.1.linear1.bias\n",
            "transformer.encoder.layers.1.linear2.weight\n",
            "transformer.encoder.layers.1.linear2.bias\n",
            "transformer.encoder.layers.1.norm2.weight\n",
            "transformer.encoder.layers.1.norm2.bias\n",
            "transformer.encoder.layers.2.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.2.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.2.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.2.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.2.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.2.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.2.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.2.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.2.norm1.weight\n",
            "transformer.encoder.layers.2.norm1.bias\n",
            "transformer.encoder.layers.2.linear1.weight\n",
            "transformer.encoder.layers.2.linear1.bias\n",
            "transformer.encoder.layers.2.linear2.weight\n",
            "transformer.encoder.layers.2.linear2.bias\n",
            "transformer.encoder.layers.2.norm2.weight\n",
            "transformer.encoder.layers.2.norm2.bias\n",
            "transformer.encoder.layers.3.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.3.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.3.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.3.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.3.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.3.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.3.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.3.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.3.norm1.weight\n",
            "transformer.encoder.layers.3.norm1.bias\n",
            "transformer.encoder.layers.3.linear1.weight\n",
            "transformer.encoder.layers.3.linear1.bias\n",
            "transformer.encoder.layers.3.linear2.weight\n",
            "transformer.encoder.layers.3.linear2.bias\n",
            "transformer.encoder.layers.3.norm2.weight\n",
            "transformer.encoder.layers.3.norm2.bias\n",
            "transformer.encoder.layers.4.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.4.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.4.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.4.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.4.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.4.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.4.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.4.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.4.norm1.weight\n",
            "transformer.encoder.layers.4.norm1.bias\n",
            "transformer.encoder.layers.4.linear1.weight\n",
            "transformer.encoder.layers.4.linear1.bias\n",
            "transformer.encoder.layers.4.linear2.weight\n",
            "transformer.encoder.layers.4.linear2.bias\n",
            "transformer.encoder.layers.4.norm2.weight\n",
            "transformer.encoder.layers.4.norm2.bias\n",
            "transformer.encoder.layers.5.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.5.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.5.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.5.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.5.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.5.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.5.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.5.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.5.norm1.weight\n",
            "transformer.encoder.layers.5.norm1.bias\n",
            "transformer.encoder.layers.5.linear1.weight\n",
            "transformer.encoder.layers.5.linear1.bias\n",
            "transformer.encoder.layers.5.linear2.weight\n",
            "transformer.encoder.layers.5.linear2.bias\n",
            "transformer.encoder.layers.5.norm2.weight\n",
            "transformer.encoder.layers.5.norm2.bias\n",
            "transformer.decoder.layers.0.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.0.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.0.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.0.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.0.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.0.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.0.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.0.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.0.norm1.weight\n",
            "transformer.decoder.layers.0.norm1.bias\n",
            "transformer.decoder.layers.0.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.0.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.0.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.0.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.0.norm2.weight\n",
            "transformer.decoder.layers.0.norm2.bias\n",
            "transformer.decoder.layers.0.linear1.weight\n",
            "transformer.decoder.layers.0.linear1.bias\n",
            "transformer.decoder.layers.0.linear2.weight\n",
            "transformer.decoder.layers.0.linear2.bias\n",
            "transformer.decoder.layers.0.norm3.weight\n",
            "transformer.decoder.layers.0.norm3.bias\n",
            "transformer.decoder.layers.1.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.1.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.1.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.1.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.1.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.1.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.1.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.1.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.1.norm1.weight\n",
            "transformer.decoder.layers.1.norm1.bias\n",
            "transformer.decoder.layers.1.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.1.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.1.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.1.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.1.norm2.weight\n",
            "transformer.decoder.layers.1.norm2.bias\n",
            "transformer.decoder.layers.1.linear1.weight\n",
            "transformer.decoder.layers.1.linear1.bias\n",
            "transformer.decoder.layers.1.linear2.weight\n",
            "transformer.decoder.layers.1.linear2.bias\n",
            "transformer.decoder.layers.1.norm3.weight\n",
            "transformer.decoder.layers.1.norm3.bias\n",
            "transformer.decoder.layers.2.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.2.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.2.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.2.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.2.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.2.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.2.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.2.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.2.norm1.weight\n",
            "transformer.decoder.layers.2.norm1.bias\n",
            "transformer.decoder.layers.2.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.2.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.2.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.2.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.2.norm2.weight\n",
            "transformer.decoder.layers.2.norm2.bias\n",
            "transformer.decoder.layers.2.linear1.weight\n",
            "transformer.decoder.layers.2.linear1.bias\n",
            "transformer.decoder.layers.2.linear2.weight\n",
            "transformer.decoder.layers.2.linear2.bias\n",
            "transformer.decoder.layers.2.norm3.weight\n",
            "transformer.decoder.layers.2.norm3.bias\n",
            "transformer.decoder.layers.3.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.3.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.3.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.3.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.3.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.3.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.3.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.3.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.3.norm1.weight\n",
            "transformer.decoder.layers.3.norm1.bias\n",
            "transformer.decoder.layers.3.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.3.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.3.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.3.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.3.norm2.weight\n",
            "transformer.decoder.layers.3.norm2.bias\n",
            "transformer.decoder.layers.3.linear1.weight\n",
            "transformer.decoder.layers.3.linear1.bias\n",
            "transformer.decoder.layers.3.linear2.weight\n",
            "transformer.decoder.layers.3.linear2.bias\n",
            "transformer.decoder.layers.3.norm3.weight\n",
            "transformer.decoder.layers.3.norm3.bias\n",
            "transformer.decoder.layers.4.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.4.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.4.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.4.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.4.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.4.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.4.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.4.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.4.norm1.weight\n",
            "transformer.decoder.layers.4.norm1.bias\n",
            "transformer.decoder.layers.4.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.4.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.4.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.4.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.4.norm2.weight\n",
            "transformer.decoder.layers.4.norm2.bias\n",
            "transformer.decoder.layers.4.linear1.weight\n",
            "transformer.decoder.layers.4.linear1.bias\n",
            "transformer.decoder.layers.4.linear2.weight\n",
            "transformer.decoder.layers.4.linear2.bias\n",
            "transformer.decoder.layers.4.norm3.weight\n",
            "transformer.decoder.layers.4.norm3.bias\n",
            "transformer.decoder.layers.5.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.5.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.5.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.5.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.5.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.5.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.5.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.5.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.5.norm1.weight\n",
            "transformer.decoder.layers.5.norm1.bias\n",
            "transformer.decoder.layers.5.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.5.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.5.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.5.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.5.norm2.weight\n",
            "transformer.decoder.layers.5.norm2.bias\n",
            "transformer.decoder.layers.5.linear1.weight\n",
            "transformer.decoder.layers.5.linear1.bias\n",
            "transformer.decoder.layers.5.linear2.weight\n",
            "transformer.decoder.layers.5.linear2.bias\n",
            "transformer.decoder.layers.5.norm3.weight\n",
            "transformer.decoder.layers.5.norm3.bias\n",
            "transformer.decoder.bbox_embed.0.layers.0.weight\n",
            "transformer.decoder.bbox_embed.0.layers.0.bias\n",
            "transformer.decoder.bbox_embed.0.layers.1.weight\n",
            "transformer.decoder.bbox_embed.0.layers.1.bias\n",
            "transformer.decoder.bbox_embed.0.layers.2.weight\n",
            "transformer.decoder.bbox_embed.0.layers.2.bias\n",
            "transformer.decoder.bbox_embed.1.layers.0.weight\n",
            "transformer.decoder.bbox_embed.1.layers.0.bias\n",
            "transformer.decoder.bbox_embed.1.layers.1.weight\n",
            "transformer.decoder.bbox_embed.1.layers.1.bias\n",
            "transformer.decoder.bbox_embed.1.layers.2.weight\n",
            "transformer.decoder.bbox_embed.1.layers.2.bias\n",
            "transformer.decoder.bbox_embed.2.layers.0.weight\n",
            "transformer.decoder.bbox_embed.2.layers.0.bias\n",
            "transformer.decoder.bbox_embed.2.layers.1.weight\n",
            "transformer.decoder.bbox_embed.2.layers.1.bias\n",
            "transformer.decoder.bbox_embed.2.layers.2.weight\n",
            "transformer.decoder.bbox_embed.2.layers.2.bias\n",
            "transformer.decoder.bbox_embed.3.layers.0.weight\n",
            "transformer.decoder.bbox_embed.3.layers.0.bias\n",
            "transformer.decoder.bbox_embed.3.layers.1.weight\n",
            "transformer.decoder.bbox_embed.3.layers.1.bias\n",
            "transformer.decoder.bbox_embed.3.layers.2.weight\n",
            "transformer.decoder.bbox_embed.3.layers.2.bias\n",
            "transformer.decoder.bbox_embed.4.layers.0.weight\n",
            "transformer.decoder.bbox_embed.4.layers.0.bias\n",
            "transformer.decoder.bbox_embed.4.layers.1.weight\n",
            "transformer.decoder.bbox_embed.4.layers.1.bias\n",
            "transformer.decoder.bbox_embed.4.layers.2.weight\n",
            "transformer.decoder.bbox_embed.4.layers.2.bias\n",
            "transformer.decoder.bbox_embed.5.layers.0.weight\n",
            "transformer.decoder.bbox_embed.5.layers.0.bias\n",
            "transformer.decoder.bbox_embed.5.layers.1.weight\n",
            "transformer.decoder.bbox_embed.5.layers.1.bias\n",
            "transformer.decoder.bbox_embed.5.layers.2.weight\n",
            "transformer.decoder.bbox_embed.5.layers.2.bias\n",
            "transformer.reference_points.weight\n",
            "transformer.reference_points.bias\n",
            "class_embed.0.weight\n",
            "class_embed.0.bias\n",
            "class_embed.1.weight\n",
            "class_embed.1.bias\n",
            "class_embed.2.weight\n",
            "class_embed.2.bias\n",
            "class_embed.3.weight\n",
            "class_embed.3.bias\n",
            "class_embed.4.weight\n",
            "class_embed.4.bias\n",
            "class_embed.5.weight\n",
            "class_embed.5.bias\n",
            "query_embed.weight\n",
            "input_proj.0.0.weight\n",
            "input_proj.0.0.bias\n",
            "input_proj.0.1.weight\n",
            "input_proj.0.1.bias\n",
            "input_proj.1.0.weight\n",
            "input_proj.1.0.bias\n",
            "input_proj.1.1.weight\n",
            "input_proj.1.1.bias\n",
            "input_proj.2.0.weight\n",
            "input_proj.2.0.bias\n",
            "input_proj.2.1.weight\n",
            "input_proj.2.1.bias\n",
            "input_proj.3.0.weight\n",
            "input_proj.3.0.bias\n",
            "input_proj.3.1.weight\n",
            "input_proj.3.1.bias\n",
            "backbone.0.body.conv1.weight\n",
            "backbone.0.body.layer1.0.conv1.weight\n",
            "backbone.0.body.layer1.0.conv2.weight\n",
            "backbone.0.body.layer1.0.conv3.weight\n",
            "backbone.0.body.layer1.0.downsample.0.weight\n",
            "backbone.0.body.layer1.1.conv1.weight\n",
            "backbone.0.body.layer1.1.conv2.weight\n",
            "backbone.0.body.layer1.1.conv3.weight\n",
            "backbone.0.body.layer1.2.conv1.weight\n",
            "backbone.0.body.layer1.2.conv2.weight\n",
            "backbone.0.body.layer1.2.conv3.weight\n",
            "backbone.0.body.layer2.0.conv1.weight\n",
            "backbone.0.body.layer2.0.conv2.weight\n",
            "backbone.0.body.layer2.0.conv3.weight\n",
            "backbone.0.body.layer2.0.downsample.0.weight\n",
            "backbone.0.body.layer2.1.conv1.weight\n",
            "backbone.0.body.layer2.1.conv2.weight\n",
            "backbone.0.body.layer2.1.conv3.weight\n",
            "backbone.0.body.layer2.2.conv1.weight\n",
            "backbone.0.body.layer2.2.conv2.weight\n",
            "backbone.0.body.layer2.2.conv3.weight\n",
            "backbone.0.body.layer2.3.conv1.weight\n",
            "backbone.0.body.layer2.3.conv2.weight\n",
            "backbone.0.body.layer2.3.conv3.weight\n",
            "backbone.0.body.layer3.0.conv1.weight\n",
            "backbone.0.body.layer3.0.conv2.weight\n",
            "backbone.0.body.layer3.0.conv3.weight\n",
            "backbone.0.body.layer3.0.downsample.0.weight\n",
            "backbone.0.body.layer3.1.conv1.weight\n",
            "backbone.0.body.layer3.1.conv2.weight\n",
            "backbone.0.body.layer3.1.conv3.weight\n",
            "backbone.0.body.layer3.2.conv1.weight\n",
            "backbone.0.body.layer3.2.conv2.weight\n",
            "backbone.0.body.layer3.2.conv3.weight\n",
            "backbone.0.body.layer3.3.conv1.weight\n",
            "backbone.0.body.layer3.3.conv2.weight\n",
            "backbone.0.body.layer3.3.conv3.weight\n",
            "backbone.0.body.layer3.4.conv1.weight\n",
            "backbone.0.body.layer3.4.conv2.weight\n",
            "backbone.0.body.layer3.4.conv3.weight\n",
            "backbone.0.body.layer3.5.conv1.weight\n",
            "backbone.0.body.layer3.5.conv2.weight\n",
            "backbone.0.body.layer3.5.conv3.weight\n",
            "backbone.0.body.layer4.0.conv1.weight\n",
            "backbone.0.body.layer4.0.conv2.weight\n",
            "backbone.0.body.layer4.0.conv3.weight\n",
            "backbone.0.body.layer4.0.downsample.0.weight\n",
            "backbone.0.body.layer4.1.conv1.weight\n",
            "backbone.0.body.layer4.1.conv2.weight\n",
            "backbone.0.body.layer4.1.conv3.weight\n",
            "backbone.0.body.layer4.2.conv1.weight\n",
            "backbone.0.body.layer4.2.conv2.weight\n",
            "backbone.0.body.layer4.2.conv3.weight\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Test:  [  0/417]  eta: 0:28:28  class_error: 33.68  loss: 14.3505 (14.3505)  loss_ce: 1.1280 (1.1280)  loss_bbox: 0.3758 (0.3758)  loss_giou: 0.8380 (0.8380)  loss_ce_0: 1.0862 (1.0862)  loss_bbox_0: 0.4809 (0.4809)  loss_giou_0: 0.9660 (0.9660)  loss_ce_1: 1.0976 (1.0976)  loss_bbox_1: 0.4062 (0.4062)  loss_giou_1: 0.8766 (0.8766)  loss_ce_2: 1.1329 (1.1329)  loss_bbox_2: 0.3756 (0.3756)  loss_giou_2: 0.8494 (0.8494)  loss_ce_3: 1.1422 (1.1422)  loss_bbox_3: 0.3764 (0.3764)  loss_giou_3: 0.8421 (0.8421)  loss_ce_4: 1.1563 (1.1563)  loss_bbox_4: 0.3799 (0.3799)  loss_giou_4: 0.8403 (0.8403)  loss_ce_unscaled: 0.5640 (0.5640)  class_error_unscaled: 33.6842 (33.6842)  loss_bbox_unscaled: 0.0752 (0.0752)  loss_giou_unscaled: 0.4190 (0.4190)  cardinality_error_unscaled: 289.8333 (289.8333)  loss_ce_0_unscaled: 0.5431 (0.5431)  loss_bbox_0_unscaled: 0.0962 (0.0962)  loss_giou_0_unscaled: 0.4830 (0.4830)  cardinality_error_0_unscaled: 286.4167 (286.4167)  loss_ce_1_unscaled: 0.5488 (0.5488)  loss_bbox_1_unscaled: 0.0812 (0.0812)  loss_giou_1_unscaled: 0.4383 (0.4383)  cardinality_error_1_unscaled: 288.3333 (288.3333)  loss_ce_2_unscaled: 0.5664 (0.5664)  loss_bbox_2_unscaled: 0.0751 (0.0751)  loss_giou_2_unscaled: 0.4247 (0.4247)  cardinality_error_2_unscaled: 289.6667 (289.6667)  loss_ce_3_unscaled: 0.5711 (0.5711)  loss_bbox_3_unscaled: 0.0753 (0.0753)  loss_giou_3_unscaled: 0.4210 (0.4210)  cardinality_error_3_unscaled: 288.6667 (288.6667)  loss_ce_4_unscaled: 0.5781 (0.5781)  loss_bbox_4_unscaled: 0.0760 (0.0760)  loss_giou_4_unscaled: 0.4202 (0.4202)  cardinality_error_4_unscaled: 286.0833 (286.0833)  time: 4.0968  data: 0.9217  max mem: 6678\n",
            "Test:  [ 10/417]  eta: 0:20:57  class_error: 54.55  loss: 15.0480 (15.7376)  loss_ce: 1.2560 (1.3172)  loss_bbox: 0.4091 (0.4402)  loss_giou: 0.8380 (0.8296)  loss_ce_0: 1.2595 (1.3216)  loss_bbox_0: 0.4809 (0.4884)  loss_giou_0: 0.9660 (0.9366)  loss_ce_1: 1.1778 (1.2908)  loss_bbox_1: 0.4280 (0.4451)  loss_giou_1: 0.8766 (0.8572)  loss_ce_2: 1.2526 (1.3264)  loss_bbox_2: 0.4182 (0.4392)  loss_giou_2: 0.8494 (0.8360)  loss_ce_3: 1.2675 (1.3335)  loss_bbox_3: 0.4162 (0.4399)  loss_giou_3: 0.8421 (0.8307)  loss_ce_4: 1.2686 (1.3314)  loss_bbox_4: 0.4175 (0.4420)  loss_giou_4: 0.8403 (0.8318)  loss_ce_unscaled: 0.6280 (0.6586)  class_error_unscaled: 46.6667 (45.6761)  loss_bbox_unscaled: 0.0818 (0.0880)  loss_giou_unscaled: 0.4190 (0.4148)  cardinality_error_unscaled: 290.2500 (290.4167)  loss_ce_0_unscaled: 0.6298 (0.6608)  loss_bbox_0_unscaled: 0.0962 (0.0977)  loss_giou_0_unscaled: 0.4830 (0.4683)  cardinality_error_0_unscaled: 290.2500 (290.0682)  loss_ce_1_unscaled: 0.5889 (0.6454)  loss_bbox_1_unscaled: 0.0856 (0.0890)  loss_giou_1_unscaled: 0.4383 (0.4286)  cardinality_error_1_unscaled: 290.7500 (290.8864)  loss_ce_2_unscaled: 0.6263 (0.6632)  loss_bbox_2_unscaled: 0.0836 (0.0878)  loss_giou_2_unscaled: 0.4247 (0.4180)  cardinality_error_2_unscaled: 290.8333 (290.4546)  loss_ce_3_unscaled: 0.6337 (0.6668)  loss_bbox_3_unscaled: 0.0832 (0.0880)  loss_giou_3_unscaled: 0.4210 (0.4153)  cardinality_error_3_unscaled: 290.3333 (290.4697)  loss_ce_4_unscaled: 0.6343 (0.6657)  loss_bbox_4_unscaled: 0.0835 (0.0884)  loss_giou_4_unscaled: 0.4202 (0.4159)  cardinality_error_4_unscaled: 289.7500 (289.1818)  time: 3.0904  data: 0.1120  max mem: 7374\n",
            "Test:  [ 20/417]  eta: 0:20:00  class_error: 45.74  loss: 14.5653 (15.1659)  loss_ce: 1.2560 (1.2699)  loss_bbox: 0.4079 (0.4191)  loss_giou: 0.7546 (0.7953)  loss_ce_0: 1.2967 (1.2846)  loss_bbox_0: 0.4540 (0.4809)  loss_giou_0: 0.8841 (0.9117)  loss_ce_1: 1.2878 (1.2596)  loss_bbox_1: 0.4204 (0.4322)  loss_giou_1: 0.7751 (0.8214)  loss_ce_2: 1.2526 (1.2811)  loss_bbox_2: 0.3969 (0.4193)  loss_giou_2: 0.7667 (0.8027)  loss_ce_3: 1.2675 (1.2812)  loss_bbox_3: 0.4125 (0.4203)  loss_giou_3: 0.7456 (0.7961)  loss_ce_4: 1.2415 (1.2724)  loss_bbox_4: 0.4108 (0.4206)  loss_giou_4: 0.7527 (0.7976)  loss_ce_unscaled: 0.6280 (0.6349)  class_error_unscaled: 45.7447 (44.2081)  loss_bbox_unscaled: 0.0816 (0.0838)  loss_giou_unscaled: 0.3773 (0.3976)  cardinality_error_unscaled: 290.9167 (290.7579)  loss_ce_0_unscaled: 0.6483 (0.6423)  loss_bbox_0_unscaled: 0.0908 (0.0962)  loss_giou_0_unscaled: 0.4420 (0.4559)  cardinality_error_0_unscaled: 290.2500 (290.5714)  loss_ce_1_unscaled: 0.6439 (0.6298)  loss_bbox_1_unscaled: 0.0841 (0.0864)  loss_giou_1_unscaled: 0.3876 (0.4107)  cardinality_error_1_unscaled: 291.0833 (291.1587)  loss_ce_2_unscaled: 0.6263 (0.6406)  loss_bbox_2_unscaled: 0.0794 (0.0839)  loss_giou_2_unscaled: 0.3834 (0.4014)  cardinality_error_2_unscaled: 290.9167 (290.9405)  loss_ce_3_unscaled: 0.6337 (0.6406)  loss_bbox_3_unscaled: 0.0825 (0.0841)  loss_giou_3_unscaled: 0.3728 (0.3980)  cardinality_error_3_unscaled: 290.7500 (290.9087)  loss_ce_4_unscaled: 0.6208 (0.6362)  loss_bbox_4_unscaled: 0.0822 (0.0841)  loss_giou_4_unscaled: 0.3763 (0.3988)  cardinality_error_4_unscaled: 289.8333 (289.8651)  time: 2.9705  data: 0.0297  max mem: 7755\n",
            "Test:  [ 30/417]  eta: 0:19:36  class_error: 39.44  loss: 13.9681 (14.6397)  loss_ce: 1.1353 (1.2078)  loss_bbox: 0.3595 (0.3884)  loss_giou: 0.7439 (0.7993)  loss_ce_0: 1.1447 (1.2244)  loss_bbox_0: 0.4305 (0.4524)  loss_giou_0: 0.8841 (0.9243)  loss_ce_1: 1.1180 (1.1998)  loss_bbox_1: 0.3822 (0.3993)  loss_giou_1: 0.7702 (0.8238)  loss_ce_2: 1.1382 (1.2207)  loss_bbox_2: 0.3538 (0.3883)  loss_giou_2: 0.7463 (0.8055)  loss_ce_3: 1.1252 (1.2153)  loss_bbox_3: 0.3614 (0.3899)  loss_giou_3: 0.7443 (0.8008)  loss_ce_4: 1.1359 (1.2071)  loss_bbox_4: 0.3632 (0.3901)  loss_giou_4: 0.7456 (0.8023)  loss_ce_unscaled: 0.5677 (0.6039)  class_error_unscaled: 39.4366 (42.5655)  loss_bbox_unscaled: 0.0719 (0.0777)  loss_giou_unscaled: 0.3720 (0.3996)  cardinality_error_unscaled: 290.1667 (290.2016)  loss_ce_0_unscaled: 0.5724 (0.6122)  loss_bbox_0_unscaled: 0.0861 (0.0905)  loss_giou_0_unscaled: 0.4420 (0.4622)  cardinality_error_0_unscaled: 289.9167 (290.3495)  loss_ce_1_unscaled: 0.5590 (0.5999)  loss_bbox_1_unscaled: 0.0764 (0.0799)  loss_giou_1_unscaled: 0.3851 (0.4119)  cardinality_error_1_unscaled: 290.7500 (290.7527)  loss_ce_2_unscaled: 0.5691 (0.6104)  loss_bbox_2_unscaled: 0.0708 (0.0777)  loss_giou_2_unscaled: 0.3732 (0.4028)  cardinality_error_2_unscaled: 290.8333 (290.5484)  loss_ce_3_unscaled: 0.5626 (0.6077)  loss_bbox_3_unscaled: 0.0723 (0.0780)  loss_giou_3_unscaled: 0.3721 (0.4004)  cardinality_error_3_unscaled: 290.7500 (290.4436)  loss_ce_4_unscaled: 0.5680 (0.6036)  loss_bbox_4_unscaled: 0.0726 (0.0780)  loss_giou_4_unscaled: 0.3728 (0.4011)  cardinality_error_4_unscaled: 289.6667 (289.3360)  time: 3.0118  data: 0.0290  max mem: 8137\n",
            "Test:  [ 40/417]  eta: 0:19:24  class_error: 40.34  loss: 13.9681 (14.5436)  loss_ce: 1.0749 (1.1955)  loss_bbox: 0.3595 (0.3913)  loss_giou: 0.7642 (0.7942)  loss_ce_0: 1.1305 (1.2119)  loss_bbox_0: 0.4178 (0.4593)  loss_giou_0: 0.9132 (0.9176)  loss_ce_1: 1.1005 (1.1867)  loss_bbox_1: 0.3579 (0.4029)  loss_giou_1: 0.7907 (0.8175)  loss_ce_2: 1.0879 (1.2055)  loss_bbox_2: 0.3538 (0.3917)  loss_giou_2: 0.7619 (0.7993)  loss_ce_3: 1.0685 (1.2026)  loss_bbox_3: 0.3556 (0.3913)  loss_giou_3: 0.7584 (0.7942)  loss_ce_4: 1.0790 (1.1935)  loss_bbox_4: 0.3597 (0.3923)  loss_giou_4: 0.7574 (0.7963)  loss_ce_unscaled: 0.5375 (0.5978)  class_error_unscaled: 38.3929 (41.4506)  loss_bbox_unscaled: 0.0719 (0.0783)  loss_giou_unscaled: 0.3821 (0.3971)  cardinality_error_unscaled: 290.1667 (290.3008)  loss_ce_0_unscaled: 0.5653 (0.6060)  loss_bbox_0_unscaled: 0.0836 (0.0919)  loss_giou_0_unscaled: 0.4566 (0.4588)  cardinality_error_0_unscaled: 290.5000 (290.5285)  loss_ce_1_unscaled: 0.5502 (0.5934)  loss_bbox_1_unscaled: 0.0716 (0.0806)  loss_giou_1_unscaled: 0.3953 (0.4088)  cardinality_error_1_unscaled: 290.7500 (290.9817)  loss_ce_2_unscaled: 0.5440 (0.6027)  loss_bbox_2_unscaled: 0.0708 (0.0783)  loss_giou_2_unscaled: 0.3809 (0.3997)  cardinality_error_2_unscaled: 290.8333 (290.5996)  loss_ce_3_unscaled: 0.5342 (0.6013)  loss_bbox_3_unscaled: 0.0711 (0.0783)  loss_giou_3_unscaled: 0.3792 (0.3971)  cardinality_error_3_unscaled: 290.7500 (290.4594)  loss_ce_4_unscaled: 0.5395 (0.5968)  loss_bbox_4_unscaled: 0.0719 (0.0785)  loss_giou_4_unscaled: 0.3787 (0.3982)  cardinality_error_4_unscaled: 289.6667 (289.1931)  time: 3.1571  data: 0.0313  max mem: 8137\n",
            "Test:  [ 50/417]  eta: 0:18:56  class_error: 37.78  loss: 15.7099 (14.8865)  loss_ce: 1.2224 (1.2287)  loss_bbox: 0.4215 (0.4020)  loss_giou: 0.8361 (0.8069)  loss_ce_0: 1.2423 (1.2380)  loss_bbox_0: 0.4929 (0.4695)  loss_giou_0: 0.9351 (0.9284)  loss_ce_1: 1.1980 (1.2198)  loss_bbox_1: 0.4284 (0.4137)  loss_giou_1: 0.8803 (0.8310)  loss_ce_2: 1.2428 (1.2430)  loss_bbox_2: 0.4252 (0.4022)  loss_giou_2: 0.8423 (0.8122)  loss_ce_3: 1.2397 (1.2402)  loss_bbox_3: 0.4082 (0.4008)  loss_giou_3: 0.8276 (0.8072)  loss_ce_4: 1.2360 (1.2320)  loss_bbox_4: 0.4075 (0.4021)  loss_giou_4: 0.8283 (0.8088)  loss_ce_unscaled: 0.6112 (0.6143)  class_error_unscaled: 42.8571 (42.6363)  loss_bbox_unscaled: 0.0843 (0.0804)  loss_giou_unscaled: 0.4181 (0.4034)  cardinality_error_unscaled: 289.7500 (289.7664)  loss_ce_0_unscaled: 0.6212 (0.6190)  loss_bbox_0_unscaled: 0.0986 (0.0939)  loss_giou_0_unscaled: 0.4675 (0.4642)  cardinality_error_0_unscaled: 289.2500 (289.9918)  loss_ce_1_unscaled: 0.5990 (0.6099)  loss_bbox_1_unscaled: 0.0857 (0.0827)  loss_giou_1_unscaled: 0.4401 (0.4155)  cardinality_error_1_unscaled: 290.2500 (290.4330)  loss_ce_2_unscaled: 0.6214 (0.6215)  loss_bbox_2_unscaled: 0.0850 (0.0804)  loss_giou_2_unscaled: 0.4211 (0.4061)  cardinality_error_2_unscaled: 290.0833 (289.9886)  loss_ce_3_unscaled: 0.6199 (0.6201)  loss_bbox_3_unscaled: 0.0816 (0.0802)  loss_giou_3_unscaled: 0.4138 (0.4036)  cardinality_error_3_unscaled: 289.1667 (289.8579)  loss_ce_4_unscaled: 0.6180 (0.6160)  loss_bbox_4_unscaled: 0.0815 (0.0804)  loss_giou_4_unscaled: 0.4141 (0.4044)  cardinality_error_4_unscaled: 288.2500 (288.3954)  time: 3.1866  data: 0.0317  max mem: 8138\n",
            "Test:  [ 60/417]  eta: 0:18:26  class_error: 35.23  loss: 16.3242 (15.0865)  loss_ce: 1.3312 (1.2469)  loss_bbox: 0.4338 (0.4061)  loss_giou: 0.8966 (0.8176)  loss_ce_0: 1.3311 (1.2554)  loss_bbox_0: 0.4929 (0.4732)  loss_giou_0: 1.0094 (0.9384)  loss_ce_1: 1.3330 (1.2352)  loss_bbox_1: 0.4457 (0.4188)  loss_giou_1: 0.9167 (0.8430)  loss_ce_2: 1.3431 (1.2609)  loss_bbox_2: 0.4397 (0.4072)  loss_giou_2: 0.9030 (0.8226)  loss_ce_3: 1.3405 (1.2592)  loss_bbox_3: 0.4285 (0.4049)  loss_giou_3: 0.8858 (0.8181)  loss_ce_4: 1.3628 (1.2530)  loss_bbox_4: 0.4271 (0.4061)  loss_giou_4: 0.8904 (0.8197)  loss_ce_unscaled: 0.6656 (0.6235)  class_error_unscaled: 45.2381 (43.5298)  loss_bbox_unscaled: 0.0868 (0.0812)  loss_giou_unscaled: 0.4483 (0.4088)  cardinality_error_unscaled: 289.8333 (289.6926)  loss_ce_0_unscaled: 0.6656 (0.6277)  loss_bbox_0_unscaled: 0.0986 (0.0946)  loss_giou_0_unscaled: 0.5047 (0.4692)  cardinality_error_0_unscaled: 289.1667 (289.8060)  loss_ce_1_unscaled: 0.6665 (0.6176)  loss_bbox_1_unscaled: 0.0891 (0.0838)  loss_giou_1_unscaled: 0.4583 (0.4215)  cardinality_error_1_unscaled: 289.9167 (290.2090)  loss_ce_2_unscaled: 0.6716 (0.6304)  loss_bbox_2_unscaled: 0.0879 (0.0814)  loss_giou_2_unscaled: 0.4515 (0.4113)  cardinality_error_2_unscaled: 290.0833 (289.8675)  loss_ce_3_unscaled: 0.6703 (0.6296)  loss_bbox_3_unscaled: 0.0857 (0.0810)  loss_giou_3_unscaled: 0.4429 (0.4090)  cardinality_error_3_unscaled: 289.1667 (289.6913)  loss_ce_4_unscaled: 0.6814 (0.6265)  loss_bbox_4_unscaled: 0.0854 (0.0812)  loss_giou_4_unscaled: 0.4452 (0.4098)  cardinality_error_4_unscaled: 288.7500 (288.4372)  time: 3.1248  data: 0.0324  max mem: 8138\n",
            "Test:  [ 70/417]  eta: 0:18:04  class_error: 39.78  loss: 15.1195 (14.9360)  loss_ce: 1.2625 (1.2329)  loss_bbox: 0.3913 (0.4022)  loss_giou: 0.7776 (0.8107)  loss_ce_0: 1.2631 (1.2427)  loss_bbox_0: 0.4805 (0.4714)  loss_giou_0: 0.9063 (0.9302)  loss_ce_1: 1.2666 (1.2219)  loss_bbox_1: 0.4097 (0.4141)  loss_giou_1: 0.8167 (0.8361)  loss_ce_2: 1.2840 (1.2474)  loss_bbox_2: 0.4031 (0.4033)  loss_giou_2: 0.7906 (0.8153)  loss_ce_3: 1.2497 (1.2425)  loss_bbox_3: 0.3924 (0.4009)  loss_giou_3: 0.7768 (0.8113)  loss_ce_4: 1.2678 (1.2378)  loss_bbox_4: 0.3909 (0.4025)  loss_giou_4: 0.7765 (0.8128)  loss_ce_unscaled: 0.6313 (0.6165)  class_error_unscaled: 42.6471 (42.5812)  loss_bbox_unscaled: 0.0783 (0.0804)  loss_giou_unscaled: 0.3888 (0.4054)  cardinality_error_unscaled: 290.5833 (289.7488)  loss_ce_0_unscaled: 0.6316 (0.6213)  loss_bbox_0_unscaled: 0.0961 (0.0943)  loss_giou_0_unscaled: 0.4531 (0.4651)  cardinality_error_0_unscaled: 290.1667 (289.8803)  loss_ce_1_unscaled: 0.6333 (0.6109)  loss_bbox_1_unscaled: 0.0819 (0.0828)  loss_giou_1_unscaled: 0.4084 (0.4180)  cardinality_error_1_unscaled: 290.5000 (290.2617)  loss_ce_2_unscaled: 0.6420 (0.6237)  loss_bbox_2_unscaled: 0.0806 (0.0807)  loss_giou_2_unscaled: 0.3953 (0.4077)  cardinality_error_2_unscaled: 290.6667 (289.8721)  loss_ce_3_unscaled: 0.6249 (0.6213)  loss_bbox_3_unscaled: 0.0785 (0.0802)  loss_giou_3_unscaled: 0.3884 (0.4057)  cardinality_error_3_unscaled: 289.9167 (289.6725)  loss_ce_4_unscaled: 0.6339 (0.6189)  loss_bbox_4_unscaled: 0.0782 (0.0805)  loss_giou_4_unscaled: 0.3882 (0.4064)  cardinality_error_4_unscaled: 290.0833 (288.4953)  time: 3.1989  data: 0.0324  max mem: 8138\n",
            "Test:  [ 80/417]  eta: 0:17:28  class_error: 58.59  loss: 14.2264 (14.8153)  loss_ce: 1.1731 (1.2277)  loss_bbox: 0.3689 (0.3987)  loss_giou: 0.7643 (0.7988)  loss_ce_0: 1.1838 (1.2379)  loss_bbox_0: 0.4608 (0.4703)  loss_giou_0: 0.8701 (0.9190)  loss_ce_1: 1.1441 (1.2182)  loss_bbox_1: 0.3865 (0.4114)  loss_giou_1: 0.7682 (0.8231)  loss_ce_2: 1.1519 (1.2417)  loss_bbox_2: 0.3763 (0.4003)  loss_giou_2: 0.7591 (0.8035)  loss_ce_3: 1.1877 (1.2361)  loss_bbox_3: 0.3692 (0.3975)  loss_giou_3: 0.7661 (0.7995)  loss_ce_4: 1.1998 (1.2317)  loss_bbox_4: 0.3693 (0.3992)  loss_giou_4: 0.7687 (0.8008)  loss_ce_unscaled: 0.5865 (0.6138)  class_error_unscaled: 39.4366 (42.2654)  loss_bbox_unscaled: 0.0738 (0.0797)  loss_giou_unscaled: 0.3822 (0.3994)  cardinality_error_unscaled: 290.0833 (289.8200)  loss_ce_0_unscaled: 0.5919 (0.6190)  loss_bbox_0_unscaled: 0.0922 (0.0941)  loss_giou_0_unscaled: 0.4351 (0.4595)  cardinality_error_0_unscaled: 290.5833 (289.9218)  loss_ce_1_unscaled: 0.5720 (0.6091)  loss_bbox_1_unscaled: 0.0773 (0.0823)  loss_giou_1_unscaled: 0.3841 (0.4116)  cardinality_error_1_unscaled: 290.7500 (290.2593)  loss_ce_2_unscaled: 0.5759 (0.6208)  loss_bbox_2_unscaled: 0.0753 (0.0801)  loss_giou_2_unscaled: 0.3795 (0.4018)  cardinality_error_2_unscaled: 290.6667 (289.8910)  loss_ce_3_unscaled: 0.5938 (0.6180)  loss_bbox_3_unscaled: 0.0738 (0.0795)  loss_giou_3_unscaled: 0.3831 (0.3997)  cardinality_error_3_unscaled: 289.5833 (289.7099)  loss_ce_4_unscaled: 0.5999 (0.6158)  loss_bbox_4_unscaled: 0.0739 (0.0798)  loss_giou_4_unscaled: 0.3844 (0.4004)  cardinality_error_4_unscaled: 289.5833 (288.4784)  time: 3.1477  data: 0.0306  max mem: 8138\n",
            "Test:  [ 90/417]  eta: 0:16:57  class_error: 25.00  loss: 14.2264 (14.8657)  loss_ce: 1.1731 (1.2295)  loss_bbox: 0.3744 (0.4014)  loss_giou: 0.7686 (0.8038)  loss_ce_0: 1.1881 (1.2385)  loss_bbox_0: 0.4703 (0.4722)  loss_giou_0: 0.8883 (0.9233)  loss_ce_1: 1.1441 (1.2179)  loss_bbox_1: 0.3865 (0.4144)  loss_giou_1: 0.7989 (0.8286)  loss_ce_2: 1.1519 (1.2419)  loss_bbox_2: 0.3815 (0.4031)  loss_giou_2: 0.7789 (0.8089)  loss_ce_3: 1.1680 (1.2373)  loss_bbox_3: 0.3726 (0.4004)  loss_giou_3: 0.7661 (0.8047)  loss_ce_4: 1.1765 (1.2319)  loss_bbox_4: 0.3726 (0.4022)  loss_giou_4: 0.7715 (0.8058)  loss_ce_unscaled: 0.5865 (0.6147)  class_error_unscaled: 43.2432 (42.4163)  loss_bbox_unscaled: 0.0749 (0.0803)  loss_giou_unscaled: 0.3843 (0.4019)  cardinality_error_unscaled: 290.6667 (289.8050)  loss_ce_0_unscaled: 0.5940 (0.6193)  loss_bbox_0_unscaled: 0.0941 (0.0944)  loss_giou_0_unscaled: 0.4442 (0.4616)  cardinality_error_0_unscaled: 290.5000 (289.8974)  loss_ce_1_unscaled: 0.5720 (0.6090)  loss_bbox_1_unscaled: 0.0773 (0.0829)  loss_giou_1_unscaled: 0.3994 (0.4143)  cardinality_error_1_unscaled: 290.6667 (290.2583)  loss_ce_2_unscaled: 0.5759 (0.6209)  loss_bbox_2_unscaled: 0.0763 (0.0806)  loss_giou_2_unscaled: 0.3894 (0.4044)  cardinality_error_2_unscaled: 290.6667 (289.9139)  loss_ce_3_unscaled: 0.5840 (0.6187)  loss_bbox_3_unscaled: 0.0745 (0.0801)  loss_giou_3_unscaled: 0.3831 (0.4024)  cardinality_error_3_unscaled: 290.5000 (289.7079)  loss_ce_4_unscaled: 0.5882 (0.6160)  loss_bbox_4_unscaled: 0.0745 (0.0804)  loss_giou_4_unscaled: 0.3858 (0.4029)  cardinality_error_4_unscaled: 289.8333 (288.5476)  time: 3.0549  data: 0.0316  max mem: 8138\n",
            "Test:  [100/417]  eta: 0:16:23  class_error: 54.93  loss: 15.0328 (14.9275)  loss_ce: 1.3422 (1.2379)  loss_bbox: 0.4187 (0.4024)  loss_giou: 0.7965 (0.8049)  loss_ce_0: 1.3094 (1.2473)  loss_bbox_0: 0.4780 (0.4716)  loss_giou_0: 0.9192 (0.9236)  loss_ce_1: 1.3031 (1.2271)  loss_bbox_1: 0.4259 (0.4144)  loss_giou_1: 0.8317 (0.8290)  loss_ce_2: 1.3799 (1.2532)  loss_bbox_2: 0.4242 (0.4033)  loss_giou_2: 0.8043 (0.8093)  loss_ce_3: 1.3643 (1.2471)  loss_bbox_3: 0.4077 (0.4007)  loss_giou_3: 0.8013 (0.8055)  loss_ce_4: 1.3589 (1.2404)  loss_bbox_4: 0.4185 (0.4028)  loss_giou_4: 0.7988 (0.8070)  loss_ce_unscaled: 0.6711 (0.6190)  class_error_unscaled: 44.8980 (42.7776)  loss_bbox_unscaled: 0.0837 (0.0805)  loss_giou_unscaled: 0.3982 (0.4025)  cardinality_error_unscaled: 290.6667 (289.8729)  loss_ce_0_unscaled: 0.6547 (0.6237)  loss_bbox_0_unscaled: 0.0956 (0.0943)  loss_giou_0_unscaled: 0.4596 (0.4618)  cardinality_error_0_unscaled: 289.8333 (289.9018)  loss_ce_1_unscaled: 0.6515 (0.6136)  loss_bbox_1_unscaled: 0.0852 (0.0829)  loss_giou_1_unscaled: 0.4158 (0.4145)  cardinality_error_1_unscaled: 290.6667 (290.2698)  loss_ce_2_unscaled: 0.6899 (0.6266)  loss_bbox_2_unscaled: 0.0848 (0.0807)  loss_giou_2_unscaled: 0.4022 (0.4046)  cardinality_error_2_unscaled: 290.0833 (289.8325)  loss_ce_3_unscaled: 0.6821 (0.6235)  loss_bbox_3_unscaled: 0.0815 (0.0801)  loss_giou_3_unscaled: 0.4006 (0.4027)  cardinality_error_3_unscaled: 290.5000 (289.6815)  loss_ce_4_unscaled: 0.6795 (0.6202)  loss_bbox_4_unscaled: 0.0837 (0.0806)  loss_giou_4_unscaled: 0.3994 (0.4035)  cardinality_error_4_unscaled: 289.5000 (288.6031)  time: 3.0657  data: 0.0308  max mem: 8138\n",
            "Test:  [110/417]  eta: 0:15:51  class_error: 51.43  loss: 16.1813 (15.0951)  loss_ce: 1.3422 (1.2521)  loss_bbox: 0.4503 (0.4098)  loss_giou: 0.8121 (0.8120)  loss_ce_0: 1.3346 (1.2611)  loss_bbox_0: 0.4824 (0.4773)  loss_giou_0: 0.9337 (0.9277)  loss_ce_1: 1.3031 (1.2408)  loss_bbox_1: 0.4524 (0.4219)  loss_giou_1: 0.8404 (0.8365)  loss_ce_2: 1.3899 (1.2674)  loss_bbox_2: 0.4431 (0.4103)  loss_giou_2: 0.8192 (0.8167)  loss_ce_3: 1.3653 (1.2615)  loss_bbox_3: 0.4338 (0.4076)  loss_giou_3: 0.8101 (0.8129)  loss_ce_4: 1.3589 (1.2553)  loss_bbox_4: 0.4427 (0.4101)  loss_giou_4: 0.8160 (0.8143)  loss_ce_unscaled: 0.6711 (0.6260)  class_error_unscaled: 45.1219 (43.0853)  loss_bbox_unscaled: 0.0901 (0.0820)  loss_giou_unscaled: 0.4060 (0.4060)  cardinality_error_unscaled: 290.0000 (289.8844)  loss_ce_0_unscaled: 0.6673 (0.6305)  loss_bbox_0_unscaled: 0.0965 (0.0955)  loss_giou_0_unscaled: 0.4669 (0.4639)  cardinality_error_0_unscaled: 289.6667 (289.9670)  loss_ce_1_unscaled: 0.6515 (0.6204)  loss_bbox_1_unscaled: 0.0905 (0.0844)  loss_giou_1_unscaled: 0.4202 (0.4182)  cardinality_error_1_unscaled: 290.4167 (290.2898)  loss_ce_2_unscaled: 0.6949 (0.6337)  loss_bbox_2_unscaled: 0.0886 (0.0821)  loss_giou_2_unscaled: 0.4096 (0.4084)  cardinality_error_2_unscaled: 289.5833 (289.8746)  loss_ce_3_unscaled: 0.6827 (0.6308)  loss_bbox_3_unscaled: 0.0868 (0.0815)  loss_giou_3_unscaled: 0.4051 (0.4064)  cardinality_error_3_unscaled: 290.3333 (289.7027)  loss_ce_4_unscaled: 0.6795 (0.6276)  loss_bbox_4_unscaled: 0.0885 (0.0820)  loss_giou_4_unscaled: 0.4080 (0.4071)  cardinality_error_4_unscaled: 289.4167 (288.6682)  time: 3.0426  data: 0.0292  max mem: 8138\n",
            "Test:  [120/417]  eta: 0:15:15  class_error: 38.18  loss: 14.7569 (15.0650)  loss_ce: 1.2158 (1.2460)  loss_bbox: 0.4342 (0.4094)  loss_giou: 0.8100 (0.8135)  loss_ce_0: 1.2589 (1.2573)  loss_bbox_0: 0.4758 (0.4757)  loss_giou_0: 0.9040 (0.9281)  loss_ce_1: 1.2397 (1.2356)  loss_bbox_1: 0.4498 (0.4211)  loss_giou_1: 0.8404 (0.8376)  loss_ce_2: 1.2333 (1.2614)  loss_bbox_2: 0.4336 (0.4099)  loss_giou_2: 0.8260 (0.8181)  loss_ce_3: 1.2707 (1.2554)  loss_bbox_3: 0.4293 (0.4072)  loss_giou_3: 0.8101 (0.8142)  loss_ce_4: 1.2117 (1.2492)  loss_bbox_4: 0.4351 (0.4097)  loss_giou_4: 0.8160 (0.8157)  loss_ce_unscaled: 0.6079 (0.6230)  class_error_unscaled: 39.3443 (42.7746)  loss_bbox_unscaled: 0.0868 (0.0819)  loss_giou_unscaled: 0.4050 (0.4067)  cardinality_error_unscaled: 289.8333 (289.6984)  loss_ce_0_unscaled: 0.6295 (0.6286)  loss_bbox_0_unscaled: 0.0952 (0.0951)  loss_giou_0_unscaled: 0.4520 (0.4641)  cardinality_error_0_unscaled: 289.9167 (289.7679)  loss_ce_1_unscaled: 0.6199 (0.6178)  loss_bbox_1_unscaled: 0.0900 (0.0842)  loss_giou_1_unscaled: 0.4202 (0.4188)  cardinality_error_1_unscaled: 290.4167 (290.0909)  loss_ce_2_unscaled: 0.6166 (0.6307)  loss_bbox_2_unscaled: 0.0867 (0.0820)  loss_giou_2_unscaled: 0.4130 (0.4091)  cardinality_error_2_unscaled: 289.2500 (289.6426)  loss_ce_3_unscaled: 0.6353 (0.6277)  loss_bbox_3_unscaled: 0.0859 (0.0814)  loss_giou_3_unscaled: 0.4051 (0.4071)  cardinality_error_3_unscaled: 289.8333 (289.4649)  loss_ce_4_unscaled: 0.6058 (0.6246)  loss_bbox_4_unscaled: 0.0870 (0.0819)  loss_giou_4_unscaled: 0.4080 (0.4078)  cardinality_error_4_unscaled: 289.0833 (288.3802)  time: 2.9829  data: 0.0291  max mem: 8138\n",
            "Test:  [130/417]  eta: 0:14:42  class_error: 49.18  loss: 14.6638 (15.0730)  loss_ce: 1.1872 (1.2475)  loss_bbox: 0.4053 (0.4108)  loss_giou: 0.8289 (0.8120)  loss_ce_0: 1.2221 (1.2598)  loss_bbox_0: 0.4594 (0.4776)  loss_giou_0: 0.9530 (0.9262)  loss_ce_1: 1.1775 (1.2372)  loss_bbox_1: 0.4065 (0.4226)  loss_giou_1: 0.8537 (0.8357)  loss_ce_2: 1.1778 (1.2625)  loss_bbox_2: 0.3997 (0.4111)  loss_giou_2: 0.8380 (0.8163)  loss_ce_3: 1.1786 (1.2568)  loss_bbox_3: 0.3952 (0.4086)  loss_giou_3: 0.8334 (0.8126)  loss_ce_4: 1.1887 (1.2504)  loss_bbox_4: 0.4088 (0.4112)  loss_giou_4: 0.8346 (0.8141)  loss_ce_unscaled: 0.5936 (0.6238)  class_error_unscaled: 39.3443 (42.8408)  loss_bbox_unscaled: 0.0811 (0.0822)  loss_giou_unscaled: 0.4145 (0.4060)  cardinality_error_unscaled: 288.7500 (289.6419)  loss_ce_0_unscaled: 0.6111 (0.6299)  loss_bbox_0_unscaled: 0.0919 (0.0955)  loss_giou_0_unscaled: 0.4765 (0.4631)  cardinality_error_0_unscaled: 289.8333 (289.6775)  loss_ce_1_unscaled: 0.5887 (0.6186)  loss_bbox_1_unscaled: 0.0813 (0.0845)  loss_giou_1_unscaled: 0.4268 (0.4178)  cardinality_error_1_unscaled: 289.6667 (290.0325)  loss_ce_2_unscaled: 0.5889 (0.6313)  loss_bbox_2_unscaled: 0.0799 (0.0822)  loss_giou_2_unscaled: 0.4190 (0.4082)  cardinality_error_2_unscaled: 288.4167 (289.5503)  loss_ce_3_unscaled: 0.5893 (0.6284)  loss_bbox_3_unscaled: 0.0790 (0.0817)  loss_giou_3_unscaled: 0.4167 (0.4063)  cardinality_error_3_unscaled: 289.0000 (289.3970)  loss_ce_4_unscaled: 0.5943 (0.6252)  loss_bbox_4_unscaled: 0.0818 (0.0822)  loss_giou_4_unscaled: 0.4173 (0.4071)  cardinality_error_4_unscaled: 287.8333 (288.3181)  time: 2.9431  data: 0.0288  max mem: 8138\n",
            "Test:  [140/417]  eta: 0:14:12  class_error: 65.00  loss: 16.2762 (15.1404)  loss_ce: 1.3745 (1.2587)  loss_bbox: 0.4136 (0.4103)  loss_giou: 0.8532 (0.8127)  loss_ce_0: 1.3565 (1.2701)  loss_bbox_0: 0.4876 (0.4771)  loss_giou_0: 0.9530 (0.9265)  loss_ce_1: 1.3218 (1.2473)  loss_bbox_1: 0.4374 (0.4225)  loss_giou_1: 0.8655 (0.8362)  loss_ce_2: 1.3513 (1.2732)  loss_bbox_2: 0.4186 (0.4109)  loss_giou_2: 0.8623 (0.8170)  loss_ce_3: 1.3595 (1.2683)  loss_bbox_3: 0.4090 (0.4083)  loss_giou_3: 0.8540 (0.8132)  loss_ce_4: 1.3732 (1.2627)  loss_bbox_4: 0.4184 (0.4108)  loss_giou_4: 0.8442 (0.8147)  loss_ce_unscaled: 0.6873 (0.6293)  class_error_unscaled: 49.0741 (43.6000)  loss_bbox_unscaled: 0.0827 (0.0821)  loss_giou_unscaled: 0.4266 (0.4063)  cardinality_error_unscaled: 288.6667 (289.6046)  loss_ce_0_unscaled: 0.6782 (0.6351)  loss_bbox_0_unscaled: 0.0975 (0.0954)  loss_giou_0_unscaled: 0.4765 (0.4632)  cardinality_error_0_unscaled: 289.9167 (289.6330)  loss_ce_1_unscaled: 0.6609 (0.6237)  loss_bbox_1_unscaled: 0.0875 (0.0845)  loss_giou_1_unscaled: 0.4327 (0.4181)  cardinality_error_1_unscaled: 290.0833 (290.0189)  loss_ce_2_unscaled: 0.6756 (0.6366)  loss_bbox_2_unscaled: 0.0837 (0.0822)  loss_giou_2_unscaled: 0.4312 (0.4085)  cardinality_error_2_unscaled: 288.4167 (289.5077)  loss_ce_3_unscaled: 0.6797 (0.6342)  loss_bbox_3_unscaled: 0.0818 (0.0817)  loss_giou_3_unscaled: 0.4270 (0.4066)  cardinality_error_3_unscaled: 289.0000 (289.3859)  loss_ce_4_unscaled: 0.6866 (0.6313)  loss_bbox_4_unscaled: 0.0837 (0.0822)  loss_giou_4_unscaled: 0.4221 (0.4074)  cardinality_error_4_unscaled: 287.8333 (288.2961)  time: 3.0495  data: 0.0301  max mem: 8138\n",
            "Test:  [150/417]  eta: 0:13:40  class_error: 44.00  loss: 14.7623 (15.0896)  loss_ce: 1.2771 (1.2525)  loss_bbox: 0.3889 (0.4098)  loss_giou: 0.8177 (0.8100)  loss_ce_0: 1.2552 (1.2668)  loss_bbox_0: 0.4850 (0.4767)  loss_giou_0: 0.8883 (0.9240)  loss_ce_1: 1.2377 (1.2429)  loss_bbox_1: 0.4217 (0.4211)  loss_giou_1: 0.8495 (0.8334)  loss_ce_2: 1.2693 (1.2681)  loss_bbox_2: 0.3889 (0.4102)  loss_giou_2: 0.8171 (0.8139)  loss_ce_3: 1.2413 (1.2629)  loss_bbox_3: 0.3937 (0.4076)  loss_giou_3: 0.8236 (0.8103)  loss_ce_4: 1.2464 (1.2570)  loss_bbox_4: 0.3880 (0.4102)  loss_giou_4: 0.8214 (0.8120)  loss_ce_unscaled: 0.6385 (0.6263)  class_error_unscaled: 43.5294 (43.3835)  loss_bbox_unscaled: 0.0778 (0.0820)  loss_giou_unscaled: 0.4089 (0.4050)  cardinality_error_unscaled: 289.9167 (289.5116)  loss_ce_0_unscaled: 0.6276 (0.6334)  loss_bbox_0_unscaled: 0.0970 (0.0953)  loss_giou_0_unscaled: 0.4442 (0.4620)  cardinality_error_0_unscaled: 289.9167 (289.5469)  loss_ce_1_unscaled: 0.6189 (0.6214)  loss_bbox_1_unscaled: 0.0843 (0.0842)  loss_giou_1_unscaled: 0.4248 (0.4167)  cardinality_error_1_unscaled: 290.5833 (289.9371)  loss_ce_2_unscaled: 0.6347 (0.6341)  loss_bbox_2_unscaled: 0.0778 (0.0820)  loss_giou_2_unscaled: 0.4086 (0.4070)  cardinality_error_2_unscaled: 289.6667 (289.4233)  loss_ce_3_unscaled: 0.6206 (0.6315)  loss_bbox_3_unscaled: 0.0787 (0.0815)  loss_giou_3_unscaled: 0.4118 (0.4051)  cardinality_error_3_unscaled: 289.1667 (289.3080)  loss_ce_4_unscaled: 0.6232 (0.6285)  loss_bbox_4_unscaled: 0.0776 (0.0820)  loss_giou_4_unscaled: 0.4107 (0.4060)  cardinality_error_4_unscaled: 287.1667 (288.1998)  time: 3.0756  data: 0.0316  max mem: 8138\n",
            "Test:  [160/417]  eta: 0:13:09  class_error: 28.00  loss: 14.7361 (15.0307)  loss_ce: 1.1937 (1.2486)  loss_bbox: 0.3621 (0.4068)  loss_giou: 0.7322 (0.8076)  loss_ce_0: 1.1954 (1.2607)  loss_bbox_0: 0.4293 (0.4738)  loss_giou_0: 0.8733 (0.9229)  loss_ce_1: 1.1513 (1.2387)  loss_bbox_1: 0.3747 (0.4180)  loss_giou_1: 0.7547 (0.8309)  loss_ce_2: 1.1997 (1.2626)  loss_bbox_2: 0.3619 (0.4075)  loss_giou_2: 0.7250 (0.8116)  loss_ce_3: 1.1963 (1.2585)  loss_bbox_3: 0.3583 (0.4049)  loss_giou_3: 0.7238 (0.8079)  loss_ce_4: 1.1804 (1.2527)  loss_bbox_4: 0.3517 (0.4074)  loss_giou_4: 0.7425 (0.8096)  loss_ce_unscaled: 0.5968 (0.6243)  class_error_unscaled: 41.4286 (43.2195)  loss_bbox_unscaled: 0.0724 (0.0814)  loss_giou_unscaled: 0.3661 (0.4038)  cardinality_error_unscaled: 290.4167 (289.5839)  loss_ce_0_unscaled: 0.5977 (0.6303)  loss_bbox_0_unscaled: 0.0859 (0.0948)  loss_giou_0_unscaled: 0.4367 (0.4615)  cardinality_error_0_unscaled: 290.9167 (289.6103)  loss_ce_1_unscaled: 0.5757 (0.6194)  loss_bbox_1_unscaled: 0.0749 (0.0836)  loss_giou_1_unscaled: 0.3773 (0.4154)  cardinality_error_1_unscaled: 291.0833 (289.9886)  loss_ce_2_unscaled: 0.5998 (0.6313)  loss_bbox_2_unscaled: 0.0724 (0.0815)  loss_giou_2_unscaled: 0.3625 (0.4058)  cardinality_error_2_unscaled: 290.5000 (289.4964)  loss_ce_3_unscaled: 0.5982 (0.6293)  loss_bbox_3_unscaled: 0.0717 (0.0810)  loss_giou_3_unscaled: 0.3619 (0.4040)  cardinality_error_3_unscaled: 289.3333 (289.3551)  loss_ce_4_unscaled: 0.5902 (0.6263)  loss_bbox_4_unscaled: 0.0703 (0.0815)  loss_giou_4_unscaled: 0.3713 (0.4048)  cardinality_error_4_unscaled: 289.0000 (288.2992)  time: 3.0224  data: 0.0318  max mem: 8138\n",
            "Test:  [170/417]  eta: 0:12:38  class_error: 52.81  loss: 15.6090 (15.0632)  loss_ce: 1.3117 (1.2510)  loss_bbox: 0.3930 (0.4086)  loss_giou: 0.7678 (0.8092)  loss_ce_0: 1.3069 (1.2625)  loss_bbox_0: 0.4197 (0.4748)  loss_giou_0: 0.8792 (0.9239)  loss_ce_1: 1.2952 (1.2411)  loss_bbox_1: 0.3941 (0.4194)  loss_giou_1: 0.7944 (0.8324)  loss_ce_2: 1.3218 (1.2650)  loss_bbox_2: 0.3788 (0.4091)  loss_giou_2: 0.7748 (0.8133)  loss_ce_3: 1.3165 (1.2608)  loss_bbox_3: 0.3798 (0.4067)  loss_giou_3: 0.7674 (0.8097)  loss_ce_4: 1.3086 (1.2553)  loss_bbox_4: 0.3922 (0.4093)  loss_giou_4: 0.7736 (0.8112)  loss_ce_unscaled: 0.6558 (0.6255)  class_error_unscaled: 46.7290 (43.3607)  loss_bbox_unscaled: 0.0786 (0.0817)  loss_giou_unscaled: 0.3839 (0.4046)  cardinality_error_unscaled: 289.5833 (289.4683)  loss_ce_0_unscaled: 0.6534 (0.6313)  loss_bbox_0_unscaled: 0.0839 (0.0950)  loss_giou_0_unscaled: 0.4396 (0.4619)  cardinality_error_0_unscaled: 289.5833 (289.5132)  loss_ce_1_unscaled: 0.6476 (0.6205)  loss_bbox_1_unscaled: 0.0788 (0.0839)  loss_giou_1_unscaled: 0.3972 (0.4162)  cardinality_error_1_unscaled: 289.8333 (289.8899)  loss_ce_2_unscaled: 0.6609 (0.6325)  loss_bbox_2_unscaled: 0.0758 (0.0818)  loss_giou_2_unscaled: 0.3874 (0.4066)  cardinality_error_2_unscaled: 290.4167 (289.3957)  loss_ce_3_unscaled: 0.6582 (0.6304)  loss_bbox_3_unscaled: 0.0760 (0.0813)  loss_giou_3_unscaled: 0.3837 (0.4048)  cardinality_error_3_unscaled: 289.3333 (289.2383)  loss_ce_4_unscaled: 0.6543 (0.6276)  loss_bbox_4_unscaled: 0.0784 (0.0819)  loss_giou_4_unscaled: 0.3868 (0.4056)  cardinality_error_4_unscaled: 289.0833 (288.1813)  time: 3.0320  data: 0.0312  max mem: 8138\n",
            "Test:  [180/417]  eta: 0:12:07  class_error: 40.32  loss: 14.1450 (14.9576)  loss_ce: 1.2925 (1.2415)  loss_bbox: 0.3990 (0.4069)  loss_giou: 0.7471 (0.8022)  loss_ce_0: 1.1918 (1.2540)  loss_bbox_0: 0.4433 (0.4730)  loss_giou_0: 0.8703 (0.9176)  loss_ce_1: 1.1656 (1.2321)  loss_bbox_1: 0.3984 (0.4178)  loss_giou_1: 0.7691 (0.8259)  loss_ce_2: 1.2613 (1.2559)  loss_bbox_2: 0.3908 (0.4074)  loss_giou_2: 0.7578 (0.8066)  loss_ce_3: 1.2614 (1.2509)  loss_bbox_3: 0.3975 (0.4053)  loss_giou_3: 0.7508 (0.8028)  loss_ce_4: 1.2785 (1.2455)  loss_bbox_4: 0.3931 (0.4077)  loss_giou_4: 0.7494 (0.8044)  loss_ce_unscaled: 0.6462 (0.6207)  class_error_unscaled: 40.3226 (43.0154)  loss_bbox_unscaled: 0.0798 (0.0814)  loss_giou_unscaled: 0.3736 (0.4011)  cardinality_error_unscaled: 289.5000 (289.4263)  loss_ce_0_unscaled: 0.5959 (0.6270)  loss_bbox_0_unscaled: 0.0887 (0.0946)  loss_giou_0_unscaled: 0.4351 (0.4588)  cardinality_error_0_unscaled: 289.5833 (289.4636)  loss_ce_1_unscaled: 0.5828 (0.6161)  loss_bbox_1_unscaled: 0.0797 (0.0836)  loss_giou_1_unscaled: 0.3846 (0.4129)  cardinality_error_1_unscaled: 289.8333 (289.8467)  loss_ce_2_unscaled: 0.6307 (0.6280)  loss_bbox_2_unscaled: 0.0782 (0.0815)  loss_giou_2_unscaled: 0.3789 (0.4033)  cardinality_error_2_unscaled: 289.6667 (289.3467)  loss_ce_3_unscaled: 0.6307 (0.6255)  loss_bbox_3_unscaled: 0.0795 (0.0811)  loss_giou_3_unscaled: 0.3754 (0.4014)  cardinality_error_3_unscaled: 289.5833 (289.1492)  loss_ce_4_unscaled: 0.6392 (0.6228)  loss_bbox_4_unscaled: 0.0786 (0.0815)  loss_giou_4_unscaled: 0.3747 (0.4022)  cardinality_error_4_unscaled: 288.0833 (288.0985)  time: 3.0356  data: 0.0308  max mem: 8138\n",
            "Test:  [190/417]  eta: 0:11:35  class_error: 37.50  loss: 14.0379 (15.0478)  loss_ce: 1.2083 (1.2535)  loss_bbox: 0.3897 (0.4085)  loss_giou: 0.7471 (0.8041)  loss_ce_0: 1.1903 (1.2651)  loss_bbox_0: 0.4446 (0.4745)  loss_giou_0: 0.8703 (0.9185)  loss_ce_1: 1.1656 (1.2432)  loss_bbox_1: 0.4027 (0.4196)  loss_giou_1: 0.7691 (0.8272)  loss_ce_2: 1.2574 (1.2682)  loss_bbox_2: 0.3878 (0.4092)  loss_giou_2: 0.7578 (0.8082)  loss_ce_3: 1.2414 (1.2630)  loss_bbox_3: 0.3973 (0.4073)  loss_giou_3: 0.7508 (0.8044)  loss_ce_4: 1.1876 (1.2580)  loss_bbox_4: 0.3908 (0.4094)  loss_giou_4: 0.7494 (0.8060)  loss_ce_unscaled: 0.6042 (0.6268)  class_error_unscaled: 39.7849 (43.4708)  loss_bbox_unscaled: 0.0779 (0.0817)  loss_giou_unscaled: 0.3736 (0.4021)  cardinality_error_unscaled: 290.3333 (289.4808)  loss_ce_0_unscaled: 0.5952 (0.6326)  loss_bbox_0_unscaled: 0.0889 (0.0949)  loss_giou_0_unscaled: 0.4351 (0.4593)  cardinality_error_0_unscaled: 290.5833 (289.4965)  loss_ce_1_unscaled: 0.5828 (0.6216)  loss_bbox_1_unscaled: 0.0805 (0.0839)  loss_giou_1_unscaled: 0.3846 (0.4136)  cardinality_error_1_unscaled: 291.1667 (289.8970)  loss_ce_2_unscaled: 0.6287 (0.6341)  loss_bbox_2_unscaled: 0.0776 (0.0818)  loss_giou_2_unscaled: 0.3789 (0.4041)  cardinality_error_2_unscaled: 290.5000 (289.4154)  loss_ce_3_unscaled: 0.6207 (0.6315)  loss_bbox_3_unscaled: 0.0795 (0.0815)  loss_giou_3_unscaled: 0.3754 (0.4022)  cardinality_error_3_unscaled: 290.3333 (289.2020)  loss_ce_4_unscaled: 0.5938 (0.6290)  loss_bbox_4_unscaled: 0.0782 (0.0819)  loss_giou_4_unscaled: 0.3747 (0.4030)  cardinality_error_4_unscaled: 289.8333 (288.1571)  time: 3.0230  data: 0.0319  max mem: 8138\n",
            "Test:  [200/417]  eta: 0:11:03  class_error: 43.64  loss: 15.5911 (15.0523)  loss_ce: 1.2357 (1.2516)  loss_bbox: 0.4209 (0.4100)  loss_giou: 0.7829 (0.8055)  loss_ce_0: 1.2685 (1.2635)  loss_bbox_0: 0.4756 (0.4751)  loss_giou_0: 0.8950 (0.9193)  loss_ce_1: 1.2327 (1.2410)  loss_bbox_1: 0.4449 (0.4211)  loss_giou_1: 0.8081 (0.8281)  loss_ce_2: 1.2574 (1.2666)  loss_bbox_2: 0.4313 (0.4106)  loss_giou_2: 0.7875 (0.8096)  loss_ce_3: 1.2564 (1.2611)  loss_bbox_3: 0.4240 (0.4089)  loss_giou_3: 0.7835 (0.8059)  loss_ce_4: 1.2556 (1.2562)  loss_bbox_4: 0.4236 (0.4110)  loss_giou_4: 0.7873 (0.8074)  loss_ce_unscaled: 0.6178 (0.6258)  class_error_unscaled: 42.4528 (43.3238)  loss_bbox_unscaled: 0.0842 (0.0820)  loss_giou_unscaled: 0.3915 (0.4027)  cardinality_error_unscaled: 289.9167 (289.4627)  loss_ce_0_unscaled: 0.6343 (0.6317)  loss_bbox_0_unscaled: 0.0951 (0.0950)  loss_giou_0_unscaled: 0.4475 (0.4597)  cardinality_error_0_unscaled: 289.7500 (289.4602)  loss_ce_1_unscaled: 0.6164 (0.6205)  loss_bbox_1_unscaled: 0.0890 (0.0842)  loss_giou_1_unscaled: 0.4040 (0.4140)  cardinality_error_1_unscaled: 290.2500 (289.8715)  loss_ce_2_unscaled: 0.6287 (0.6333)  loss_bbox_2_unscaled: 0.0863 (0.0821)  loss_giou_2_unscaled: 0.3937 (0.4048)  cardinality_error_2_unscaled: 289.0833 (289.3595)  loss_ce_3_unscaled: 0.6282 (0.6305)  loss_bbox_3_unscaled: 0.0848 (0.0818)  loss_giou_3_unscaled: 0.3917 (0.4030)  cardinality_error_3_unscaled: 289.4167 (289.1675)  loss_ce_4_unscaled: 0.6278 (0.6281)  loss_bbox_4_unscaled: 0.0847 (0.0822)  loss_giou_4_unscaled: 0.3936 (0.4037)  cardinality_error_4_unscaled: 289.5833 (288.1522)  time: 2.9830  data: 0.0306  max mem: 8138\n",
            "Test:  [210/417]  eta: 0:10:32  class_error: 41.89  loss: 15.1773 (15.1030)  loss_ce: 1.2409 (1.2561)  loss_bbox: 0.4209 (0.4107)  loss_giou: 0.7859 (0.8088)  loss_ce_0: 1.2361 (1.2686)  loss_bbox_0: 0.5138 (0.4752)  loss_giou_0: 0.9067 (0.9223)  loss_ce_1: 1.1963 (1.2452)  loss_bbox_1: 0.4449 (0.4216)  loss_giou_1: 0.8255 (0.8316)  loss_ce_2: 1.2353 (1.2715)  loss_bbox_2: 0.4313 (0.4114)  loss_giou_2: 0.7988 (0.8128)  loss_ce_3: 1.2460 (1.2655)  loss_bbox_3: 0.4240 (0.4096)  loss_giou_3: 0.7949 (0.8093)  loss_ce_4: 1.2346 (1.2605)  loss_bbox_4: 0.4236 (0.4116)  loss_giou_4: 0.7877 (0.8108)  loss_ce_unscaled: 0.6204 (0.6281)  class_error_unscaled: 43.6364 (43.5042)  loss_bbox_unscaled: 0.0842 (0.0821)  loss_giou_unscaled: 0.3929 (0.4044)  cardinality_error_unscaled: 289.4167 (289.4214)  loss_ce_0_unscaled: 0.6181 (0.6343)  loss_bbox_0_unscaled: 0.1028 (0.0950)  loss_giou_0_unscaled: 0.4533 (0.4611)  cardinality_error_0_unscaled: 289.0000 (289.3839)  loss_ce_1_unscaled: 0.5982 (0.6226)  loss_bbox_1_unscaled: 0.0890 (0.0843)  loss_giou_1_unscaled: 0.4128 (0.4158)  cardinality_error_1_unscaled: 289.7500 (289.8326)  loss_ce_2_unscaled: 0.6177 (0.6357)  loss_bbox_2_unscaled: 0.0863 (0.0823)  loss_giou_2_unscaled: 0.3994 (0.4064)  cardinality_error_2_unscaled: 288.9167 (289.2982)  loss_ce_3_unscaled: 0.6230 (0.6328)  loss_bbox_3_unscaled: 0.0848 (0.0819)  loss_giou_3_unscaled: 0.3974 (0.4047)  cardinality_error_3_unscaled: 289.0000 (289.1173)  loss_ce_4_unscaled: 0.6173 (0.6302)  loss_bbox_4_unscaled: 0.0847 (0.0823)  loss_giou_4_unscaled: 0.3939 (0.4054)  cardinality_error_4_unscaled: 288.4167 (288.0640)  time: 2.9662  data: 0.0287  max mem: 8138\n",
            "Test:  [220/417]  eta: 0:10:02  class_error: 36.45  loss: 15.6515 (15.1350)  loss_ce: 1.2618 (1.2569)  loss_bbox: 0.3908 (0.4110)  loss_giou: 0.8420 (0.8132)  loss_ce_0: 1.3280 (1.2698)  loss_bbox_0: 0.4632 (0.4745)  loss_giou_0: 0.9475 (0.9256)  loss_ce_1: 1.2640 (1.2464)  loss_bbox_1: 0.4230 (0.4217)  loss_giou_1: 0.8609 (0.8357)  loss_ce_2: 1.2700 (1.2728)  loss_bbox_2: 0.4141 (0.4116)  loss_giou_2: 0.8524 (0.8172)  loss_ce_3: 1.2568 (1.2667)  loss_bbox_3: 0.4018 (0.4098)  loss_giou_3: 0.8389 (0.8136)  loss_ce_4: 1.2703 (1.2616)  loss_bbox_4: 0.3998 (0.4119)  loss_giou_4: 0.8427 (0.8151)  loss_ce_unscaled: 0.6309 (0.6284)  class_error_unscaled: 43.7500 (43.5661)  loss_bbox_unscaled: 0.0782 (0.0822)  loss_giou_unscaled: 0.4210 (0.4066)  cardinality_error_unscaled: 289.8333 (289.4001)  loss_ce_0_unscaled: 0.6640 (0.6349)  loss_bbox_0_unscaled: 0.0926 (0.0949)  loss_giou_0_unscaled: 0.4738 (0.4628)  cardinality_error_0_unscaled: 289.2500 (289.3567)  loss_ce_1_unscaled: 0.6320 (0.6232)  loss_bbox_1_unscaled: 0.0846 (0.0843)  loss_giou_1_unscaled: 0.4304 (0.4178)  cardinality_error_1_unscaled: 290.0000 (289.8171)  loss_ce_2_unscaled: 0.6350 (0.6364)  loss_bbox_2_unscaled: 0.0828 (0.0823)  loss_giou_2_unscaled: 0.4262 (0.4086)  cardinality_error_2_unscaled: 289.6667 (289.2979)  loss_ce_3_unscaled: 0.6284 (0.6333)  loss_bbox_3_unscaled: 0.0804 (0.0820)  loss_giou_3_unscaled: 0.4195 (0.4068)  cardinality_error_3_unscaled: 289.0833 (289.0950)  loss_ce_4_unscaled: 0.6352 (0.6308)  loss_bbox_4_unscaled: 0.0800 (0.0824)  loss_giou_4_unscaled: 0.4214 (0.4075)  cardinality_error_4_unscaled: 288.2500 (288.0634)  time: 3.0324  data: 0.0295  max mem: 8138\n",
            "Test:  [230/417]  eta: 0:09:30  class_error: 36.25  loss: 14.4779 (15.1110)  loss_ce: 1.1845 (1.2546)  loss_bbox: 0.3754 (0.4095)  loss_giou: 0.8413 (0.8126)  loss_ce_0: 1.2423 (1.2681)  loss_bbox_0: 0.4324 (0.4735)  loss_giou_0: 0.9432 (0.9258)  loss_ce_1: 1.1764 (1.2440)  loss_bbox_1: 0.4040 (0.4206)  loss_giou_1: 0.8579 (0.8355)  loss_ce_2: 1.2166 (1.2708)  loss_bbox_2: 0.3843 (0.4103)  loss_giou_2: 0.8415 (0.8167)  loss_ce_3: 1.1848 (1.2643)  loss_bbox_3: 0.3767 (0.4083)  loss_giou_3: 0.8346 (0.8128)  loss_ce_4: 1.1703 (1.2586)  loss_bbox_4: 0.3825 (0.4104)  loss_giou_4: 0.8402 (0.8145)  loss_ce_unscaled: 0.5922 (0.6273)  class_error_unscaled: 40.4040 (43.5102)  loss_bbox_unscaled: 0.0751 (0.0819)  loss_giou_unscaled: 0.4206 (0.4063)  cardinality_error_unscaled: 289.9167 (289.3709)  loss_ce_0_unscaled: 0.6211 (0.6341)  loss_bbox_0_unscaled: 0.0865 (0.0947)  loss_giou_0_unscaled: 0.4716 (0.4629)  cardinality_error_0_unscaled: 289.2500 (289.2893)  loss_ce_1_unscaled: 0.5882 (0.6220)  loss_bbox_1_unscaled: 0.0808 (0.0841)  loss_giou_1_unscaled: 0.4289 (0.4177)  cardinality_error_1_unscaled: 290.0000 (289.7565)  loss_ce_2_unscaled: 0.6083 (0.6354)  loss_bbox_2_unscaled: 0.0769 (0.0821)  loss_giou_2_unscaled: 0.4207 (0.4084)  cardinality_error_2_unscaled: 289.6667 (289.2414)  loss_ce_3_unscaled: 0.5924 (0.6322)  loss_bbox_3_unscaled: 0.0753 (0.0817)  loss_giou_3_unscaled: 0.4173 (0.4064)  cardinality_error_3_unscaled: 289.5000 (289.0534)  loss_ce_4_unscaled: 0.5851 (0.6293)  loss_bbox_4_unscaled: 0.0765 (0.0821)  loss_giou_4_unscaled: 0.4201 (0.4073)  cardinality_error_4_unscaled: 288.3333 (288.0087)  time: 2.9883  data: 0.0295  max mem: 8138\n",
            "Test:  [240/417]  eta: 0:08:59  class_error: 43.69  loss: 14.4754 (15.0760)  loss_ce: 1.1356 (1.2503)  loss_bbox: 0.3810 (0.4094)  loss_giou: 0.7579 (0.8109)  loss_ce_0: 1.1642 (1.2641)  loss_bbox_0: 0.4656 (0.4737)  loss_giou_0: 0.9101 (0.9245)  loss_ce_1: 1.1764 (1.2404)  loss_bbox_1: 0.4040 (0.4203)  loss_giou_1: 0.7744 (0.8336)  loss_ce_2: 1.1849 (1.2665)  loss_bbox_2: 0.3859 (0.4102)  loss_giou_2: 0.7757 (0.8150)  loss_ce_3: 1.1517 (1.2602)  loss_bbox_3: 0.3767 (0.4084)  loss_giou_3: 0.7573 (0.8110)  loss_ce_4: 1.1623 (1.2542)  loss_bbox_4: 0.3825 (0.4105)  loss_giou_4: 0.7582 (0.8128)  loss_ce_unscaled: 0.5678 (0.6251)  class_error_unscaled: 36.2500 (43.1985)  loss_bbox_unscaled: 0.0762 (0.0819)  loss_giou_unscaled: 0.3789 (0.4055)  cardinality_error_unscaled: 289.7500 (289.3869)  loss_ce_0_unscaled: 0.5821 (0.6320)  loss_bbox_0_unscaled: 0.0931 (0.0947)  loss_giou_0_unscaled: 0.4551 (0.4623)  cardinality_error_0_unscaled: 289.1667 (289.2884)  loss_ce_1_unscaled: 0.5882 (0.6202)  loss_bbox_1_unscaled: 0.0808 (0.0841)  loss_giou_1_unscaled: 0.3872 (0.4168)  cardinality_error_1_unscaled: 289.1667 (289.7562)  loss_ce_2_unscaled: 0.5924 (0.6333)  loss_bbox_2_unscaled: 0.0772 (0.0820)  loss_giou_2_unscaled: 0.3878 (0.4075)  cardinality_error_2_unscaled: 289.2500 (289.2510)  loss_ce_3_unscaled: 0.5759 (0.6301)  loss_bbox_3_unscaled: 0.0753 (0.0817)  loss_giou_3_unscaled: 0.3786 (0.4055)  cardinality_error_3_unscaled: 289.4167 (289.0654)  loss_ce_4_unscaled: 0.5811 (0.6271)  loss_bbox_4_unscaled: 0.0765 (0.0821)  loss_giou_4_unscaled: 0.3791 (0.4064)  cardinality_error_4_unscaled: 288.4167 (288.0159)  time: 2.9828  data: 0.0296  max mem: 8138\n",
            "Test:  [250/417]  eta: 0:08:29  class_error: 33.96  loss: 13.9259 (15.0554)  loss_ce: 1.1148 (1.2487)  loss_bbox: 0.4011 (0.4084)  loss_giou: 0.7478 (0.8102)  loss_ce_0: 1.1642 (1.2623)  loss_bbox_0: 0.4792 (0.4727)  loss_giou_0: 0.8984 (0.9238)  loss_ce_1: 1.0874 (1.2383)  loss_bbox_1: 0.3953 (0.4191)  loss_giou_1: 0.7744 (0.8327)  loss_ce_2: 1.1292 (1.2652)  loss_bbox_2: 0.3942 (0.4093)  loss_giou_2: 0.7692 (0.8141)  loss_ce_3: 1.1075 (1.2584)  loss_bbox_3: 0.3936 (0.4075)  loss_giou_3: 0.7464 (0.8103)  loss_ce_4: 1.0825 (1.2530)  loss_bbox_4: 0.3865 (0.4095)  loss_giou_4: 0.7582 (0.8121)  loss_ce_unscaled: 0.5574 (0.6244)  class_error_unscaled: 34.7826 (43.1811)  loss_bbox_unscaled: 0.0802 (0.0817)  loss_giou_unscaled: 0.3739 (0.4051)  cardinality_error_unscaled: 290.6667 (289.4482)  loss_ce_0_unscaled: 0.5821 (0.6312)  loss_bbox_0_unscaled: 0.0958 (0.0945)  loss_giou_0_unscaled: 0.4492 (0.4619)  cardinality_error_0_unscaled: 290.5000 (289.3264)  loss_ce_1_unscaled: 0.5437 (0.6191)  loss_bbox_1_unscaled: 0.0791 (0.0838)  loss_giou_1_unscaled: 0.3872 (0.4163)  cardinality_error_1_unscaled: 290.7500 (289.7932)  loss_ce_2_unscaled: 0.5646 (0.6326)  loss_bbox_2_unscaled: 0.0788 (0.0819)  loss_giou_2_unscaled: 0.3846 (0.4071)  cardinality_error_2_unscaled: 290.2500 (289.3121)  loss_ce_3_unscaled: 0.5538 (0.6292)  loss_bbox_3_unscaled: 0.0787 (0.0815)  loss_giou_3_unscaled: 0.3732 (0.4051)  cardinality_error_3_unscaled: 290.5000 (289.1285)  loss_ce_4_unscaled: 0.5412 (0.6265)  loss_bbox_4_unscaled: 0.0773 (0.0819)  loss_giou_4_unscaled: 0.3791 (0.4060)  cardinality_error_4_unscaled: 289.5000 (288.0714)  time: 3.0872  data: 0.0310  max mem: 8138\n",
            "Test:  [260/417]  eta: 0:07:58  class_error: 65.00  loss: 14.5007 (15.0596)  loss_ce: 1.1577 (1.2486)  loss_bbox: 0.4098 (0.4092)  loss_giou: 0.7431 (0.8097)  loss_ce_0: 1.2082 (1.2642)  loss_bbox_0: 0.4782 (0.4735)  loss_giou_0: 0.8734 (0.9232)  loss_ce_1: 1.1767 (1.2389)  loss_bbox_1: 0.4147 (0.4202)  loss_giou_1: 0.7702 (0.8325)  loss_ce_2: 1.1956 (1.2656)  loss_bbox_2: 0.4083 (0.4101)  loss_giou_2: 0.7585 (0.8138)  loss_ce_3: 1.1383 (1.2580)  loss_bbox_3: 0.4112 (0.4082)  loss_giou_3: 0.7464 (0.8099)  loss_ce_4: 1.1428 (1.2526)  loss_bbox_4: 0.4116 (0.4102)  loss_giou_4: 0.7554 (0.8116)  loss_ce_unscaled: 0.5789 (0.6243)  class_error_unscaled: 42.6667 (43.2742)  loss_bbox_unscaled: 0.0820 (0.0818)  loss_giou_unscaled: 0.3715 (0.4048)  cardinality_error_unscaled: 291.5833 (289.4611)  loss_ce_0_unscaled: 0.6041 (0.6321)  loss_bbox_0_unscaled: 0.0956 (0.0947)  loss_giou_0_unscaled: 0.4367 (0.4616)  cardinality_error_0_unscaled: 290.1667 (289.3065)  loss_ce_1_unscaled: 0.5883 (0.6194)  loss_bbox_1_unscaled: 0.0829 (0.0840)  loss_giou_1_unscaled: 0.3851 (0.4162)  cardinality_error_1_unscaled: 291.0000 (289.7794)  loss_ce_2_unscaled: 0.5978 (0.6328)  loss_bbox_2_unscaled: 0.0817 (0.0820)  loss_giou_2_unscaled: 0.3792 (0.4069)  cardinality_error_2_unscaled: 291.0000 (289.3049)  loss_ce_3_unscaled: 0.5691 (0.6290)  loss_bbox_3_unscaled: 0.0822 (0.0816)  loss_giou_3_unscaled: 0.3732 (0.4049)  cardinality_error_3_unscaled: 290.9167 (289.1415)  loss_ce_4_unscaled: 0.5714 (0.6263)  loss_bbox_4_unscaled: 0.0823 (0.0820)  loss_giou_4_unscaled: 0.3777 (0.4058)  cardinality_error_4_unscaled: 289.6667 (288.0802)  time: 3.0553  data: 0.0312  max mem: 8138\n",
            "Test:  [270/417]  eta: 0:07:29  class_error: 30.77  loss: 14.5007 (15.0032)  loss_ce: 1.1577 (1.2436)  loss_bbox: 0.4098 (0.4081)  loss_giou: 0.7211 (0.8060)  loss_ce_0: 1.2131 (1.2601)  loss_bbox_0: 0.4782 (0.4726)  loss_giou_0: 0.8450 (0.9198)  loss_ce_1: 1.1721 (1.2348)  loss_bbox_1: 0.4147 (0.4188)  loss_giou_1: 0.7569 (0.8288)  loss_ce_2: 1.1956 (1.2607)  loss_bbox_2: 0.4104 (0.4089)  loss_giou_2: 0.7293 (0.8102)  loss_ce_3: 1.1383 (1.2528)  loss_bbox_3: 0.4112 (0.4070)  loss_giou_3: 0.7273 (0.8062)  loss_ce_4: 1.1428 (1.2478)  loss_bbox_4: 0.4116 (0.4090)  loss_giou_4: 0.7253 (0.8079)  loss_ce_unscaled: 0.5789 (0.6218)  class_error_unscaled: 42.6667 (43.1040)  loss_bbox_unscaled: 0.0820 (0.0816)  loss_giou_unscaled: 0.3605 (0.4030)  cardinality_error_unscaled: 288.3333 (289.3401)  loss_ce_0_unscaled: 0.6065 (0.6301)  loss_bbox_0_unscaled: 0.0956 (0.0945)  loss_giou_0_unscaled: 0.4225 (0.4599)  cardinality_error_0_unscaled: 287.9167 (289.1845)  loss_ce_1_unscaled: 0.5860 (0.6174)  loss_bbox_1_unscaled: 0.0829 (0.0838)  loss_giou_1_unscaled: 0.3784 (0.4144)  cardinality_error_1_unscaled: 288.4167 (289.6571)  loss_ce_2_unscaled: 0.5978 (0.6304)  loss_bbox_2_unscaled: 0.0821 (0.0818)  loss_giou_2_unscaled: 0.3646 (0.4051)  cardinality_error_2_unscaled: 288.2500 (289.2045)  loss_ce_3_unscaled: 0.5691 (0.6264)  loss_bbox_3_unscaled: 0.0822 (0.0814)  loss_giou_3_unscaled: 0.3636 (0.4031)  cardinality_error_3_unscaled: 288.4167 (289.0314)  loss_ce_4_unscaled: 0.5714 (0.6239)  loss_bbox_4_unscaled: 0.0823 (0.0818)  loss_giou_4_unscaled: 0.3627 (0.4039)  cardinality_error_4_unscaled: 288.1667 (287.9613)  time: 3.1401  data: 0.0317  max mem: 8138\n",
            "Test:  [280/417]  eta: 0:06:58  class_error: 45.52  loss: 14.7896 (15.0185)  loss_ce: 1.1820 (1.2430)  loss_bbox: 0.4294 (0.4089)  loss_giou: 0.8141 (0.8086)  loss_ce_0: 1.1920 (1.2593)  loss_bbox_0: 0.4775 (0.4726)  loss_giou_0: 0.9454 (0.9226)  loss_ce_1: 1.1668 (1.2342)  loss_bbox_1: 0.4202 (0.4194)  loss_giou_1: 0.8429 (0.8317)  loss_ce_2: 1.1808 (1.2600)  loss_bbox_2: 0.4203 (0.4096)  loss_giou_2: 0.8296 (0.8128)  loss_ce_3: 1.1810 (1.2518)  loss_bbox_3: 0.4157 (0.4078)  loss_giou_3: 0.8150 (0.8088)  loss_ce_4: 1.1878 (1.2470)  loss_bbox_4: 0.4175 (0.4099)  loss_giou_4: 0.8120 (0.8104)  loss_ce_unscaled: 0.5910 (0.6215)  class_error_unscaled: 42.5197 (43.0805)  loss_bbox_unscaled: 0.0859 (0.0818)  loss_giou_unscaled: 0.4070 (0.4043)  cardinality_error_unscaled: 288.2500 (289.2877)  loss_ce_0_unscaled: 0.5960 (0.6296)  loss_bbox_0_unscaled: 0.0955 (0.0945)  loss_giou_0_unscaled: 0.4727 (0.4613)  cardinality_error_0_unscaled: 287.6667 (289.1430)  loss_ce_1_unscaled: 0.5834 (0.6171)  loss_bbox_1_unscaled: 0.0840 (0.0839)  loss_giou_1_unscaled: 0.4214 (0.4158)  cardinality_error_1_unscaled: 288.3333 (289.6272)  loss_ce_2_unscaled: 0.5904 (0.6300)  loss_bbox_2_unscaled: 0.0841 (0.0819)  loss_giou_2_unscaled: 0.4148 (0.4064)  cardinality_error_2_unscaled: 287.2500 (289.1640)  loss_ce_3_unscaled: 0.5905 (0.6259)  loss_bbox_3_unscaled: 0.0831 (0.0816)  loss_giou_3_unscaled: 0.4075 (0.4044)  cardinality_error_3_unscaled: 287.5000 (289.0033)  loss_ce_4_unscaled: 0.5939 (0.6235)  loss_bbox_4_unscaled: 0.0835 (0.0820)  loss_giou_4_unscaled: 0.4060 (0.4052)  cardinality_error_4_unscaled: 286.5833 (287.9113)  time: 3.0953  data: 0.0311  max mem: 8138\n",
            "Test:  [290/417]  eta: 0:06:27  class_error: 30.30  loss: 15.4095 (15.0383)  loss_ce: 1.2142 (1.2442)  loss_bbox: 0.4294 (0.4102)  loss_giou: 0.8331 (0.8091)  loss_ce_0: 1.2673 (1.2610)  loss_bbox_0: 0.4770 (0.4739)  loss_giou_0: 0.9543 (0.9228)  loss_ce_1: 1.2582 (1.2359)  loss_bbox_1: 0.4361 (0.4209)  loss_giou_1: 0.8661 (0.8320)  loss_ce_2: 1.2606 (1.2619)  loss_bbox_2: 0.4306 (0.4109)  loss_giou_2: 0.8300 (0.8133)  loss_ce_3: 1.2031 (1.2532)  loss_bbox_3: 0.4300 (0.4091)  loss_giou_3: 0.8298 (0.8092)  loss_ce_4: 1.2114 (1.2483)  loss_bbox_4: 0.4325 (0.4113)  loss_giou_4: 0.8332 (0.8108)  loss_ce_unscaled: 0.6071 (0.6221)  class_error_unscaled: 41.6667 (43.0931)  loss_bbox_unscaled: 0.0859 (0.0820)  loss_giou_unscaled: 0.4165 (0.4046)  cardinality_error_unscaled: 290.3333 (289.3780)  loss_ce_0_unscaled: 0.6337 (0.6305)  loss_bbox_0_unscaled: 0.0954 (0.0948)  loss_giou_0_unscaled: 0.4772 (0.4614)  cardinality_error_0_unscaled: 289.1667 (289.2185)  loss_ce_1_unscaled: 0.6291 (0.6179)  loss_bbox_1_unscaled: 0.0872 (0.0842)  loss_giou_1_unscaled: 0.4331 (0.4160)  cardinality_error_1_unscaled: 290.5833 (289.7202)  loss_ce_2_unscaled: 0.6303 (0.6309)  loss_bbox_2_unscaled: 0.0861 (0.0822)  loss_giou_2_unscaled: 0.4150 (0.4067)  cardinality_error_2_unscaled: 289.9167 (289.2555)  loss_ce_3_unscaled: 0.6016 (0.6266)  loss_bbox_3_unscaled: 0.0860 (0.0818)  loss_giou_3_unscaled: 0.4149 (0.4046)  cardinality_error_3_unscaled: 289.9167 (289.0879)  loss_ce_4_unscaled: 0.6057 (0.6241)  loss_bbox_4_unscaled: 0.0865 (0.0823)  loss_giou_4_unscaled: 0.4166 (0.4054)  cardinality_error_4_unscaled: 289.2500 (288.0095)  time: 2.9497  data: 0.0292  max mem: 8138\n",
            "Test:  [300/417]  eta: 0:05:56  class_error: 45.19  loss: 15.3724 (15.0776)  loss_ce: 1.2653 (1.2471)  loss_bbox: 0.4230 (0.4111)  loss_giou: 0.8118 (0.8118)  loss_ce_0: 1.3169 (1.2637)  loss_bbox_0: 0.4770 (0.4749)  loss_giou_0: 0.9169 (0.9251)  loss_ce_1: 1.2611 (1.2387)  loss_bbox_1: 0.4263 (0.4219)  loss_giou_1: 0.8314 (0.8348)  loss_ce_2: 1.2690 (1.2652)  loss_bbox_2: 0.4201 (0.4119)  loss_giou_2: 0.8111 (0.8160)  loss_ce_3: 1.2698 (1.2562)  loss_bbox_3: 0.4213 (0.4101)  loss_giou_3: 0.8103 (0.8118)  loss_ce_4: 1.2739 (1.2518)  loss_bbox_4: 0.4247 (0.4121)  loss_giou_4: 0.8155 (0.8134)  loss_ce_unscaled: 0.6327 (0.6236)  class_error_unscaled: 45.1923 (43.2113)  loss_bbox_unscaled: 0.0846 (0.0822)  loss_giou_unscaled: 0.4059 (0.4059)  cardinality_error_unscaled: 290.7500 (289.3931)  loss_ce_0_unscaled: 0.6585 (0.6319)  loss_bbox_0_unscaled: 0.0954 (0.0950)  loss_giou_0_unscaled: 0.4584 (0.4626)  cardinality_error_0_unscaled: 290.6667 (289.2373)  loss_ce_1_unscaled: 0.6306 (0.6194)  loss_bbox_1_unscaled: 0.0853 (0.0844)  loss_giou_1_unscaled: 0.4157 (0.4174)  cardinality_error_1_unscaled: 291.1667 (289.7331)  loss_ce_2_unscaled: 0.6345 (0.6326)  loss_bbox_2_unscaled: 0.0840 (0.0824)  loss_giou_2_unscaled: 0.4055 (0.4080)  cardinality_error_2_unscaled: 291.1667 (289.2832)  loss_ce_3_unscaled: 0.6349 (0.6281)  loss_bbox_3_unscaled: 0.0843 (0.0820)  loss_giou_3_unscaled: 0.4051 (0.4059)  cardinality_error_3_unscaled: 290.5833 (289.1246)  loss_ce_4_unscaled: 0.6370 (0.6259)  loss_bbox_4_unscaled: 0.0849 (0.0824)  loss_giou_4_unscaled: 0.4078 (0.4067)  cardinality_error_4_unscaled: 290.5000 (288.0421)  time: 2.9751  data: 0.0288  max mem: 8138\n",
            "Test:  [310/417]  eta: 0:05:26  class_error: 56.52  loss: 15.1785 (15.0731)  loss_ce: 1.3153 (1.2474)  loss_bbox: 0.4038 (0.4106)  loss_giou: 0.8426 (0.8109)  loss_ce_0: 1.2917 (1.2648)  loss_bbox_0: 0.4920 (0.4748)  loss_giou_0: 0.9314 (0.9246)  loss_ce_1: 1.2503 (1.2392)  loss_bbox_1: 0.4196 (0.4215)  loss_giou_1: 0.8602 (0.8338)  loss_ce_2: 1.2709 (1.2651)  loss_bbox_2: 0.4091 (0.4115)  loss_giou_2: 0.8426 (0.8152)  loss_ce_3: 1.3173 (1.2567)  loss_bbox_3: 0.4066 (0.4097)  loss_giou_3: 0.8416 (0.8108)  loss_ce_4: 1.2959 (1.2523)  loss_bbox_4: 0.4063 (0.4116)  loss_giou_4: 0.8402 (0.8124)  loss_ce_unscaled: 0.6577 (0.6237)  class_error_unscaled: 42.4242 (43.2393)  loss_bbox_unscaled: 0.0808 (0.0821)  loss_giou_unscaled: 0.4213 (0.4055)  cardinality_error_unscaled: 290.8333 (289.4062)  loss_ce_0_unscaled: 0.6459 (0.6324)  loss_bbox_0_unscaled: 0.0984 (0.0950)  loss_giou_0_unscaled: 0.4657 (0.4623)  cardinality_error_0_unscaled: 290.6667 (289.2688)  loss_ce_1_unscaled: 0.6252 (0.6196)  loss_bbox_1_unscaled: 0.0839 (0.0843)  loss_giou_1_unscaled: 0.4301 (0.4169)  cardinality_error_1_unscaled: 291.0000 (289.7473)  loss_ce_2_unscaled: 0.6354 (0.6326)  loss_bbox_2_unscaled: 0.0818 (0.0823)  loss_giou_2_unscaled: 0.4213 (0.4076)  cardinality_error_2_unscaled: 291.1667 (289.3189)  loss_ce_3_unscaled: 0.6587 (0.6284)  loss_bbox_3_unscaled: 0.0813 (0.0819)  loss_giou_3_unscaled: 0.4208 (0.4054)  cardinality_error_3_unscaled: 290.5833 (289.1471)  loss_ce_4_unscaled: 0.6479 (0.6262)  loss_bbox_4_unscaled: 0.0813 (0.0823)  loss_giou_4_unscaled: 0.4201 (0.4062)  cardinality_error_4_unscaled: 290.5000 (288.0876)  time: 3.0234  data: 0.0300  max mem: 8138\n",
            "Test:  [320/417]  eta: 0:04:55  class_error: 28.36  loss: 14.6727 (15.0601)  loss_ce: 1.2593 (1.2467)  loss_bbox: 0.3775 (0.4097)  loss_giou: 0.7564 (0.8101)  loss_ce_0: 1.2917 (1.2642)  loss_bbox_0: 0.4657 (0.4741)  loss_giou_0: 0.9008 (0.9245)  loss_ce_1: 1.2503 (1.2384)  loss_bbox_1: 0.3963 (0.4205)  loss_giou_1: 0.7910 (0.8333)  loss_ce_2: 1.2114 (1.2643)  loss_bbox_2: 0.3849 (0.4105)  loss_giou_2: 0.7686 (0.8146)  loss_ce_3: 1.2429 (1.2561)  loss_bbox_3: 0.3736 (0.4087)  loss_giou_3: 0.7585 (0.8101)  loss_ce_4: 1.2723 (1.2517)  loss_bbox_4: 0.3790 (0.4108)  loss_giou_4: 0.7661 (0.8117)  loss_ce_unscaled: 0.6296 (0.6233)  class_error_unscaled: 40.9639 (43.2605)  loss_bbox_unscaled: 0.0755 (0.0819)  loss_giou_unscaled: 0.3782 (0.4051)  cardinality_error_unscaled: 290.7500 (289.4242)  loss_ce_0_unscaled: 0.6459 (0.6321)  loss_bbox_0_unscaled: 0.0931 (0.0948)  loss_giou_0_unscaled: 0.4504 (0.4623)  cardinality_error_0_unscaled: 289.4167 (289.2697)  loss_ce_1_unscaled: 0.6252 (0.6192)  loss_bbox_1_unscaled: 0.0793 (0.0841)  loss_giou_1_unscaled: 0.3955 (0.4166)  cardinality_error_1_unscaled: 290.8333 (289.7479)  loss_ce_2_unscaled: 0.6057 (0.6321)  loss_bbox_2_unscaled: 0.0770 (0.0821)  loss_giou_2_unscaled: 0.3843 (0.4073)  cardinality_error_2_unscaled: 291.1667 (289.3318)  loss_ce_3_unscaled: 0.6214 (0.6281)  loss_bbox_3_unscaled: 0.0747 (0.0817)  loss_giou_3_unscaled: 0.3793 (0.4051)  cardinality_error_3_unscaled: 290.0000 (289.1628)  loss_ce_4_unscaled: 0.6362 (0.6258)  loss_bbox_4_unscaled: 0.0758 (0.0822)  loss_giou_4_unscaled: 0.3831 (0.4058)  cardinality_error_4_unscaled: 289.4167 (288.1057)  time: 3.0091  data: 0.0311  max mem: 8138\n",
            "Test:  [330/417]  eta: 0:04:24  class_error: 48.15  loss: 15.3120 (15.0668)  loss_ce: 1.2593 (1.2465)  loss_bbox: 0.4031 (0.4114)  loss_giou: 0.7661 (0.8094)  loss_ce_0: 1.2845 (1.2642)  loss_bbox_0: 0.4774 (0.4760)  loss_giou_0: 0.9255 (0.9235)  loss_ce_1: 1.2658 (1.2385)  loss_bbox_1: 0.4144 (0.4227)  loss_giou_1: 0.8357 (0.8328)  loss_ce_2: 1.2453 (1.2645)  loss_bbox_2: 0.4016 (0.4125)  loss_giou_2: 0.7850 (0.8139)  loss_ce_3: 1.2429 (1.2560)  loss_bbox_3: 0.3919 (0.4104)  loss_giou_3: 0.7719 (0.8095)  loss_ce_4: 1.2543 (1.2516)  loss_bbox_4: 0.4005 (0.4124)  loss_giou_4: 0.7721 (0.8109)  loss_ce_unscaled: 0.6296 (0.6233)  class_error_unscaled: 43.7500 (43.2384)  loss_bbox_unscaled: 0.0806 (0.0823)  loss_giou_unscaled: 0.3831 (0.4047)  cardinality_error_unscaled: 290.1667 (289.4872)  loss_ce_0_unscaled: 0.6423 (0.6321)  loss_bbox_0_unscaled: 0.0955 (0.0952)  loss_giou_0_unscaled: 0.4627 (0.4618)  cardinality_error_0_unscaled: 289.4167 (289.3313)  loss_ce_1_unscaled: 0.6329 (0.6192)  loss_bbox_1_unscaled: 0.0829 (0.0845)  loss_giou_1_unscaled: 0.4179 (0.4164)  cardinality_error_1_unscaled: 290.0000 (289.8046)  loss_ce_2_unscaled: 0.6227 (0.6322)  loss_bbox_2_unscaled: 0.0803 (0.0825)  loss_giou_2_unscaled: 0.3925 (0.4070)  cardinality_error_2_unscaled: 290.0000 (289.3986)  loss_ce_3_unscaled: 0.6214 (0.6280)  loss_bbox_3_unscaled: 0.0784 (0.0821)  loss_giou_3_unscaled: 0.3859 (0.4047)  cardinality_error_3_unscaled: 289.5833 (289.2347)  loss_ce_4_unscaled: 0.6272 (0.6258)  loss_bbox_4_unscaled: 0.0801 (0.0825)  loss_giou_4_unscaled: 0.3860 (0.4055)  cardinality_error_4_unscaled: 289.2500 (288.1846)  time: 2.8832  data: 0.0291  max mem: 8138\n",
            "Test:  [340/417]  eta: 0:03:53  class_error: 23.85  loss: 15.1022 (15.0524)  loss_ce: 1.1597 (1.2444)  loss_bbox: 0.4383 (0.4115)  loss_giou: 0.8255 (0.8090)  loss_ce_0: 1.1763 (1.2626)  loss_bbox_0: 0.5006 (0.4759)  loss_giou_0: 0.9385 (0.9230)  loss_ce_1: 1.1726 (1.2367)  loss_bbox_1: 0.4458 (0.4228)  loss_giou_1: 0.8656 (0.8324)  loss_ce_2: 1.1914 (1.2622)  loss_bbox_2: 0.4425 (0.4125)  loss_giou_2: 0.8279 (0.8134)  loss_ce_3: 1.1908 (1.2539)  loss_bbox_3: 0.4415 (0.4106)  loss_giou_3: 0.8154 (0.8090)  loss_ce_4: 1.1816 (1.2496)  loss_bbox_4: 0.4412 (0.4126)  loss_giou_4: 0.8163 (0.8104)  loss_ce_unscaled: 0.5799 (0.6222)  class_error_unscaled: 38.8060 (43.1560)  loss_bbox_unscaled: 0.0877 (0.0823)  loss_giou_unscaled: 0.4127 (0.4045)  cardinality_error_unscaled: 290.1667 (289.4504)  loss_ce_0_unscaled: 0.5881 (0.6313)  loss_bbox_0_unscaled: 0.1001 (0.0952)  loss_giou_0_unscaled: 0.4692 (0.4615)  cardinality_error_0_unscaled: 290.1667 (289.2989)  loss_ce_1_unscaled: 0.5863 (0.6183)  loss_bbox_1_unscaled: 0.0892 (0.0846)  loss_giou_1_unscaled: 0.4328 (0.4162)  cardinality_error_1_unscaled: 289.9167 (289.7757)  loss_ce_2_unscaled: 0.5957 (0.6311)  loss_bbox_2_unscaled: 0.0885 (0.0825)  loss_giou_2_unscaled: 0.4140 (0.4067)  cardinality_error_2_unscaled: 290.0000 (289.3649)  loss_ce_3_unscaled: 0.5954 (0.6270)  loss_bbox_3_unscaled: 0.0883 (0.0821)  loss_giou_3_unscaled: 0.4077 (0.4045)  cardinality_error_3_unscaled: 289.5833 (289.1977)  loss_ce_4_unscaled: 0.5908 (0.6248)  loss_bbox_4_unscaled: 0.0882 (0.0825)  loss_giou_4_unscaled: 0.4081 (0.4052)  cardinality_error_4_unscaled: 288.7500 (288.1545)  time: 2.8533  data: 0.0274  max mem: 8138\n",
            "Test:  [350/417]  eta: 0:03:23  class_error: 31.58  loss: 13.2304 (15.0187)  loss_ce: 1.0754 (1.2394)  loss_bbox: 0.4215 (0.4113)  loss_giou: 0.7913 (0.8084)  loss_ce_0: 1.1310 (1.2583)  loss_bbox_0: 0.4602 (0.4754)  loss_giou_0: 0.9199 (0.9223)  loss_ce_1: 1.0852 (1.2321)  loss_bbox_1: 0.4354 (0.4223)  loss_giou_1: 0.8548 (0.8319)  loss_ce_2: 1.0798 (1.2575)  loss_bbox_2: 0.4211 (0.4123)  loss_giou_2: 0.8018 (0.8130)  loss_ce_3: 1.0882 (1.2489)  loss_bbox_3: 0.4186 (0.4104)  loss_giou_3: 0.7928 (0.8085)  loss_ce_4: 1.0615 (1.2443)  loss_bbox_4: 0.4258 (0.4124)  loss_giou_4: 0.7913 (0.8099)  loss_ce_unscaled: 0.5377 (0.6197)  class_error_unscaled: 35.0516 (42.9519)  loss_bbox_unscaled: 0.0843 (0.0823)  loss_giou_unscaled: 0.3957 (0.4042)  cardinality_error_unscaled: 289.4167 (289.4397)  loss_ce_0_unscaled: 0.5655 (0.6292)  loss_bbox_0_unscaled: 0.0920 (0.0951)  loss_giou_0_unscaled: 0.4599 (0.4612)  cardinality_error_0_unscaled: 289.2500 (289.2906)  loss_ce_1_unscaled: 0.5426 (0.6160)  loss_bbox_1_unscaled: 0.0871 (0.0845)  loss_giou_1_unscaled: 0.4274 (0.4160)  cardinality_error_1_unscaled: 289.7500 (289.7647)  loss_ce_2_unscaled: 0.5399 (0.6288)  loss_bbox_2_unscaled: 0.0842 (0.0825)  loss_giou_2_unscaled: 0.4009 (0.4065)  cardinality_error_2_unscaled: 289.7500 (289.3609)  loss_ce_3_unscaled: 0.5441 (0.6244)  loss_bbox_3_unscaled: 0.0837 (0.0821)  loss_giou_3_unscaled: 0.3964 (0.4042)  cardinality_error_3_unscaled: 289.5000 (289.2040)  loss_ce_4_unscaled: 0.5307 (0.6222)  loss_bbox_4_unscaled: 0.0852 (0.0825)  loss_giou_4_unscaled: 0.3957 (0.4050)  cardinality_error_4_unscaled: 288.5000 (288.1629)  time: 2.9394  data: 0.0280  max mem: 8138\n",
            "Test:  [360/417]  eta: 0:02:53  class_error: 52.08  loss: 13.7987 (15.0146)  loss_ce: 1.1061 (1.2388)  loss_bbox: 0.4057 (0.4113)  loss_giou: 0.7772 (0.8081)  loss_ce_0: 1.1732 (1.2587)  loss_bbox_0: 0.4593 (0.4754)  loss_giou_0: 0.9199 (0.9220)  loss_ce_1: 1.1145 (1.2316)  loss_bbox_1: 0.4101 (0.4221)  loss_giou_1: 0.8080 (0.8319)  loss_ce_2: 1.1337 (1.2569)  loss_bbox_2: 0.4109 (0.4124)  loss_giou_2: 0.7707 (0.8128)  loss_ce_3: 1.1144 (1.2481)  loss_bbox_3: 0.4067 (0.4105)  loss_giou_3: 0.7702 (0.8081)  loss_ce_4: 1.1165 (1.2439)  loss_bbox_4: 0.3986 (0.4124)  loss_giou_4: 0.7812 (0.8096)  loss_ce_unscaled: 0.5531 (0.6194)  class_error_unscaled: 40.4412 (42.9779)  loss_bbox_unscaled: 0.0811 (0.0823)  loss_giou_unscaled: 0.3886 (0.4040)  cardinality_error_unscaled: 289.7500 (289.4072)  loss_ce_0_unscaled: 0.5866 (0.6293)  loss_bbox_0_unscaled: 0.0919 (0.0951)  loss_giou_0_unscaled: 0.4599 (0.4610)  cardinality_error_0_unscaled: 289.8333 (289.2669)  loss_ce_1_unscaled: 0.5573 (0.6158)  loss_bbox_1_unscaled: 0.0820 (0.0844)  loss_giou_1_unscaled: 0.4040 (0.4159)  cardinality_error_1_unscaled: 290.0000 (289.7235)  loss_ce_2_unscaled: 0.5668 (0.6284)  loss_bbox_2_unscaled: 0.0822 (0.0825)  loss_giou_2_unscaled: 0.3853 (0.4064)  cardinality_error_2_unscaled: 289.7500 (289.3110)  loss_ce_3_unscaled: 0.5572 (0.6241)  loss_bbox_3_unscaled: 0.0813 (0.0821)  loss_giou_3_unscaled: 0.3851 (0.4041)  cardinality_error_3_unscaled: 289.6667 (289.1570)  loss_ce_4_unscaled: 0.5583 (0.6219)  loss_bbox_4_unscaled: 0.0797 (0.0825)  loss_giou_4_unscaled: 0.3906 (0.4048)  cardinality_error_4_unscaled: 288.9167 (288.1297)  time: 3.0555  data: 0.0294  max mem: 8138\n",
            "Test:  [370/417]  eta: 0:02:22  class_error: 34.72  loss: 15.0710 (15.0389)  loss_ce: 1.2661 (1.2409)  loss_bbox: 0.4057 (0.4121)  loss_giou: 0.8689 (0.8093)  loss_ce_0: 1.2954 (1.2611)  loss_bbox_0: 0.4590 (0.4758)  loss_giou_0: 0.9699 (0.9230)  loss_ce_1: 1.2560 (1.2339)  loss_bbox_1: 0.3965 (0.4226)  loss_giou_1: 0.8799 (0.8329)  loss_ce_2: 1.2910 (1.2593)  loss_bbox_2: 0.3932 (0.4131)  loss_giou_2: 0.8647 (0.8138)  loss_ce_3: 1.2860 (1.2505)  loss_bbox_3: 0.4022 (0.4113)  loss_giou_3: 0.8681 (0.8092)  loss_ce_4: 1.2562 (1.2464)  loss_bbox_4: 0.3986 (0.4132)  loss_giou_4: 0.8675 (0.8106)  loss_ce_unscaled: 0.6331 (0.6205)  class_error_unscaled: 44.7368 (42.9968)  loss_bbox_unscaled: 0.0811 (0.0824)  loss_giou_unscaled: 0.4344 (0.4046)  cardinality_error_unscaled: 289.7500 (289.3947)  loss_ce_0_unscaled: 0.6477 (0.6305)  loss_bbox_0_unscaled: 0.0918 (0.0952)  loss_giou_0_unscaled: 0.4849 (0.4615)  cardinality_error_0_unscaled: 289.5000 (289.2635)  loss_ce_1_unscaled: 0.6280 (0.6169)  loss_bbox_1_unscaled: 0.0793 (0.0845)  loss_giou_1_unscaled: 0.4399 (0.4164)  cardinality_error_1_unscaled: 289.9167 (289.7107)  loss_ce_2_unscaled: 0.6455 (0.6296)  loss_bbox_2_unscaled: 0.0786 (0.0826)  loss_giou_2_unscaled: 0.4324 (0.4069)  cardinality_error_2_unscaled: 288.8333 (289.2893)  loss_ce_3_unscaled: 0.6430 (0.6252)  loss_bbox_3_unscaled: 0.0804 (0.0823)  loss_giou_3_unscaled: 0.4340 (0.4046)  cardinality_error_3_unscaled: 289.6667 (289.1545)  loss_ce_4_unscaled: 0.6281 (0.6232)  loss_bbox_4_unscaled: 0.0797 (0.0826)  loss_giou_4_unscaled: 0.4337 (0.4053)  cardinality_error_4_unscaled: 288.0833 (288.1103)  time: 3.0440  data: 0.0297  max mem: 8138\n",
            "Test:  [380/417]  eta: 0:01:52  class_error: 25.17  loss: 15.1209 (15.0355)  loss_ce: 1.2661 (1.2402)  loss_bbox: 0.3979 (0.4122)  loss_giou: 0.8417 (0.8094)  loss_ce_0: 1.2954 (1.2608)  loss_bbox_0: 0.4590 (0.4755)  loss_giou_0: 0.9537 (0.9231)  loss_ce_1: 1.2560 (1.2335)  loss_bbox_1: 0.3965 (0.4226)  loss_giou_1: 0.8597 (0.8329)  loss_ce_2: 1.2910 (1.2589)  loss_bbox_2: 0.3912 (0.4130)  loss_giou_2: 0.8380 (0.8139)  loss_ce_3: 1.2775 (1.2496)  loss_bbox_3: 0.3898 (0.4113)  loss_giou_3: 0.8412 (0.8094)  loss_ce_4: 1.2562 (1.2455)  loss_bbox_4: 0.3904 (0.4133)  loss_giou_4: 0.8423 (0.8108)  loss_ce_unscaled: 0.6331 (0.6201)  class_error_unscaled: 41.0714 (42.8899)  loss_bbox_unscaled: 0.0796 (0.0824)  loss_giou_unscaled: 0.4209 (0.4047)  cardinality_error_unscaled: 290.6667 (289.4281)  loss_ce_0_unscaled: 0.6477 (0.6304)  loss_bbox_0_unscaled: 0.0918 (0.0951)  loss_giou_0_unscaled: 0.4768 (0.4615)  cardinality_error_0_unscaled: 289.5000 (289.2841)  loss_ce_1_unscaled: 0.6280 (0.6167)  loss_bbox_1_unscaled: 0.0793 (0.0845)  loss_giou_1_unscaled: 0.4298 (0.4164)  cardinality_error_1_unscaled: 290.0833 (289.7334)  loss_ce_2_unscaled: 0.6455 (0.6294)  loss_bbox_2_unscaled: 0.0782 (0.0826)  loss_giou_2_unscaled: 0.4190 (0.4069)  cardinality_error_2_unscaled: 290.0000 (289.3193)  loss_ce_3_unscaled: 0.6387 (0.6248)  loss_bbox_3_unscaled: 0.0780 (0.0823)  loss_giou_3_unscaled: 0.4206 (0.4047)  cardinality_error_3_unscaled: 290.8333 (289.1934)  loss_ce_4_unscaled: 0.6281 (0.6227)  loss_bbox_4_unscaled: 0.0781 (0.0827)  loss_giou_4_unscaled: 0.4212 (0.4054)  cardinality_error_4_unscaled: 290.4167 (288.1627)  time: 3.0081  data: 0.0289  max mem: 8138\n",
            "Test:  [390/417]  eta: 0:01:21  class_error: 42.53  loss: 14.2011 (15.0134)  loss_ce: 1.1316 (1.2377)  loss_bbox: 0.3903 (0.4113)  loss_giou: 0.8147 (0.8089)  loss_ce_0: 1.2005 (1.2588)  loss_bbox_0: 0.4493 (0.4748)  loss_giou_0: 0.9272 (0.9230)  loss_ce_1: 1.1347 (1.2312)  loss_bbox_1: 0.3983 (0.4217)  loss_giou_1: 0.8306 (0.8322)  loss_ce_2: 1.1432 (1.2564)  loss_bbox_2: 0.3912 (0.4122)  loss_giou_2: 0.8122 (0.8133)  loss_ce_3: 1.1387 (1.2471)  loss_bbox_3: 0.3898 (0.4106)  loss_giou_3: 0.8131 (0.8088)  loss_ce_4: 1.1332 (1.2429)  loss_bbox_4: 0.3879 (0.4123)  loss_giou_4: 0.8078 (0.8102)  loss_ce_unscaled: 0.5658 (0.6189)  class_error_unscaled: 37.7778 (42.8252)  loss_bbox_unscaled: 0.0781 (0.0823)  loss_giou_unscaled: 0.4073 (0.4044)  cardinality_error_unscaled: 291.5833 (289.4305)  loss_ce_0_unscaled: 0.6002 (0.6294)  loss_bbox_0_unscaled: 0.0899 (0.0950)  loss_giou_0_unscaled: 0.4636 (0.4615)  cardinality_error_0_unscaled: 290.3333 (289.2822)  loss_ce_1_unscaled: 0.5674 (0.6156)  loss_bbox_1_unscaled: 0.0797 (0.0843)  loss_giou_1_unscaled: 0.4153 (0.4161)  cardinality_error_1_unscaled: 291.5000 (289.7453)  loss_ce_2_unscaled: 0.5716 (0.6282)  loss_bbox_2_unscaled: 0.0782 (0.0824)  loss_giou_2_unscaled: 0.4061 (0.4066)  cardinality_error_2_unscaled: 290.6667 (289.3306)  loss_ce_3_unscaled: 0.5693 (0.6236)  loss_bbox_3_unscaled: 0.0780 (0.0821)  loss_giou_3_unscaled: 0.4065 (0.4044)  cardinality_error_3_unscaled: 290.8333 (289.2004)  loss_ce_4_unscaled: 0.5666 (0.6215)  loss_bbox_4_unscaled: 0.0776 (0.0825)  loss_giou_4_unscaled: 0.4039 (0.4051)  cardinality_error_4_unscaled: 290.6667 (288.1782)  time: 2.9653  data: 0.0279  max mem: 8138\n",
            "Test:  [400/417]  eta: 0:00:51  class_error: 47.92  loss: 14.3397 (15.0161)  loss_ce: 1.1405 (1.2380)  loss_bbox: 0.3930 (0.4115)  loss_giou: 0.8133 (0.8087)  loss_ce_0: 1.2194 (1.2586)  loss_bbox_0: 0.4640 (0.4754)  loss_giou_0: 0.9272 (0.9233)  loss_ce_1: 1.1669 (1.2313)  loss_bbox_1: 0.3983 (0.4220)  loss_giou_1: 0.8462 (0.8323)  loss_ce_2: 1.1636 (1.2567)  loss_bbox_2: 0.3866 (0.4123)  loss_giou_2: 0.8121 (0.8132)  loss_ce_3: 1.1463 (1.2477)  loss_bbox_3: 0.3797 (0.4106)  loss_giou_3: 0.8091 (0.8086)  loss_ce_4: 1.1431 (1.2435)  loss_bbox_4: 0.3820 (0.4123)  loss_giou_4: 0.8088 (0.8100)  loss_ce_unscaled: 0.5702 (0.6190)  class_error_unscaled: 42.5287 (42.8312)  loss_bbox_unscaled: 0.0786 (0.0823)  loss_giou_unscaled: 0.4066 (0.4043)  cardinality_error_unscaled: 290.5833 (289.4260)  loss_ce_0_unscaled: 0.6097 (0.6293)  loss_bbox_0_unscaled: 0.0928 (0.0951)  loss_giou_0_unscaled: 0.4636 (0.4616)  cardinality_error_0_unscaled: 290.0000 (289.2880)  loss_ce_1_unscaled: 0.5835 (0.6156)  loss_bbox_1_unscaled: 0.0797 (0.0844)  loss_giou_1_unscaled: 0.4231 (0.4162)  cardinality_error_1_unscaled: 290.9167 (289.7467)  loss_ce_2_unscaled: 0.5818 (0.6284)  loss_bbox_2_unscaled: 0.0773 (0.0825)  loss_giou_2_unscaled: 0.4061 (0.4066)  cardinality_error_2_unscaled: 290.6667 (289.3273)  loss_ce_3_unscaled: 0.5731 (0.6239)  loss_bbox_3_unscaled: 0.0759 (0.0821)  loss_giou_3_unscaled: 0.4046 (0.4043)  cardinality_error_3_unscaled: 289.7500 (289.1966)  loss_ce_4_unscaled: 0.5716 (0.6217)  loss_bbox_4_unscaled: 0.0764 (0.0825)  loss_giou_4_unscaled: 0.4044 (0.4050)  cardinality_error_4_unscaled: 288.0833 (288.1473)  time: 2.9004  data: 0.0293  max mem: 8138\n",
            "Test:  [410/417]  eta: 0:00:21  class_error: 49.17  loss: 15.8205 (15.0429)  loss_ce: 1.2847 (1.2404)  loss_bbox: 0.4029 (0.4117)  loss_giou: 0.8387 (0.8106)  loss_ce_0: 1.3052 (1.2610)  loss_bbox_0: 0.5012 (0.4755)  loss_giou_0: 0.9408 (0.9251)  loss_ce_1: 1.2553 (1.2335)  loss_bbox_1: 0.4177 (0.4221)  loss_giou_1: 0.8617 (0.8342)  loss_ce_2: 1.3153 (1.2593)  loss_bbox_2: 0.3994 (0.4124)  loss_giou_2: 0.8433 (0.8150)  loss_ce_3: 1.2934 (1.2503)  loss_bbox_3: 0.4022 (0.4109)  loss_giou_3: 0.8401 (0.8104)  loss_ce_4: 1.2969 (1.2459)  loss_bbox_4: 0.4095 (0.4126)  loss_giou_4: 0.8363 (0.8119)  loss_ce_unscaled: 0.6423 (0.6202)  class_error_unscaled: 45.0000 (42.8930)  loss_bbox_unscaled: 0.0806 (0.0823)  loss_giou_unscaled: 0.4194 (0.4053)  cardinality_error_unscaled: 289.7500 (289.4292)  loss_ce_0_unscaled: 0.6526 (0.6305)  loss_bbox_0_unscaled: 0.1002 (0.0951)  loss_giou_0_unscaled: 0.4704 (0.4625)  cardinality_error_0_unscaled: 289.9167 (289.2904)  loss_ce_1_unscaled: 0.6277 (0.6167)  loss_bbox_1_unscaled: 0.0835 (0.0844)  loss_giou_1_unscaled: 0.4309 (0.4171)  cardinality_error_1_unscaled: 290.5000 (289.7634)  loss_ce_2_unscaled: 0.6577 (0.6296)  loss_bbox_2_unscaled: 0.0799 (0.0825)  loss_giou_2_unscaled: 0.4216 (0.4075)  cardinality_error_2_unscaled: 290.0000 (289.3360)  loss_ce_3_unscaled: 0.6467 (0.6251)  loss_bbox_3_unscaled: 0.0804 (0.0822)  loss_giou_3_unscaled: 0.4201 (0.4052)  cardinality_error_3_unscaled: 289.2500 (289.1965)  loss_ce_4_unscaled: 0.6485 (0.6229)  loss_bbox_4_unscaled: 0.0819 (0.0825)  loss_giou_4_unscaled: 0.4182 (0.4060)  cardinality_error_4_unscaled: 287.8333 (288.1399)  time: 2.8975  data: 0.0296  max mem: 8138\n",
            "Test:  [416/417]  eta: 0:00:03  class_error: 29.27  loss: 15.3379 (15.0160)  loss_ce: 1.2148 (1.2384)  loss_bbox: 0.3997 (0.4108)  loss_giou: 0.7781 (0.8091)  loss_ce_0: 1.2420 (1.2592)  loss_bbox_0: 0.4868 (0.4748)  loss_giou_0: 0.8982 (0.9237)  loss_ce_1: 1.2308 (1.2315)  loss_bbox_1: 0.4161 (0.4213)  loss_giou_1: 0.8078 (0.8327)  loss_ce_2: 1.2559 (1.2570)  loss_bbox_2: 0.3917 (0.4114)  loss_giou_2: 0.7792 (0.8135)  loss_ce_3: 1.2664 (1.2480)  loss_bbox_3: 0.3960 (0.4099)  loss_giou_3: 0.7827 (0.8091)  loss_ce_4: 1.2331 (1.2436)  loss_bbox_4: 0.4109 (0.4117)  loss_giou_4: 0.7823 (0.8104)  loss_ce_unscaled: 0.6074 (0.6192)  class_error_unscaled: 40.9836 (42.8286)  loss_bbox_unscaled: 0.0799 (0.0822)  loss_giou_unscaled: 0.3890 (0.4046)  cardinality_error_unscaled: 290.5000 (289.4597)  loss_ce_0_unscaled: 0.6210 (0.6296)  loss_bbox_0_unscaled: 0.0974 (0.0950)  loss_giou_0_unscaled: 0.4491 (0.4619)  cardinality_error_0_unscaled: 290.6667 (289.3242)  loss_ce_1_unscaled: 0.6154 (0.6157)  loss_bbox_1_unscaled: 0.0832 (0.0843)  loss_giou_1_unscaled: 0.4039 (0.4163)  cardinality_error_1_unscaled: 290.9167 (289.7943)  loss_ce_2_unscaled: 0.6280 (0.6285)  loss_bbox_2_unscaled: 0.0783 (0.0823)  loss_giou_2_unscaled: 0.3896 (0.4068)  cardinality_error_2_unscaled: 290.3333 (289.3670)  loss_ce_3_unscaled: 0.6332 (0.6240)  loss_bbox_3_unscaled: 0.0792 (0.0820)  loss_giou_3_unscaled: 0.3914 (0.4045)  cardinality_error_3_unscaled: 290.5000 (289.2299)  loss_ce_4_unscaled: 0.6165 (0.6218)  loss_bbox_4_unscaled: 0.0822 (0.0823)  loss_giou_4_unscaled: 0.3912 (0.4052)  cardinality_error_4_unscaled: 288.0833 (288.1715)  time: 2.8896  data: 0.0286  max mem: 8138\n",
            "Test: Total time: 0:21:00 (3.0223 s / it)\n",
            "Averaged stats: class_error: 29.27  loss: 15.3379 (15.0160)  loss_ce: 1.2148 (1.2384)  loss_bbox: 0.3997 (0.4108)  loss_giou: 0.7781 (0.8091)  loss_ce_0: 1.2420 (1.2592)  loss_bbox_0: 0.4868 (0.4748)  loss_giou_0: 0.8982 (0.9237)  loss_ce_1: 1.2308 (1.2315)  loss_bbox_1: 0.4161 (0.4213)  loss_giou_1: 0.8078 (0.8327)  loss_ce_2: 1.2559 (1.2570)  loss_bbox_2: 0.3917 (0.4114)  loss_giou_2: 0.7792 (0.8135)  loss_ce_3: 1.2664 (1.2480)  loss_bbox_3: 0.3960 (0.4099)  loss_giou_3: 0.7827 (0.8091)  loss_ce_4: 1.2331 (1.2436)  loss_bbox_4: 0.4109 (0.4117)  loss_giou_4: 0.7823 (0.8104)  loss_ce_unscaled: 0.6074 (0.6192)  class_error_unscaled: 40.9836 (42.8286)  loss_bbox_unscaled: 0.0799 (0.0822)  loss_giou_unscaled: 0.3890 (0.4046)  cardinality_error_unscaled: 290.5000 (289.4597)  loss_ce_0_unscaled: 0.6210 (0.6296)  loss_bbox_0_unscaled: 0.0974 (0.0950)  loss_giou_0_unscaled: 0.4491 (0.4619)  cardinality_error_0_unscaled: 290.6667 (289.3242)  loss_ce_1_unscaled: 0.6154 (0.6157)  loss_bbox_1_unscaled: 0.0832 (0.0843)  loss_giou_1_unscaled: 0.4039 (0.4163)  cardinality_error_1_unscaled: 290.9167 (289.7943)  loss_ce_2_unscaled: 0.6280 (0.6285)  loss_bbox_2_unscaled: 0.0783 (0.0823)  loss_giou_2_unscaled: 0.3896 (0.4068)  cardinality_error_2_unscaled: 290.3333 (289.3670)  loss_ce_3_unscaled: 0.6332 (0.6240)  loss_bbox_3_unscaled: 0.0792 (0.0820)  loss_giou_3_unscaled: 0.3914 (0.4045)  cardinality_error_3_unscaled: 290.5000 (289.2299)  loss_ce_4_unscaled: 0.6165 (0.6218)  loss_bbox_4_unscaled: 0.0822 (0.0823)  loss_giou_4_unscaled: 0.3912 (0.4052)  cardinality_error_4_unscaled: 288.0833 (288.1715)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=13.41s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.266\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.399\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.282\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.134\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.280\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.388\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.265\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.431\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.462\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.250\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.487\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.680\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py \\\n",
        "    --output_dir exps/deform \\\n",
        "    --coco_path ../COCODIR \\\n",
        "    --batch_size 12 \\\n",
        "    --resume ./pth/cd-detr-v1.pth \\\n",
        "    --with_box_refine \\\n",
        "    --eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvnaZLVrLzsA",
        "outputId": "5cbce06a-4956-42e4-b962-bb9caa678332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "git:\n",
            "  sha: 11169a60c33333af00a4849f1808023eba96a931, status: has uncommited changes, branch: main\n",
            "\n",
            "Namespace(lr=0.0002, lr_backbone_names=['backbone.0'], lr_backbone=2e-05, lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, batch_size=12, weight_decay=0.0001, epochs=50, lr_drop=40, lr_drop_epochs=None, clip_max_norm=0.1, sgd=False, with_box_refine=True, two_stage=False, frozen_weights=None, backbone='resnet50', dilation=False, position_embedding='sine', position_embedding_scale=6.283185307179586, num_feature_levels=4, enc_layers=6, dec_layers=6, dim_feedforward=1024, hidden_dim=256, dropout=0.1, nheads=8, num_queries=300, dec_n_points=4, enc_n_points=4, masks=False, aux_loss=True, set_cost_class=2, set_cost_bbox=5, set_cost_giou=2, mask_loss_coef=1, dice_loss_coef=1, cls_loss_coef=2, bbox_loss_coef=5, giou_loss_coef=2, focal_alpha=0.25, dataset_file='coco', coco_path='../COCODIR', coco_panoptic_path=None, remove_difficult=False, output_dir='exps/deform', device='cuda', seed=42, resume='./pth/cd-detr-v1.pth', start_epoch=0, eval=True, num_workers=2, cache_mode=False, distributed=False)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "number of params: 40627260\n",
            "loading annotations into memory...\n",
            "Done (t=15.06s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=1.87s)\n",
            "creating index...\n",
            "index created!\n",
            "transformer.level_embed\n",
            "transformer.encoder.layers.0.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.0.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.0.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.0.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.0.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.0.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.0.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.0.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.0.norm1.weight\n",
            "transformer.encoder.layers.0.norm1.bias\n",
            "transformer.encoder.layers.0.linear1.weight\n",
            "transformer.encoder.layers.0.linear1.bias\n",
            "transformer.encoder.layers.0.linear2.weight\n",
            "transformer.encoder.layers.0.linear2.bias\n",
            "transformer.encoder.layers.0.norm2.weight\n",
            "transformer.encoder.layers.0.norm2.bias\n",
            "transformer.encoder.layers.1.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.1.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.1.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.1.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.1.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.1.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.1.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.1.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.1.norm1.weight\n",
            "transformer.encoder.layers.1.norm1.bias\n",
            "transformer.encoder.layers.1.linear1.weight\n",
            "transformer.encoder.layers.1.linear1.bias\n",
            "transformer.encoder.layers.1.linear2.weight\n",
            "transformer.encoder.layers.1.linear2.bias\n",
            "transformer.encoder.layers.1.norm2.weight\n",
            "transformer.encoder.layers.1.norm2.bias\n",
            "transformer.encoder.layers.2.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.2.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.2.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.2.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.2.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.2.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.2.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.2.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.2.norm1.weight\n",
            "transformer.encoder.layers.2.norm1.bias\n",
            "transformer.encoder.layers.2.linear1.weight\n",
            "transformer.encoder.layers.2.linear1.bias\n",
            "transformer.encoder.layers.2.linear2.weight\n",
            "transformer.encoder.layers.2.linear2.bias\n",
            "transformer.encoder.layers.2.norm2.weight\n",
            "transformer.encoder.layers.2.norm2.bias\n",
            "transformer.encoder.layers.3.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.3.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.3.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.3.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.3.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.3.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.3.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.3.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.3.norm1.weight\n",
            "transformer.encoder.layers.3.norm1.bias\n",
            "transformer.encoder.layers.3.linear1.weight\n",
            "transformer.encoder.layers.3.linear1.bias\n",
            "transformer.encoder.layers.3.linear2.weight\n",
            "transformer.encoder.layers.3.linear2.bias\n",
            "transformer.encoder.layers.3.norm2.weight\n",
            "transformer.encoder.layers.3.norm2.bias\n",
            "transformer.encoder.layers.4.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.4.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.4.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.4.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.4.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.4.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.4.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.4.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.4.norm1.weight\n",
            "transformer.encoder.layers.4.norm1.bias\n",
            "transformer.encoder.layers.4.linear1.weight\n",
            "transformer.encoder.layers.4.linear1.bias\n",
            "transformer.encoder.layers.4.linear2.weight\n",
            "transformer.encoder.layers.4.linear2.bias\n",
            "transformer.encoder.layers.4.norm2.weight\n",
            "transformer.encoder.layers.4.norm2.bias\n",
            "transformer.encoder.layers.5.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.5.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.5.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.5.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.5.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.5.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.5.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.5.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.5.norm1.weight\n",
            "transformer.encoder.layers.5.norm1.bias\n",
            "transformer.encoder.layers.5.linear1.weight\n",
            "transformer.encoder.layers.5.linear1.bias\n",
            "transformer.encoder.layers.5.linear2.weight\n",
            "transformer.encoder.layers.5.linear2.bias\n",
            "transformer.encoder.layers.5.norm2.weight\n",
            "transformer.encoder.layers.5.norm2.bias\n",
            "transformer.decoder.layers.0.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.0.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.0.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.0.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.0.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.0.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.0.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.0.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.0.norm1.weight\n",
            "transformer.decoder.layers.0.norm1.bias\n",
            "transformer.decoder.layers.0.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.0.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.0.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.0.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.0.norm2.weight\n",
            "transformer.decoder.layers.0.norm2.bias\n",
            "transformer.decoder.layers.0.linear1.weight\n",
            "transformer.decoder.layers.0.linear1.bias\n",
            "transformer.decoder.layers.0.linear2.weight\n",
            "transformer.decoder.layers.0.linear2.bias\n",
            "transformer.decoder.layers.0.norm3.weight\n",
            "transformer.decoder.layers.0.norm3.bias\n",
            "transformer.decoder.layers.1.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.1.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.1.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.1.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.1.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.1.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.1.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.1.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.1.norm1.weight\n",
            "transformer.decoder.layers.1.norm1.bias\n",
            "transformer.decoder.layers.1.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.1.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.1.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.1.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.1.norm2.weight\n",
            "transformer.decoder.layers.1.norm2.bias\n",
            "transformer.decoder.layers.1.linear1.weight\n",
            "transformer.decoder.layers.1.linear1.bias\n",
            "transformer.decoder.layers.1.linear2.weight\n",
            "transformer.decoder.layers.1.linear2.bias\n",
            "transformer.decoder.layers.1.norm3.weight\n",
            "transformer.decoder.layers.1.norm3.bias\n",
            "transformer.decoder.layers.2.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.2.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.2.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.2.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.2.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.2.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.2.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.2.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.2.norm1.weight\n",
            "transformer.decoder.layers.2.norm1.bias\n",
            "transformer.decoder.layers.2.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.2.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.2.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.2.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.2.norm2.weight\n",
            "transformer.decoder.layers.2.norm2.bias\n",
            "transformer.decoder.layers.2.linear1.weight\n",
            "transformer.decoder.layers.2.linear1.bias\n",
            "transformer.decoder.layers.2.linear2.weight\n",
            "transformer.decoder.layers.2.linear2.bias\n",
            "transformer.decoder.layers.2.norm3.weight\n",
            "transformer.decoder.layers.2.norm3.bias\n",
            "transformer.decoder.layers.3.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.3.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.3.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.3.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.3.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.3.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.3.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.3.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.3.norm1.weight\n",
            "transformer.decoder.layers.3.norm1.bias\n",
            "transformer.decoder.layers.3.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.3.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.3.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.3.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.3.norm2.weight\n",
            "transformer.decoder.layers.3.norm2.bias\n",
            "transformer.decoder.layers.3.linear1.weight\n",
            "transformer.decoder.layers.3.linear1.bias\n",
            "transformer.decoder.layers.3.linear2.weight\n",
            "transformer.decoder.layers.3.linear2.bias\n",
            "transformer.decoder.layers.3.norm3.weight\n",
            "transformer.decoder.layers.3.norm3.bias\n",
            "transformer.decoder.layers.4.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.4.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.4.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.4.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.4.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.4.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.4.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.4.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.4.norm1.weight\n",
            "transformer.decoder.layers.4.norm1.bias\n",
            "transformer.decoder.layers.4.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.4.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.4.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.4.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.4.norm2.weight\n",
            "transformer.decoder.layers.4.norm2.bias\n",
            "transformer.decoder.layers.4.linear1.weight\n",
            "transformer.decoder.layers.4.linear1.bias\n",
            "transformer.decoder.layers.4.linear2.weight\n",
            "transformer.decoder.layers.4.linear2.bias\n",
            "transformer.decoder.layers.4.norm3.weight\n",
            "transformer.decoder.layers.4.norm3.bias\n",
            "transformer.decoder.layers.5.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.5.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.5.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.5.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.5.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.5.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.5.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.5.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.5.norm1.weight\n",
            "transformer.decoder.layers.5.norm1.bias\n",
            "transformer.decoder.layers.5.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.5.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.5.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.5.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.5.norm2.weight\n",
            "transformer.decoder.layers.5.norm2.bias\n",
            "transformer.decoder.layers.5.linear1.weight\n",
            "transformer.decoder.layers.5.linear1.bias\n",
            "transformer.decoder.layers.5.linear2.weight\n",
            "transformer.decoder.layers.5.linear2.bias\n",
            "transformer.decoder.layers.5.norm3.weight\n",
            "transformer.decoder.layers.5.norm3.bias\n",
            "transformer.decoder.bbox_embed.0.layers.0.weight\n",
            "transformer.decoder.bbox_embed.0.layers.0.bias\n",
            "transformer.decoder.bbox_embed.0.layers.1.weight\n",
            "transformer.decoder.bbox_embed.0.layers.1.bias\n",
            "transformer.decoder.bbox_embed.0.layers.2.weight\n",
            "transformer.decoder.bbox_embed.0.layers.2.bias\n",
            "transformer.decoder.bbox_embed.1.layers.0.weight\n",
            "transformer.decoder.bbox_embed.1.layers.0.bias\n",
            "transformer.decoder.bbox_embed.1.layers.1.weight\n",
            "transformer.decoder.bbox_embed.1.layers.1.bias\n",
            "transformer.decoder.bbox_embed.1.layers.2.weight\n",
            "transformer.decoder.bbox_embed.1.layers.2.bias\n",
            "transformer.decoder.bbox_embed.2.layers.0.weight\n",
            "transformer.decoder.bbox_embed.2.layers.0.bias\n",
            "transformer.decoder.bbox_embed.2.layers.1.weight\n",
            "transformer.decoder.bbox_embed.2.layers.1.bias\n",
            "transformer.decoder.bbox_embed.2.layers.2.weight\n",
            "transformer.decoder.bbox_embed.2.layers.2.bias\n",
            "transformer.decoder.bbox_embed.3.layers.0.weight\n",
            "transformer.decoder.bbox_embed.3.layers.0.bias\n",
            "transformer.decoder.bbox_embed.3.layers.1.weight\n",
            "transformer.decoder.bbox_embed.3.layers.1.bias\n",
            "transformer.decoder.bbox_embed.3.layers.2.weight\n",
            "transformer.decoder.bbox_embed.3.layers.2.bias\n",
            "transformer.decoder.bbox_embed.4.layers.0.weight\n",
            "transformer.decoder.bbox_embed.4.layers.0.bias\n",
            "transformer.decoder.bbox_embed.4.layers.1.weight\n",
            "transformer.decoder.bbox_embed.4.layers.1.bias\n",
            "transformer.decoder.bbox_embed.4.layers.2.weight\n",
            "transformer.decoder.bbox_embed.4.layers.2.bias\n",
            "transformer.decoder.bbox_embed.5.layers.0.weight\n",
            "transformer.decoder.bbox_embed.5.layers.0.bias\n",
            "transformer.decoder.bbox_embed.5.layers.1.weight\n",
            "transformer.decoder.bbox_embed.5.layers.1.bias\n",
            "transformer.decoder.bbox_embed.5.layers.2.weight\n",
            "transformer.decoder.bbox_embed.5.layers.2.bias\n",
            "transformer.reference_points.weight\n",
            "transformer.reference_points.bias\n",
            "class_embed.0.weight\n",
            "class_embed.0.bias\n",
            "class_embed.1.weight\n",
            "class_embed.1.bias\n",
            "class_embed.2.weight\n",
            "class_embed.2.bias\n",
            "class_embed.3.weight\n",
            "class_embed.3.bias\n",
            "class_embed.4.weight\n",
            "class_embed.4.bias\n",
            "class_embed.5.weight\n",
            "class_embed.5.bias\n",
            "query_embed.weight\n",
            "input_proj.0.0.weight\n",
            "input_proj.0.0.bias\n",
            "input_proj.0.1.weight\n",
            "input_proj.0.1.bias\n",
            "input_proj.1.0.weight\n",
            "input_proj.1.0.bias\n",
            "input_proj.1.1.weight\n",
            "input_proj.1.1.bias\n",
            "input_proj.2.0.weight\n",
            "input_proj.2.0.bias\n",
            "input_proj.2.1.weight\n",
            "input_proj.2.1.bias\n",
            "input_proj.3.0.weight\n",
            "input_proj.3.0.bias\n",
            "input_proj.3.1.weight\n",
            "input_proj.3.1.bias\n",
            "backbone.0.body.conv1.weight\n",
            "backbone.0.body.layer1.0.conv1.weight\n",
            "backbone.0.body.layer1.0.conv2.weight\n",
            "backbone.0.body.layer1.0.conv3.weight\n",
            "backbone.0.body.layer1.0.downsample.0.weight\n",
            "backbone.0.body.layer1.1.conv1.weight\n",
            "backbone.0.body.layer1.1.conv2.weight\n",
            "backbone.0.body.layer1.1.conv3.weight\n",
            "backbone.0.body.layer1.2.conv1.weight\n",
            "backbone.0.body.layer1.2.conv2.weight\n",
            "backbone.0.body.layer1.2.conv3.weight\n",
            "backbone.0.body.layer2.0.conv1.weight\n",
            "backbone.0.body.layer2.0.conv2.weight\n",
            "backbone.0.body.layer2.0.conv3.weight\n",
            "backbone.0.body.layer2.0.downsample.0.weight\n",
            "backbone.0.body.layer2.1.conv1.weight\n",
            "backbone.0.body.layer2.1.conv2.weight\n",
            "backbone.0.body.layer2.1.conv3.weight\n",
            "backbone.0.body.layer2.2.conv1.weight\n",
            "backbone.0.body.layer2.2.conv2.weight\n",
            "backbone.0.body.layer2.2.conv3.weight\n",
            "backbone.0.body.layer2.3.conv1.weight\n",
            "backbone.0.body.layer2.3.conv2.weight\n",
            "backbone.0.body.layer2.3.conv3.weight\n",
            "backbone.0.body.layer3.0.conv1.weight\n",
            "backbone.0.body.layer3.0.conv2.weight\n",
            "backbone.0.body.layer3.0.conv3.weight\n",
            "backbone.0.body.layer3.0.downsample.0.weight\n",
            "backbone.0.body.layer3.1.conv1.weight\n",
            "backbone.0.body.layer3.1.conv2.weight\n",
            "backbone.0.body.layer3.1.conv3.weight\n",
            "backbone.0.body.layer3.2.conv1.weight\n",
            "backbone.0.body.layer3.2.conv2.weight\n",
            "backbone.0.body.layer3.2.conv3.weight\n",
            "backbone.0.body.layer3.3.conv1.weight\n",
            "backbone.0.body.layer3.3.conv2.weight\n",
            "backbone.0.body.layer3.3.conv3.weight\n",
            "backbone.0.body.layer3.4.conv1.weight\n",
            "backbone.0.body.layer3.4.conv2.weight\n",
            "backbone.0.body.layer3.4.conv3.weight\n",
            "backbone.0.body.layer3.5.conv1.weight\n",
            "backbone.0.body.layer3.5.conv2.weight\n",
            "backbone.0.body.layer3.5.conv3.weight\n",
            "backbone.0.body.layer4.0.conv1.weight\n",
            "backbone.0.body.layer4.0.conv2.weight\n",
            "backbone.0.body.layer4.0.conv3.weight\n",
            "backbone.0.body.layer4.0.downsample.0.weight\n",
            "backbone.0.body.layer4.1.conv1.weight\n",
            "backbone.0.body.layer4.1.conv2.weight\n",
            "backbone.0.body.layer4.1.conv3.weight\n",
            "backbone.0.body.layer4.2.conv1.weight\n",
            "backbone.0.body.layer4.2.conv2.weight\n",
            "backbone.0.body.layer4.2.conv3.weight\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Test:  [  0/417]  eta: 0:28:08  class_error: 52.63  loss: 19.8554 (19.8554)  loss_ce: 1.7171 (1.7171)  loss_bbox: 0.6358 (0.6358)  loss_giou: 0.9845 (0.9845)  loss_ce_0: 1.6706 (1.6706)  loss_bbox_0: 0.6730 (0.6730)  loss_giou_0: 1.0262 (1.0262)  loss_ce_1: 1.6849 (1.6849)  loss_bbox_1: 0.6378 (0.6378)  loss_giou_1: 0.9922 (0.9922)  loss_ce_2: 1.6443 (1.6443)  loss_bbox_2: 0.6473 (0.6473)  loss_giou_2: 0.9962 (0.9962)  loss_ce_3: 1.6765 (1.6765)  loss_bbox_3: 0.6359 (0.6359)  loss_giou_3: 0.9840 (0.9840)  loss_ce_4: 1.6292 (1.6292)  loss_bbox_4: 0.6360 (0.6360)  loss_giou_4: 0.9841 (0.9841)  loss_ce_unscaled: 0.8586 (0.8586)  class_error_unscaled: 52.6316 (52.6316)  loss_bbox_unscaled: 0.1272 (0.1272)  loss_giou_unscaled: 0.4922 (0.4922)  cardinality_error_unscaled: 291.9167 (291.9167)  loss_ce_0_unscaled: 0.8353 (0.8353)  loss_bbox_0_unscaled: 0.1346 (0.1346)  loss_giou_0_unscaled: 0.5131 (0.5131)  cardinality_error_0_unscaled: 291.9167 (291.9167)  loss_ce_1_unscaled: 0.8424 (0.8424)  loss_bbox_1_unscaled: 0.1276 (0.1276)  loss_giou_1_unscaled: 0.4961 (0.4961)  cardinality_error_1_unscaled: 292.0833 (292.0833)  loss_ce_2_unscaled: 0.8221 (0.8221)  loss_bbox_2_unscaled: 0.1295 (0.1295)  loss_giou_2_unscaled: 0.4981 (0.4981)  cardinality_error_2_unscaled: 292.0833 (292.0833)  loss_ce_3_unscaled: 0.8382 (0.8382)  loss_bbox_3_unscaled: 0.1272 (0.1272)  loss_giou_3_unscaled: 0.4920 (0.4920)  cardinality_error_3_unscaled: 292.0833 (292.0833)  loss_ce_4_unscaled: 0.8146 (0.8146)  loss_bbox_4_unscaled: 0.1272 (0.1272)  loss_giou_4_unscaled: 0.4921 (0.4921)  cardinality_error_4_unscaled: 292.0000 (292.0000)  time: 4.0492  data: 0.9581  max mem: 6678\n",
            "Test:  [ 10/417]  eta: 0:20:10  class_error: 74.75  loss: 19.1770 (19.6038)  loss_ce: 1.7943 (1.8387)  loss_bbox: 0.5391 (0.5388)  loss_giou: 0.8541 (0.8832)  loss_ce_0: 1.7648 (1.8205)  loss_bbox_0: 0.5968 (0.5929)  loss_giou_0: 0.9726 (0.9841)  loss_ce_1: 1.7184 (1.8106)  loss_bbox_1: 0.5563 (0.5512)  loss_giou_1: 0.8914 (0.9066)  loss_ce_2: 1.7219 (1.8266)  loss_bbox_2: 0.5248 (0.5367)  loss_giou_2: 0.8545 (0.8882)  loss_ce_3: 1.7858 (1.8142)  loss_bbox_3: 0.5238 (0.5371)  loss_giou_3: 0.8506 (0.8772)  loss_ce_4: 1.7929 (1.7841)  loss_bbox_4: 0.5249 (0.5366)  loss_giou_4: 0.8541 (0.8764)  loss_ce_unscaled: 0.8971 (0.9193)  class_error_unscaled: 58.9474 (61.1536)  loss_bbox_unscaled: 0.1078 (0.1078)  loss_giou_unscaled: 0.4271 (0.4416)  cardinality_error_unscaled: 292.3333 (292.7576)  loss_ce_0_unscaled: 0.8824 (0.9103)  loss_bbox_0_unscaled: 0.1194 (0.1186)  loss_giou_0_unscaled: 0.4863 (0.4921)  cardinality_error_0_unscaled: 292.1667 (292.8561)  loss_ce_1_unscaled: 0.8592 (0.9053)  loss_bbox_1_unscaled: 0.1113 (0.1102)  loss_giou_1_unscaled: 0.4457 (0.4533)  cardinality_error_1_unscaled: 292.2500 (292.8182)  loss_ce_2_unscaled: 0.8610 (0.9133)  loss_bbox_2_unscaled: 0.1050 (0.1073)  loss_giou_2_unscaled: 0.4273 (0.4441)  cardinality_error_2_unscaled: 292.3333 (292.8864)  loss_ce_3_unscaled: 0.8929 (0.9071)  loss_bbox_3_unscaled: 0.1048 (0.1074)  loss_giou_3_unscaled: 0.4253 (0.4386)  cardinality_error_3_unscaled: 292.3333 (292.8030)  loss_ce_4_unscaled: 0.8965 (0.8921)  loss_bbox_4_unscaled: 0.1050 (0.1073)  loss_giou_4_unscaled: 0.4270 (0.4382)  cardinality_error_4_unscaled: 292.3333 (292.8106)  time: 2.9751  data: 0.1195  max mem: 7374\n",
            "Test:  [ 20/417]  eta: 0:19:22  class_error: 56.38  loss: 17.6341 (18.6139)  loss_ce: 1.6785 (1.7677)  loss_bbox: 0.4371 (0.4872)  loss_giou: 0.8060 (0.8346)  loss_ce_0: 1.7248 (1.7532)  loss_bbox_0: 0.4766 (0.5396)  loss_giou_0: 0.9274 (0.9412)  loss_ce_1: 1.7184 (1.7484)  loss_bbox_1: 0.4377 (0.4982)  loss_giou_1: 0.8273 (0.8579)  loss_ce_2: 1.7127 (1.7600)  loss_bbox_2: 0.4462 (0.4879)  loss_giou_2: 0.8104 (0.8390)  loss_ce_3: 1.6429 (1.7471)  loss_bbox_3: 0.4325 (0.4861)  loss_giou_3: 0.7993 (0.8309)  loss_ce_4: 1.6499 (1.7197)  loss_bbox_4: 0.4287 (0.4849)  loss_giou_4: 0.8062 (0.8303)  loss_ce_unscaled: 0.8393 (0.8839)  class_error_unscaled: 57.1429 (58.4427)  loss_bbox_unscaled: 0.0874 (0.0974)  loss_giou_unscaled: 0.4030 (0.4173)  cardinality_error_unscaled: 292.3333 (292.9048)  loss_ce_0_unscaled: 0.8624 (0.8766)  loss_bbox_0_unscaled: 0.0953 (0.1079)  loss_giou_0_unscaled: 0.4637 (0.4706)  cardinality_error_0_unscaled: 292.1667 (292.9445)  loss_ce_1_unscaled: 0.8592 (0.8742)  loss_bbox_1_unscaled: 0.0875 (0.0996)  loss_giou_1_unscaled: 0.4137 (0.4289)  cardinality_error_1_unscaled: 292.2500 (292.8810)  loss_ce_2_unscaled: 0.8564 (0.8800)  loss_bbox_2_unscaled: 0.0892 (0.0976)  loss_giou_2_unscaled: 0.4052 (0.4195)  cardinality_error_2_unscaled: 292.3333 (292.9008)  loss_ce_3_unscaled: 0.8214 (0.8736)  loss_bbox_3_unscaled: 0.0865 (0.0972)  loss_giou_3_unscaled: 0.3997 (0.4155)  cardinality_error_3_unscaled: 292.3333 (292.8968)  loss_ce_4_unscaled: 0.8250 (0.8598)  loss_bbox_4_unscaled: 0.0857 (0.0970)  loss_giou_4_unscaled: 0.4031 (0.4152)  cardinality_error_4_unscaled: 292.3333 (292.9365)  time: 2.8725  data: 0.0333  max mem: 7755\n",
            "Test:  [ 30/417]  eta: 0:19:12  class_error: 54.93  loss: 15.9783 (17.8819)  loss_ce: 1.5439 (1.6805)  loss_bbox: 0.4018 (0.4495)  loss_giou: 0.7886 (0.8413)  loss_ce_0: 1.4692 (1.6544)  loss_bbox_0: 0.4467 (0.5031)  loss_giou_0: 0.9115 (0.9502)  loss_ce_1: 1.4654 (1.6491)  loss_bbox_1: 0.4195 (0.4604)  loss_giou_1: 0.8163 (0.8635)  loss_ce_2: 1.5049 (1.6632)  loss_bbox_2: 0.4112 (0.4504)  loss_giou_2: 0.7952 (0.8458)  loss_ce_3: 1.5354 (1.6640)  loss_bbox_3: 0.4012 (0.4478)  loss_giou_3: 0.7940 (0.8379)  loss_ce_4: 1.4842 (1.6347)  loss_bbox_4: 0.4007 (0.4479)  loss_giou_4: 0.7896 (0.8380)  loss_ce_unscaled: 0.7719 (0.8403)  class_error_unscaled: 52.6316 (55.8267)  loss_bbox_unscaled: 0.0804 (0.0899)  loss_giou_unscaled: 0.3943 (0.4206)  cardinality_error_unscaled: 291.7500 (292.4543)  loss_ce_0_unscaled: 0.7346 (0.8272)  loss_bbox_0_unscaled: 0.0893 (0.1006)  loss_giou_0_unscaled: 0.4558 (0.4751)  cardinality_error_0_unscaled: 291.6667 (292.4866)  loss_ce_1_unscaled: 0.7327 (0.8245)  loss_bbox_1_unscaled: 0.0839 (0.0921)  loss_giou_1_unscaled: 0.4082 (0.4318)  cardinality_error_1_unscaled: 291.6667 (292.3952)  loss_ce_2_unscaled: 0.7524 (0.8316)  loss_bbox_2_unscaled: 0.0822 (0.0901)  loss_giou_2_unscaled: 0.3976 (0.4229)  cardinality_error_2_unscaled: 291.5833 (292.4462)  loss_ce_3_unscaled: 0.7677 (0.8320)  loss_bbox_3_unscaled: 0.0802 (0.0896)  loss_giou_3_unscaled: 0.3970 (0.4189)  cardinality_error_3_unscaled: 291.5833 (292.4570)  loss_ce_4_unscaled: 0.7421 (0.8174)  loss_bbox_4_unscaled: 0.0801 (0.0896)  loss_giou_4_unscaled: 0.3948 (0.4190)  cardinality_error_4_unscaled: 291.7500 (292.5135)  time: 2.9783  data: 0.0307  max mem: 8137\n",
            "Test:  [ 40/417]  eta: 0:18:59  class_error: 65.55  loss: 17.0742 (17.7331)  loss_ce: 1.6458 (1.6815)  loss_bbox: 0.3799 (0.4420)  loss_giou: 0.7749 (0.8297)  loss_ce_0: 1.5084 (1.6391)  loss_bbox_0: 0.4370 (0.5018)  loss_giou_0: 0.9280 (0.9377)  loss_ce_1: 1.5352 (1.6414)  loss_bbox_1: 0.4041 (0.4532)  loss_giou_1: 0.8095 (0.8499)  loss_ce_2: 1.5134 (1.6530)  loss_bbox_2: 0.3877 (0.4425)  loss_giou_2: 0.7928 (0.8328)  loss_ce_3: 1.6433 (1.6614)  loss_bbox_3: 0.3748 (0.4393)  loss_giou_3: 0.7683 (0.8255)  loss_ce_4: 1.5710 (1.6353)  loss_bbox_4: 0.3890 (0.4405)  loss_giou_4: 0.7790 (0.8264)  loss_ce_unscaled: 0.8229 (0.8408)  class_error_unscaled: 56.7568 (56.3369)  loss_bbox_unscaled: 0.0760 (0.0884)  loss_giou_unscaled: 0.3874 (0.4149)  cardinality_error_unscaled: 293.3333 (292.7785)  loss_ce_0_unscaled: 0.7542 (0.8195)  loss_bbox_0_unscaled: 0.0874 (0.1004)  loss_giou_0_unscaled: 0.4640 (0.4689)  cardinality_error_0_unscaled: 293.4167 (292.8191)  loss_ce_1_unscaled: 0.7676 (0.8207)  loss_bbox_1_unscaled: 0.0808 (0.0906)  loss_giou_1_unscaled: 0.4047 (0.4250)  cardinality_error_1_unscaled: 293.2500 (292.7256)  loss_ce_2_unscaled: 0.7567 (0.8265)  loss_bbox_2_unscaled: 0.0775 (0.0885)  loss_giou_2_unscaled: 0.3964 (0.4164)  cardinality_error_2_unscaled: 293.0833 (292.7663)  loss_ce_3_unscaled: 0.8216 (0.8307)  loss_bbox_3_unscaled: 0.0750 (0.0879)  loss_giou_3_unscaled: 0.3842 (0.4127)  cardinality_error_3_unscaled: 293.3333 (292.7886)  loss_ce_4_unscaled: 0.7855 (0.8176)  loss_bbox_4_unscaled: 0.0778 (0.0881)  loss_giou_4_unscaled: 0.3895 (0.4132)  cardinality_error_4_unscaled: 293.4167 (292.8211)  time: 3.1216  data: 0.0327  max mem: 8137\n",
            "Test:  [ 50/417]  eta: 0:18:33  class_error: 47.78  loss: 17.9401 (18.0658)  loss_ce: 1.7301 (1.7115)  loss_bbox: 0.4477 (0.4549)  loss_giou: 0.7749 (0.8383)  loss_ce_0: 1.6515 (1.6736)  loss_bbox_0: 0.5189 (0.5121)  loss_giou_0: 0.9397 (0.9486)  loss_ce_1: 1.6835 (1.6788)  loss_bbox_1: 0.4707 (0.4655)  loss_giou_1: 0.8232 (0.8614)  loss_ce_2: 1.6876 (1.6883)  loss_bbox_2: 0.4492 (0.4548)  loss_giou_2: 0.7928 (0.8433)  loss_ce_3: 1.7302 (1.6942)  loss_bbox_3: 0.4441 (0.4520)  loss_giou_3: 0.7683 (0.8350)  loss_ce_4: 1.6803 (1.6656)  loss_bbox_4: 0.4475 (0.4528)  loss_giou_4: 0.7686 (0.8351)  loss_ce_unscaled: 0.8651 (0.8557)  class_error_unscaled: 60.3175 (57.1300)  loss_bbox_unscaled: 0.0895 (0.0910)  loss_giou_unscaled: 0.3874 (0.4191)  cardinality_error_unscaled: 292.5000 (292.2435)  loss_ce_0_unscaled: 0.8258 (0.8368)  loss_bbox_0_unscaled: 0.1038 (0.1024)  loss_giou_0_unscaled: 0.4699 (0.4743)  cardinality_error_0_unscaled: 292.5000 (292.3399)  loss_ce_1_unscaled: 0.8417 (0.8394)  loss_bbox_1_unscaled: 0.0941 (0.0931)  loss_giou_1_unscaled: 0.4116 (0.4307)  cardinality_error_1_unscaled: 292.5000 (292.1177)  loss_ce_2_unscaled: 0.8438 (0.8442)  loss_bbox_2_unscaled: 0.0898 (0.0910)  loss_giou_2_unscaled: 0.3964 (0.4216)  cardinality_error_2_unscaled: 292.5000 (292.2108)  loss_ce_3_unscaled: 0.8651 (0.8471)  loss_bbox_3_unscaled: 0.0888 (0.0904)  loss_giou_3_unscaled: 0.3842 (0.4175)  cardinality_error_3_unscaled: 292.5000 (292.2647)  loss_ce_4_unscaled: 0.8402 (0.8328)  loss_bbox_4_unscaled: 0.0895 (0.0906)  loss_giou_4_unscaled: 0.3843 (0.4176)  cardinality_error_4_unscaled: 292.5833 (292.3317)  time: 3.1251  data: 0.0333  max mem: 8138\n",
            "Test:  [ 60/417]  eta: 0:18:06  class_error: 47.73  loss: 18.7071 (18.2935)  loss_ce: 1.7946 (1.7347)  loss_bbox: 0.4867 (0.4597)  loss_giou: 0.9304 (0.8485)  loss_ce_0: 1.6944 (1.6929)  loss_bbox_0: 0.5407 (0.5181)  loss_giou_0: 1.0059 (0.9594)  loss_ce_1: 1.6971 (1.7016)  loss_bbox_1: 0.4970 (0.4706)  loss_giou_1: 0.9662 (0.8719)  loss_ce_2: 1.7250 (1.7108)  loss_bbox_2: 0.4893 (0.4599)  loss_giou_2: 0.9401 (0.8541)  loss_ce_3: 1.7960 (1.7180)  loss_bbox_3: 0.4755 (0.4568)  loss_giou_3: 0.9069 (0.8448)  loss_ce_4: 1.7639 (1.6902)  loss_bbox_4: 0.4786 (0.4566)  loss_giou_4: 0.9056 (0.8449)  loss_ce_unscaled: 0.8973 (0.8673)  class_error_unscaled: 60.3774 (58.0242)  loss_bbox_unscaled: 0.0973 (0.0919)  loss_giou_unscaled: 0.4652 (0.4243)  cardinality_error_unscaled: 291.0833 (292.1790)  loss_ce_0_unscaled: 0.8472 (0.8464)  loss_bbox_0_unscaled: 0.1081 (0.1036)  loss_giou_0_unscaled: 0.5030 (0.4797)  cardinality_error_0_unscaled: 291.0833 (292.2227)  loss_ce_1_unscaled: 0.8485 (0.8508)  loss_bbox_1_unscaled: 0.0994 (0.0941)  loss_giou_1_unscaled: 0.4831 (0.4360)  cardinality_error_1_unscaled: 291.1667 (292.0342)  loss_ce_2_unscaled: 0.8625 (0.8554)  loss_bbox_2_unscaled: 0.0979 (0.0920)  loss_giou_2_unscaled: 0.4700 (0.4271)  cardinality_error_2_unscaled: 291.0833 (292.1585)  loss_ce_3_unscaled: 0.8980 (0.8590)  loss_bbox_3_unscaled: 0.0951 (0.0914)  loss_giou_3_unscaled: 0.4534 (0.4224)  cardinality_error_3_unscaled: 291.0833 (292.1940)  loss_ce_4_unscaled: 0.8820 (0.8451)  loss_bbox_4_unscaled: 0.0957 (0.0913)  loss_giou_4_unscaled: 0.4528 (0.4224)  cardinality_error_4_unscaled: 291.2500 (292.2596)  time: 3.0891  data: 0.0320  max mem: 8138\n",
            "Test:  [ 70/417]  eta: 0:17:45  class_error: 60.22  loss: 18.4492 (18.1710)  loss_ce: 1.7734 (1.7197)  loss_bbox: 0.4837 (0.4578)  loss_giou: 0.8477 (0.8446)  loss_ce_0: 1.7110 (1.6813)  loss_bbox_0: 0.5296 (0.5168)  loss_giou_0: 0.9491 (0.9529)  loss_ce_1: 1.7847 (1.6879)  loss_bbox_1: 0.4830 (0.4682)  loss_giou_1: 0.8538 (0.8678)  loss_ce_2: 1.7826 (1.6963)  loss_bbox_2: 0.4653 (0.4578)  loss_giou_2: 0.8547 (0.8505)  loss_ce_3: 1.7797 (1.7022)  loss_bbox_3: 0.4755 (0.4553)  loss_giou_3: 0.8416 (0.8414)  loss_ce_4: 1.7097 (1.6736)  loss_bbox_4: 0.4770 (0.4551)  loss_giou_4: 0.8438 (0.8417)  loss_ce_unscaled: 0.8867 (0.8598)  class_error_unscaled: 59.6774 (56.9986)  loss_bbox_unscaled: 0.0967 (0.0916)  loss_giou_unscaled: 0.4239 (0.4223)  cardinality_error_unscaled: 291.8333 (292.2559)  loss_ce_0_unscaled: 0.8555 (0.8407)  loss_bbox_0_unscaled: 0.1059 (0.1034)  loss_giou_0_unscaled: 0.4746 (0.4765)  cardinality_error_0_unscaled: 291.0833 (292.3063)  loss_ce_1_unscaled: 0.8924 (0.8440)  loss_bbox_1_unscaled: 0.0966 (0.0936)  loss_giou_1_unscaled: 0.4269 (0.4339)  cardinality_error_1_unscaled: 291.3333 (292.1256)  loss_ce_2_unscaled: 0.8913 (0.8482)  loss_bbox_2_unscaled: 0.0931 (0.0916)  loss_giou_2_unscaled: 0.4273 (0.4252)  cardinality_error_2_unscaled: 291.7500 (292.2453)  loss_ce_3_unscaled: 0.8898 (0.8511)  loss_bbox_3_unscaled: 0.0951 (0.0911)  loss_giou_3_unscaled: 0.4208 (0.4207)  cardinality_error_3_unscaled: 291.7500 (292.2723)  loss_ce_4_unscaled: 0.8548 (0.8368)  loss_bbox_4_unscaled: 0.0954 (0.0910)  loss_giou_4_unscaled: 0.4219 (0.4208)  cardinality_error_4_unscaled: 291.6667 (292.3322)  time: 3.1621  data: 0.0318  max mem: 8138\n",
            "Test:  [ 80/417]  eta: 0:17:10  class_error: 67.19  loss: 18.4033 (18.1164)  loss_ce: 1.7668 (1.7200)  loss_bbox: 0.4408 (0.4566)  loss_giou: 0.7812 (0.8340)  loss_ce_0: 1.7110 (1.6873)  loss_bbox_0: 0.4923 (0.5156)  loss_giou_0: 0.8749 (0.9412)  loss_ce_1: 1.7847 (1.6936)  loss_bbox_1: 0.4565 (0.4665)  loss_giou_1: 0.7982 (0.8558)  loss_ce_2: 1.7826 (1.7008)  loss_bbox_2: 0.4476 (0.4564)  loss_giou_2: 0.7749 (0.8397)  loss_ce_3: 1.7567 (1.7048)  loss_bbox_3: 0.4267 (0.4540)  loss_giou_3: 0.7733 (0.8307)  loss_ce_4: 1.7052 (1.6740)  loss_bbox_4: 0.4446 (0.4542)  loss_giou_4: 0.7806 (0.8313)  loss_ce_unscaled: 0.8834 (0.8600)  class_error_unscaled: 52.6316 (56.7023)  loss_bbox_unscaled: 0.0882 (0.0913)  loss_giou_unscaled: 0.3906 (0.4170)  cardinality_error_unscaled: 293.0000 (292.4167)  loss_ce_0_unscaled: 0.8555 (0.8437)  loss_bbox_0_unscaled: 0.0985 (0.1031)  loss_giou_0_unscaled: 0.4374 (0.4706)  cardinality_error_0_unscaled: 293.0833 (292.4671)  loss_ce_1_unscaled: 0.8924 (0.8468)  loss_bbox_1_unscaled: 0.0913 (0.0933)  loss_giou_1_unscaled: 0.3991 (0.4279)  cardinality_error_1_unscaled: 292.8333 (292.2932)  loss_ce_2_unscaled: 0.8913 (0.8504)  loss_bbox_2_unscaled: 0.0895 (0.0913)  loss_giou_2_unscaled: 0.3874 (0.4198)  cardinality_error_2_unscaled: 293.2500 (292.4043)  loss_ce_3_unscaled: 0.8784 (0.8524)  loss_bbox_3_unscaled: 0.0853 (0.0908)  loss_giou_3_unscaled: 0.3867 (0.4154)  cardinality_error_3_unscaled: 293.1667 (292.4290)  loss_ce_4_unscaled: 0.8526 (0.8370)  loss_bbox_4_unscaled: 0.0889 (0.0908)  loss_giou_4_unscaled: 0.3903 (0.4156)  cardinality_error_4_unscaled: 293.1667 (292.4990)  time: 3.1020  data: 0.0314  max mem: 8138\n",
            "Test:  [ 90/417]  eta: 0:16:40  class_error: 44.64  loss: 16.7329 (18.1141)  loss_ce: 1.5764 (1.7188)  loss_bbox: 0.4484 (0.4533)  loss_giou: 0.8151 (0.8373)  loss_ce_0: 1.6524 (1.6872)  loss_bbox_0: 0.4814 (0.5131)  loss_giou_0: 0.9406 (0.9456)  loss_ce_1: 1.6124 (1.6937)  loss_bbox_1: 0.4329 (0.4635)  loss_giou_1: 0.8401 (0.8598)  loss_ce_2: 1.6157 (1.6997)  loss_bbox_2: 0.4476 (0.4530)  loss_giou_2: 0.8259 (0.8428)  loss_ce_3: 1.5826 (1.7033)  loss_bbox_3: 0.4267 (0.4506)  loss_giou_3: 0.8116 (0.8341)  loss_ce_4: 1.5428 (1.6727)  loss_bbox_4: 0.4552 (0.4508)  loss_giou_4: 0.8107 (0.8346)  loss_ce_unscaled: 0.7882 (0.8594)  class_error_unscaled: 55.5556 (56.6981)  loss_bbox_unscaled: 0.0897 (0.0907)  loss_giou_unscaled: 0.4075 (0.4186)  cardinality_error_unscaled: 292.4167 (292.3343)  loss_ce_0_unscaled: 0.8262 (0.8436)  loss_bbox_0_unscaled: 0.0963 (0.1026)  loss_giou_0_unscaled: 0.4703 (0.4728)  cardinality_error_0_unscaled: 292.7500 (292.3929)  loss_ce_1_unscaled: 0.8062 (0.8469)  loss_bbox_1_unscaled: 0.0866 (0.0927)  loss_giou_1_unscaled: 0.4200 (0.4299)  cardinality_error_1_unscaled: 292.3333 (292.2161)  loss_ce_2_unscaled: 0.8079 (0.8499)  loss_bbox_2_unscaled: 0.0895 (0.0906)  loss_giou_2_unscaled: 0.4130 (0.4214)  cardinality_error_2_unscaled: 292.5833 (292.3169)  loss_ce_3_unscaled: 0.7913 (0.8516)  loss_bbox_3_unscaled: 0.0853 (0.0901)  loss_giou_3_unscaled: 0.4058 (0.4170)  cardinality_error_3_unscaled: 292.5833 (292.3398)  loss_ce_4_unscaled: 0.7714 (0.8364)  loss_bbox_4_unscaled: 0.0910 (0.0902)  loss_giou_4_unscaled: 0.4054 (0.4173)  cardinality_error_4_unscaled: 292.8333 (292.4130)  time: 3.0212  data: 0.0343  max mem: 8138\n",
            "Test:  [100/417]  eta: 0:16:07  class_error: 71.13  loss: 19.1692 (18.2152)  loss_ce: 1.8840 (1.7320)  loss_bbox: 0.4484 (0.4554)  loss_giou: 0.8433 (0.8405)  loss_ce_0: 1.7773 (1.6952)  loss_bbox_0: 0.4830 (0.5136)  loss_giou_0: 0.9752 (0.9487)  loss_ce_1: 1.8342 (1.7052)  loss_bbox_1: 0.4555 (0.4654)  loss_giou_1: 0.8756 (0.8634)  loss_ce_2: 1.8752 (1.7122)  loss_bbox_2: 0.4507 (0.4552)  loss_giou_2: 0.8390 (0.8457)  loss_ce_3: 1.8995 (1.7167)  loss_bbox_3: 0.4442 (0.4528)  loss_giou_3: 0.8519 (0.8376)  loss_ce_4: 1.8430 (1.6845)  loss_bbox_4: 0.4552 (0.4530)  loss_giou_4: 0.8455 (0.8382)  loss_ce_unscaled: 0.9420 (0.8660)  class_error_unscaled: 60.7595 (57.0836)  loss_bbox_unscaled: 0.0897 (0.0911)  loss_giou_unscaled: 0.4217 (0.4203)  cardinality_error_unscaled: 291.8333 (292.3540)  loss_ce_0_unscaled: 0.8887 (0.8476)  loss_bbox_0_unscaled: 0.0966 (0.1027)  loss_giou_0_unscaled: 0.4876 (0.4743)  cardinality_error_0_unscaled: 292.0833 (292.4134)  loss_ce_1_unscaled: 0.9171 (0.8526)  loss_bbox_1_unscaled: 0.0911 (0.0931)  loss_giou_1_unscaled: 0.4378 (0.4317)  cardinality_error_1_unscaled: 291.9167 (292.2302)  loss_ce_2_unscaled: 0.9376 (0.8561)  loss_bbox_2_unscaled: 0.0901 (0.0910)  loss_giou_2_unscaled: 0.4195 (0.4229)  cardinality_error_2_unscaled: 291.9167 (292.3267)  loss_ce_3_unscaled: 0.9497 (0.8583)  loss_bbox_3_unscaled: 0.0888 (0.0906)  loss_giou_3_unscaled: 0.4260 (0.4188)  cardinality_error_3_unscaled: 292.0000 (292.3531)  loss_ce_4_unscaled: 0.9215 (0.8423)  loss_bbox_4_unscaled: 0.0910 (0.0906)  loss_giou_4_unscaled: 0.4227 (0.4191)  cardinality_error_4_unscaled: 292.0000 (292.4241)  time: 3.0329  data: 0.0337  max mem: 8138\n",
            "Test:  [110/417]  eta: 0:15:36  class_error: 54.29  loss: 20.1466 (18.4094)  loss_ce: 1.9100 (1.7479)  loss_bbox: 0.5195 (0.4630)  loss_giou: 0.9102 (0.8485)  loss_ce_0: 1.8498 (1.7103)  loss_bbox_0: 0.5782 (0.5216)  loss_giou_0: 1.0061 (0.9571)  loss_ce_1: 1.8944 (1.7233)  loss_bbox_1: 0.5328 (0.4737)  loss_giou_1: 0.9473 (0.8714)  loss_ce_2: 1.9044 (1.7297)  loss_bbox_2: 0.5132 (0.4633)  loss_giou_2: 0.9119 (0.8535)  loss_ce_3: 1.9500 (1.7336)  loss_bbox_3: 0.5117 (0.4604)  loss_giou_3: 0.9191 (0.8453)  loss_ce_4: 1.8768 (1.7002)  loss_bbox_4: 0.5101 (0.4607)  loss_giou_4: 0.9144 (0.8457)  loss_ce_unscaled: 0.9550 (0.8740)  class_error_unscaled: 64.7887 (57.7353)  loss_bbox_unscaled: 0.1039 (0.0926)  loss_giou_unscaled: 0.4551 (0.4243)  cardinality_error_unscaled: 292.0833 (292.3341)  loss_ce_0_unscaled: 0.9249 (0.8551)  loss_bbox_0_unscaled: 0.1156 (0.1043)  loss_giou_0_unscaled: 0.5030 (0.4785)  cardinality_error_0_unscaled: 292.0833 (292.4032)  loss_ce_1_unscaled: 0.9472 (0.8616)  loss_bbox_1_unscaled: 0.1066 (0.0947)  loss_giou_1_unscaled: 0.4737 (0.4357)  cardinality_error_1_unscaled: 292.0833 (292.2275)  loss_ce_2_unscaled: 0.9522 (0.8649)  loss_bbox_2_unscaled: 0.1026 (0.0927)  loss_giou_2_unscaled: 0.4560 (0.4268)  cardinality_error_2_unscaled: 292.0000 (292.3101)  loss_ce_3_unscaled: 0.9750 (0.8668)  loss_bbox_3_unscaled: 0.1023 (0.0921)  loss_giou_3_unscaled: 0.4596 (0.4227)  cardinality_error_3_unscaled: 292.0833 (292.3409)  loss_ce_4_unscaled: 0.9384 (0.8501)  loss_bbox_4_unscaled: 0.1020 (0.0921)  loss_giou_4_unscaled: 0.4572 (0.4229)  cardinality_error_4_unscaled: 292.0833 (292.4069)  time: 2.9985  data: 0.0310  max mem: 8138\n",
            "Test:  [120/417]  eta: 0:15:01  class_error: 54.55  loss: 19.0254 (18.4141)  loss_ce: 1.8625 (1.7522)  loss_bbox: 0.4434 (0.4623)  loss_giou: 0.9148 (0.8476)  loss_ce_0: 1.7416 (1.7105)  loss_bbox_0: 0.5139 (0.5193)  loss_giou_0: 1.0239 (0.9561)  loss_ce_1: 1.8057 (1.7238)  loss_bbox_1: 0.4613 (0.4728)  loss_giou_1: 0.9141 (0.8708)  loss_ce_2: 1.8202 (1.7314)  loss_bbox_2: 0.4556 (0.4628)  loss_giou_2: 0.9180 (0.8529)  loss_ce_3: 1.8840 (1.7377)  loss_bbox_3: 0.4488 (0.4600)  loss_giou_3: 0.9102 (0.8448)  loss_ce_4: 1.8577 (1.7042)  loss_bbox_4: 0.4422 (0.4600)  loss_giou_4: 0.9153 (0.8450)  loss_ce_unscaled: 0.9312 (0.8761)  class_error_unscaled: 64.7887 (58.0795)  loss_bbox_unscaled: 0.0887 (0.0925)  loss_giou_unscaled: 0.4574 (0.4238)  cardinality_error_unscaled: 291.4167 (292.2720)  loss_ce_0_unscaled: 0.8708 (0.8552)  loss_bbox_0_unscaled: 0.1028 (0.1039)  loss_giou_0_unscaled: 0.5120 (0.4780)  cardinality_error_0_unscaled: 291.5833 (292.3609)  loss_ce_1_unscaled: 0.9028 (0.8619)  loss_bbox_1_unscaled: 0.0923 (0.0946)  loss_giou_1_unscaled: 0.4571 (0.4354)  cardinality_error_1_unscaled: 291.4167 (292.1619)  loss_ce_2_unscaled: 0.9101 (0.8657)  loss_bbox_2_unscaled: 0.0911 (0.0926)  loss_giou_2_unscaled: 0.4590 (0.4265)  cardinality_error_2_unscaled: 291.4167 (292.2417)  loss_ce_3_unscaled: 0.9420 (0.8688)  loss_bbox_3_unscaled: 0.0898 (0.0920)  loss_giou_3_unscaled: 0.4551 (0.4224)  cardinality_error_3_unscaled: 291.4167 (292.2686)  loss_ce_4_unscaled: 0.9289 (0.8521)  loss_bbox_4_unscaled: 0.0884 (0.0920)  loss_giou_4_unscaled: 0.4576 (0.4225)  cardinality_error_4_unscaled: 291.4167 (292.3457)  time: 2.9409  data: 0.0309  max mem: 8138\n",
            "Test:  [130/417]  eta: 0:14:29  class_error: 55.74  loss: 18.9393 (18.4436)  loss_ce: 1.7843 (1.7542)  loss_bbox: 0.4480 (0.4656)  loss_giou: 0.8978 (0.8485)  loss_ce_0: 1.7416 (1.7120)  loss_bbox_0: 0.5095 (0.5220)  loss_giou_0: 0.9931 (0.9562)  loss_ce_1: 1.7339 (1.7240)  loss_bbox_1: 0.4770 (0.4768)  loss_giou_1: 0.9141 (0.8719)  loss_ce_2: 1.7257 (1.7321)  loss_bbox_2: 0.4750 (0.4665)  loss_giou_2: 0.8876 (0.8536)  loss_ce_3: 1.7463 (1.7367)  loss_bbox_3: 0.4622 (0.4636)  loss_giou_3: 0.8910 (0.8456)  loss_ce_4: 1.7272 (1.7053)  loss_bbox_4: 0.4400 (0.4635)  loss_giou_4: 0.9018 (0.8455)  loss_ce_unscaled: 0.8921 (0.8771)  class_error_unscaled: 60.1852 (58.1852)  loss_bbox_unscaled: 0.0896 (0.0931)  loss_giou_unscaled: 0.4489 (0.4242)  cardinality_error_unscaled: 291.5833 (292.2373)  loss_ce_0_unscaled: 0.8708 (0.8560)  loss_bbox_0_unscaled: 0.1019 (0.1044)  loss_giou_0_unscaled: 0.4965 (0.4781)  cardinality_error_0_unscaled: 292.0833 (292.3340)  loss_ce_1_unscaled: 0.8669 (0.8620)  loss_bbox_1_unscaled: 0.0954 (0.0954)  loss_giou_1_unscaled: 0.4571 (0.4359)  cardinality_error_1_unscaled: 291.5833 (292.1323)  loss_ce_2_unscaled: 0.8628 (0.8660)  loss_bbox_2_unscaled: 0.0950 (0.0933)  loss_giou_2_unscaled: 0.4438 (0.4268)  cardinality_error_2_unscaled: 291.5833 (292.2068)  loss_ce_3_unscaled: 0.8732 (0.8684)  loss_bbox_3_unscaled: 0.0924 (0.0927)  loss_giou_3_unscaled: 0.4455 (0.4228)  cardinality_error_3_unscaled: 291.5833 (292.2354)  loss_ce_4_unscaled: 0.8636 (0.8527)  loss_bbox_4_unscaled: 0.0880 (0.0927)  loss_giou_4_unscaled: 0.4509 (0.4228)  cardinality_error_4_unscaled: 291.5833 (292.3073)  time: 2.9112  data: 0.0302  max mem: 8138\n",
            "Test:  [140/417]  eta: 0:13:59  class_error: 68.00  loss: 19.3693 (18.4951)  loss_ce: 1.7843 (1.7617)  loss_bbox: 0.4706 (0.4653)  loss_giou: 0.8908 (0.8492)  loss_ce_0: 1.7900 (1.7224)  loss_bbox_0: 0.5245 (0.5209)  loss_giou_0: 0.9702 (0.9563)  loss_ce_1: 1.8064 (1.7325)  loss_bbox_1: 0.4878 (0.4764)  loss_giou_1: 0.9218 (0.8728)  loss_ce_2: 1.7313 (1.7406)  loss_bbox_2: 0.4770 (0.4663)  loss_giou_2: 0.8876 (0.8543)  loss_ce_3: 1.7463 (1.7446)  loss_bbox_3: 0.4738 (0.4634)  loss_giou_3: 0.8910 (0.8464)  loss_ce_4: 1.7272 (1.7129)  loss_bbox_4: 0.4674 (0.4631)  loss_giou_4: 0.8872 (0.8462)  loss_ce_unscaled: 0.8921 (0.8808)  class_error_unscaled: 59.3220 (58.4599)  loss_bbox_unscaled: 0.0941 (0.0931)  loss_giou_unscaled: 0.4454 (0.4246)  cardinality_error_unscaled: 291.6667 (292.1986)  loss_ce_0_unscaled: 0.8950 (0.8612)  loss_bbox_0_unscaled: 0.1049 (0.1042)  loss_giou_0_unscaled: 0.4851 (0.4782)  cardinality_error_0_unscaled: 291.6667 (292.2849)  loss_ce_1_unscaled: 0.9032 (0.8662)  loss_bbox_1_unscaled: 0.0976 (0.0953)  loss_giou_1_unscaled: 0.4609 (0.4364)  cardinality_error_1_unscaled: 291.6667 (292.0981)  loss_ce_2_unscaled: 0.8657 (0.8703)  loss_bbox_2_unscaled: 0.0954 (0.0933)  loss_giou_2_unscaled: 0.4438 (0.4271)  cardinality_error_2_unscaled: 292.0833 (292.1732)  loss_ce_3_unscaled: 0.8732 (0.8723)  loss_bbox_3_unscaled: 0.0948 (0.0927)  loss_giou_3_unscaled: 0.4455 (0.4232)  cardinality_error_3_unscaled: 291.7500 (292.1968)  loss_ce_4_unscaled: 0.8636 (0.8564)  loss_bbox_4_unscaled: 0.0935 (0.0926)  loss_giou_4_unscaled: 0.4436 (0.4231)  cardinality_error_4_unscaled: 291.7500 (292.2660)  time: 3.0150  data: 0.0319  max mem: 8138\n",
            "Test:  [150/417]  eta: 0:13:28  class_error: 51.00  loss: 18.5511 (18.4370)  loss_ce: 1.7420 (1.7558)  loss_bbox: 0.4683 (0.4649)  loss_giou: 0.8153 (0.8453)  loss_ce_0: 1.7290 (1.7165)  loss_bbox_0: 0.5097 (0.5208)  loss_giou_0: 0.9476 (0.9531)  loss_ce_1: 1.7602 (1.7273)  loss_bbox_1: 0.4584 (0.4754)  loss_giou_1: 0.8469 (0.8688)  loss_ce_2: 1.7433 (1.7359)  loss_bbox_2: 0.4662 (0.4658)  loss_giou_2: 0.8271 (0.8500)  loss_ce_3: 1.7070 (1.7397)  loss_bbox_3: 0.4625 (0.4630)  loss_giou_3: 0.8144 (0.8424)  loss_ce_4: 1.6684 (1.7076)  loss_bbox_4: 0.4621 (0.4625)  loss_giou_4: 0.8145 (0.8424)  loss_ce_unscaled: 0.8710 (0.8779)  class_error_unscaled: 57.8947 (58.3443)  loss_bbox_unscaled: 0.0937 (0.0930)  loss_giou_unscaled: 0.4077 (0.4226)  cardinality_error_unscaled: 291.6667 (292.2042)  loss_ce_0_unscaled: 0.8645 (0.8582)  loss_bbox_0_unscaled: 0.1019 (0.1042)  loss_giou_0_unscaled: 0.4738 (0.4765)  cardinality_error_0_unscaled: 291.6667 (292.2914)  loss_ce_1_unscaled: 0.8801 (0.8636)  loss_bbox_1_unscaled: 0.0917 (0.0951)  loss_giou_1_unscaled: 0.4234 (0.4344)  cardinality_error_1_unscaled: 291.6667 (292.0911)  loss_ce_2_unscaled: 0.8717 (0.8680)  loss_bbox_2_unscaled: 0.0932 (0.0932)  loss_giou_2_unscaled: 0.4135 (0.4250)  cardinality_error_2_unscaled: 292.4167 (292.1827)  loss_ce_3_unscaled: 0.8535 (0.8698)  loss_bbox_3_unscaled: 0.0925 (0.0926)  loss_giou_3_unscaled: 0.4072 (0.4212)  cardinality_error_3_unscaled: 291.7500 (292.2059)  loss_ce_4_unscaled: 0.8342 (0.8538)  loss_bbox_4_unscaled: 0.0924 (0.0925)  loss_giou_4_unscaled: 0.4073 (0.4212)  cardinality_error_4_unscaled: 291.7500 (292.2748)  time: 3.0376  data: 0.0321  max mem: 8138\n",
            "Test:  [160/417]  eta: 0:12:57  class_error: 42.00  loss: 18.5127 (18.3866)  loss_ce: 1.6327 (1.7502)  loss_bbox: 0.4484 (0.4631)  loss_giou: 0.7930 (0.8441)  loss_ce_0: 1.6080 (1.7085)  loss_bbox_0: 0.4845 (0.5194)  loss_giou_0: 0.9060 (0.9525)  loss_ce_1: 1.6091 (1.7226)  loss_bbox_1: 0.4643 (0.4738)  loss_giou_1: 0.8273 (0.8677)  loss_ce_2: 1.6154 (1.7311)  loss_bbox_2: 0.4463 (0.4641)  loss_giou_2: 0.7976 (0.8486)  loss_ce_3: 1.6251 (1.7338)  loss_bbox_3: 0.4438 (0.4612)  loss_giou_3: 0.7975 (0.8415)  loss_ce_4: 1.6067 (1.7023)  loss_bbox_4: 0.4464 (0.4607)  loss_giou_4: 0.7859 (0.8413)  loss_ce_unscaled: 0.8164 (0.8751)  class_error_unscaled: 54.2857 (57.9914)  loss_bbox_unscaled: 0.0897 (0.0926)  loss_giou_unscaled: 0.3965 (0.4220)  cardinality_error_unscaled: 292.6667 (292.2153)  loss_ce_0_unscaled: 0.8040 (0.8543)  loss_bbox_0_unscaled: 0.0969 (0.1039)  loss_giou_0_unscaled: 0.4530 (0.4763)  cardinality_error_0_unscaled: 292.9167 (292.3013)  loss_ce_1_unscaled: 0.8046 (0.8613)  loss_bbox_1_unscaled: 0.0929 (0.0948)  loss_giou_1_unscaled: 0.4137 (0.4338)  cardinality_error_1_unscaled: 292.6667 (292.1087)  loss_ce_2_unscaled: 0.8077 (0.8655)  loss_bbox_2_unscaled: 0.0893 (0.0928)  loss_giou_2_unscaled: 0.3988 (0.4243)  cardinality_error_2_unscaled: 292.8333 (292.1972)  loss_ce_3_unscaled: 0.8125 (0.8669)  loss_bbox_3_unscaled: 0.0888 (0.0922)  loss_giou_3_unscaled: 0.3987 (0.4207)  cardinality_error_3_unscaled: 292.9167 (292.2195)  loss_ce_4_unscaled: 0.8033 (0.8511)  loss_bbox_4_unscaled: 0.0893 (0.0921)  loss_giou_4_unscaled: 0.3930 (0.4206)  cardinality_error_4_unscaled: 292.9167 (292.2831)  time: 2.9915  data: 0.0314  max mem: 8138\n",
            "Test:  [170/417]  eta: 0:12:27  class_error: 58.43  loss: 18.9582 (18.3994)  loss_ce: 1.7925 (1.7499)  loss_bbox: 0.4479 (0.4638)  loss_giou: 0.8291 (0.8452)  loss_ce_0: 1.7451 (1.7101)  loss_bbox_0: 0.4932 (0.5201)  loss_giou_0: 0.9337 (0.9534)  loss_ce_1: 1.7953 (1.7242)  loss_bbox_1: 0.4643 (0.4738)  loss_giou_1: 0.8324 (0.8684)  loss_ce_2: 1.8075 (1.7319)  loss_bbox_2: 0.4463 (0.4644)  loss_giou_2: 0.8306 (0.8496)  loss_ce_3: 1.8082 (1.7342)  loss_bbox_3: 0.4438 (0.4618)  loss_giou_3: 0.8242 (0.8424)  loss_ce_4: 1.7592 (1.7029)  loss_bbox_4: 0.4464 (0.4613)  loss_giou_4: 0.8295 (0.8422)  loss_ce_unscaled: 0.8963 (0.8750)  class_error_unscaled: 59.1549 (57.8723)  loss_bbox_unscaled: 0.0896 (0.0928)  loss_giou_unscaled: 0.4146 (0.4226)  cardinality_error_unscaled: 291.4167 (292.1633)  loss_ce_0_unscaled: 0.8726 (0.8550)  loss_bbox_0_unscaled: 0.0986 (0.1040)  loss_giou_0_unscaled: 0.4669 (0.4767)  cardinality_error_0_unscaled: 291.5000 (292.2573)  loss_ce_1_unscaled: 0.8977 (0.8621)  loss_bbox_1_unscaled: 0.0929 (0.0948)  loss_giou_1_unscaled: 0.4162 (0.4342)  cardinality_error_1_unscaled: 291.4167 (292.0595)  loss_ce_2_unscaled: 0.9037 (0.8659)  loss_bbox_2_unscaled: 0.0893 (0.0929)  loss_giou_2_unscaled: 0.4153 (0.4248)  cardinality_error_2_unscaled: 291.5833 (292.1491)  loss_ce_3_unscaled: 0.9041 (0.8671)  loss_bbox_3_unscaled: 0.0888 (0.0924)  loss_giou_3_unscaled: 0.4121 (0.4212)  cardinality_error_3_unscaled: 291.5833 (292.1642)  loss_ce_4_unscaled: 0.8796 (0.8514)  loss_bbox_4_unscaled: 0.0893 (0.0923)  loss_giou_4_unscaled: 0.4148 (0.4211)  cardinality_error_4_unscaled: 291.5833 (292.2320)  time: 2.9941  data: 0.0309  max mem: 8138\n",
            "Test:  [180/417]  eta: 0:11:56  class_error: 43.55  loss: 16.5754 (18.2436)  loss_ce: 1.6543 (1.7335)  loss_bbox: 0.4147 (0.4609)  loss_giou: 0.7643 (0.8379)  loss_ce_0: 1.6219 (1.6944)  loss_bbox_0: 0.4858 (0.5180)  loss_giou_0: 0.8786 (0.9463)  loss_ce_1: 1.6468 (1.7084)  loss_bbox_1: 0.4323 (0.4709)  loss_giou_1: 0.8036 (0.8610)  loss_ce_2: 1.6328 (1.7162)  loss_bbox_2: 0.4135 (0.4614)  loss_giou_2: 0.7582 (0.8422)  loss_ce_3: 1.6357 (1.7181)  loss_bbox_3: 0.4124 (0.4588)  loss_giou_3: 0.7602 (0.8351)  loss_ce_4: 1.6179 (1.6874)  loss_bbox_4: 0.4128 (0.4583)  loss_giou_4: 0.7631 (0.8349)  loss_ce_unscaled: 0.8271 (0.8667)  class_error_unscaled: 57.3913 (57.3145)  loss_bbox_unscaled: 0.0829 (0.0922)  loss_giou_unscaled: 0.3821 (0.4189)  cardinality_error_unscaled: 292.0833 (292.1676)  loss_ce_0_unscaled: 0.8109 (0.8472)  loss_bbox_0_unscaled: 0.0972 (0.1036)  loss_giou_0_unscaled: 0.4393 (0.4732)  cardinality_error_0_unscaled: 292.0833 (292.2721)  loss_ce_1_unscaled: 0.8234 (0.8542)  loss_bbox_1_unscaled: 0.0865 (0.0942)  loss_giou_1_unscaled: 0.4018 (0.4305)  cardinality_error_1_unscaled: 292.0833 (292.0640)  loss_ce_2_unscaled: 0.8164 (0.8581)  loss_bbox_2_unscaled: 0.0827 (0.0923)  loss_giou_2_unscaled: 0.3791 (0.4211)  cardinality_error_2_unscaled: 292.0833 (292.1510)  loss_ce_3_unscaled: 0.8179 (0.8591)  loss_bbox_3_unscaled: 0.0825 (0.0918)  loss_giou_3_unscaled: 0.3801 (0.4176)  cardinality_error_3_unscaled: 292.0000 (292.1671)  loss_ce_4_unscaled: 0.8090 (0.8437)  loss_bbox_4_unscaled: 0.0826 (0.0917)  loss_giou_4_unscaled: 0.3816 (0.4174)  cardinality_error_4_unscaled: 292.0833 (292.2390)  time: 2.9969  data: 0.0311  max mem: 8138\n",
            "Test:  [190/417]  eta: 0:11:25  class_error: 67.50  loss: 16.5754 (18.3516)  loss_ce: 1.6749 (1.7479)  loss_bbox: 0.4265 (0.4633)  loss_giou: 0.7643 (0.8392)  loss_ce_0: 1.6398 (1.7079)  loss_bbox_0: 0.4863 (0.5205)  loss_giou_0: 0.8693 (0.9476)  loss_ce_1: 1.6937 (1.7234)  loss_bbox_1: 0.4371 (0.4729)  loss_giou_1: 0.7881 (0.8623)  loss_ce_2: 1.7397 (1.7309)  loss_bbox_2: 0.4151 (0.4636)  loss_giou_2: 0.7582 (0.8436)  loss_ce_3: 1.7138 (1.7325)  loss_bbox_3: 0.4131 (0.4610)  loss_giou_3: 0.7602 (0.8366)  loss_ce_4: 1.6688 (1.7017)  loss_bbox_4: 0.4170 (0.4606)  loss_giou_4: 0.7631 (0.8362)  loss_ce_unscaled: 0.8374 (0.8740)  class_error_unscaled: 60.0000 (57.8948)  loss_bbox_unscaled: 0.0853 (0.0927)  loss_giou_unscaled: 0.3821 (0.4196)  cardinality_error_unscaled: 293.2500 (292.1889)  loss_ce_0_unscaled: 0.8199 (0.8540)  loss_bbox_0_unscaled: 0.0973 (0.1041)  loss_giou_0_unscaled: 0.4346 (0.4738)  cardinality_error_0_unscaled: 293.3333 (292.3002)  loss_ce_1_unscaled: 0.8469 (0.8617)  loss_bbox_1_unscaled: 0.0874 (0.0946)  loss_giou_1_unscaled: 0.3941 (0.4311)  cardinality_error_1_unscaled: 293.1667 (292.0785)  loss_ce_2_unscaled: 0.8699 (0.8654)  loss_bbox_2_unscaled: 0.0830 (0.0927)  loss_giou_2_unscaled: 0.3791 (0.4218)  cardinality_error_2_unscaled: 293.2500 (292.1710)  loss_ce_3_unscaled: 0.8569 (0.8663)  loss_bbox_3_unscaled: 0.0826 (0.0922)  loss_giou_3_unscaled: 0.3801 (0.4183)  cardinality_error_3_unscaled: 293.2500 (292.1798)  loss_ce_4_unscaled: 0.8344 (0.8508)  loss_bbox_4_unscaled: 0.0834 (0.0921)  loss_giou_4_unscaled: 0.3816 (0.4181)  cardinality_error_4_unscaled: 293.2500 (292.2631)  time: 2.9909  data: 0.0333  max mem: 8138\n",
            "Test:  [200/417]  eta: 0:10:54  class_error: 71.82  loss: 18.3838 (18.3400)  loss_ce: 1.6749 (1.7435)  loss_bbox: 0.4794 (0.4654)  loss_giou: 0.8383 (0.8405)  loss_ce_0: 1.6911 (1.7011)  loss_bbox_0: 0.5317 (0.5228)  loss_giou_0: 0.9575 (0.9490)  loss_ce_1: 1.7003 (1.7175)  loss_bbox_1: 0.4729 (0.4750)  loss_giou_1: 0.8713 (0.8636)  loss_ce_2: 1.7397 (1.7251)  loss_bbox_2: 0.4735 (0.4658)  loss_giou_2: 0.8621 (0.8453)  loss_ce_3: 1.7191 (1.7274)  loss_bbox_3: 0.4753 (0.4632)  loss_giou_3: 0.8328 (0.8379)  loss_ce_4: 1.6996 (1.6967)  loss_bbox_4: 0.4763 (0.4628)  loss_giou_4: 0.8348 (0.8375)  loss_ce_unscaled: 0.8374 (0.8718)  class_error_unscaled: 63.6364 (57.6136)  loss_bbox_unscaled: 0.0959 (0.0931)  loss_giou_unscaled: 0.4191 (0.4203)  cardinality_error_unscaled: 291.8333 (292.1820)  loss_ce_0_unscaled: 0.8455 (0.8506)  loss_bbox_0_unscaled: 0.1063 (0.1046)  loss_giou_0_unscaled: 0.4788 (0.4745)  cardinality_error_0_unscaled: 292.6667 (292.3081)  loss_ce_1_unscaled: 0.8502 (0.8587)  loss_bbox_1_unscaled: 0.0946 (0.0950)  loss_giou_1_unscaled: 0.4357 (0.4318)  cardinality_error_1_unscaled: 291.5000 (292.0576)  loss_ce_2_unscaled: 0.8699 (0.8625)  loss_bbox_2_unscaled: 0.0947 (0.0932)  loss_giou_2_unscaled: 0.4310 (0.4226)  cardinality_error_2_unscaled: 291.7500 (292.1770)  loss_ce_3_unscaled: 0.8595 (0.8637)  loss_bbox_3_unscaled: 0.0951 (0.0926)  loss_giou_3_unscaled: 0.4164 (0.4190)  cardinality_error_3_unscaled: 292.0000 (292.1853)  loss_ce_4_unscaled: 0.8498 (0.8483)  loss_bbox_4_unscaled: 0.0953 (0.0926)  loss_giou_4_unscaled: 0.4174 (0.4187)  cardinality_error_4_unscaled: 292.3333 (292.2558)  time: 2.9509  data: 0.0316  max mem: 8138\n",
            "Test:  [210/417]  eta: 0:10:23  class_error: 63.51  loss: 19.2131 (18.4240)  loss_ce: 1.6436 (1.7512)  loss_bbox: 0.4794 (0.4668)  loss_giou: 0.8625 (0.8454)  loss_ce_0: 1.5150 (1.7090)  loss_bbox_0: 0.5317 (0.5236)  loss_giou_0: 0.9736 (0.9534)  loss_ce_1: 1.6063 (1.7256)  loss_bbox_1: 0.5009 (0.4768)  loss_giou_1: 0.8843 (0.8687)  loss_ce_2: 1.6294 (1.7331)  loss_bbox_2: 0.4846 (0.4673)  loss_giou_2: 0.8874 (0.8501)  loss_ce_3: 1.6106 (1.7354)  loss_bbox_3: 0.4753 (0.4645)  loss_giou_3: 0.8598 (0.8427)  loss_ce_4: 1.5721 (1.7042)  loss_bbox_4: 0.4763 (0.4641)  loss_giou_4: 0.8591 (0.8423)  loss_ce_unscaled: 0.8218 (0.8756)  class_error_unscaled: 55.7522 (57.7327)  loss_bbox_unscaled: 0.0959 (0.0934)  loss_giou_unscaled: 0.4312 (0.4227)  cardinality_error_unscaled: 291.7500 (292.1651)  loss_ce_0_unscaled: 0.7575 (0.8545)  loss_bbox_0_unscaled: 0.1063 (0.1047)  loss_giou_0_unscaled: 0.4868 (0.4767)  cardinality_error_0_unscaled: 291.6667 (292.2856)  loss_ce_1_unscaled: 0.8032 (0.8628)  loss_bbox_1_unscaled: 0.1002 (0.0954)  loss_giou_1_unscaled: 0.4421 (0.4343)  cardinality_error_1_unscaled: 291.5000 (292.0257)  loss_ce_2_unscaled: 0.8147 (0.8666)  loss_bbox_2_unscaled: 0.0969 (0.0935)  loss_giou_2_unscaled: 0.4437 (0.4251)  cardinality_error_2_unscaled: 291.7500 (292.1608)  loss_ce_3_unscaled: 0.8053 (0.8677)  loss_bbox_3_unscaled: 0.0951 (0.0929)  loss_giou_3_unscaled: 0.4299 (0.4213)  cardinality_error_3_unscaled: 291.7500 (292.1687)  loss_ce_4_unscaled: 0.7860 (0.8521)  loss_bbox_4_unscaled: 0.0953 (0.0928)  loss_giou_4_unscaled: 0.4295 (0.4211)  cardinality_error_4_unscaled: 291.7500 (292.2421)  time: 2.9315  data: 0.0305  max mem: 8138\n",
            "Test:  [220/417]  eta: 0:09:53  class_error: 52.34  loss: 19.9042 (18.4437)  loss_ce: 1.8289 (1.7495)  loss_bbox: 0.4665 (0.4678)  loss_giou: 0.9121 (0.8500)  loss_ce_0: 1.7991 (1.7070)  loss_bbox_0: 0.4899 (0.5233)  loss_giou_0: 0.9928 (0.9572)  loss_ce_1: 1.8528 (1.7234)  loss_bbox_1: 0.4686 (0.4776)  loss_giou_1: 0.9435 (0.8729)  loss_ce_2: 1.8296 (1.7307)  loss_bbox_2: 0.4779 (0.4685)  loss_giou_2: 0.9210 (0.8544)  loss_ce_3: 1.8461 (1.7336)  loss_bbox_3: 0.4628 (0.4657)  loss_giou_3: 0.9118 (0.8473)  loss_ce_4: 1.7936 (1.7027)  loss_bbox_4: 0.4651 (0.4652)  loss_giou_4: 0.9104 (0.8469)  loss_ce_unscaled: 0.9144 (0.8747)  class_error_unscaled: 55.7522 (57.6218)  loss_bbox_unscaled: 0.0933 (0.0936)  loss_giou_unscaled: 0.4561 (0.4250)  cardinality_error_unscaled: 291.6667 (292.1211)  loss_ce_0_unscaled: 0.8996 (0.8535)  loss_bbox_0_unscaled: 0.0980 (0.1047)  loss_giou_0_unscaled: 0.4964 (0.4786)  cardinality_error_0_unscaled: 291.4167 (292.2440)  loss_ce_1_unscaled: 0.9264 (0.8617)  loss_bbox_1_unscaled: 0.0937 (0.0955)  loss_giou_1_unscaled: 0.4717 (0.4364)  cardinality_error_1_unscaled: 291.5000 (291.9906)  loss_ce_2_unscaled: 0.9148 (0.8653)  loss_bbox_2_unscaled: 0.0956 (0.0937)  loss_giou_2_unscaled: 0.4605 (0.4272)  cardinality_error_2_unscaled: 291.7500 (292.1207)  loss_ce_3_unscaled: 0.9230 (0.8668)  loss_bbox_3_unscaled: 0.0926 (0.0931)  loss_giou_3_unscaled: 0.4559 (0.4237)  cardinality_error_3_unscaled: 291.5000 (292.1260)  loss_ce_4_unscaled: 0.8968 (0.8513)  loss_bbox_4_unscaled: 0.0930 (0.0930)  loss_giou_4_unscaled: 0.4552 (0.4235)  cardinality_error_4_unscaled: 291.7500 (292.1938)  time: 2.9981  data: 0.0316  max mem: 8138\n",
            "Test:  [230/417]  eta: 0:09:22  class_error: 57.50  loss: 18.0816 (18.4307)  loss_ce: 1.7085 (1.7498)  loss_bbox: 0.4413 (0.4656)  loss_giou: 0.9066 (0.8500)  loss_ce_0: 1.6404 (1.7062)  loss_bbox_0: 0.4864 (0.5217)  loss_giou_0: 0.9832 (0.9576)  loss_ce_1: 1.6776 (1.7230)  loss_bbox_1: 0.4492 (0.4755)  loss_giou_1: 0.9174 (0.8726)  loss_ce_2: 1.6517 (1.7308)  loss_bbox_2: 0.4439 (0.4664)  loss_giou_2: 0.9063 (0.8542)  loss_ce_3: 1.6714 (1.7340)  loss_bbox_3: 0.4392 (0.4635)  loss_giou_3: 0.8970 (0.8470)  loss_ce_4: 1.6500 (1.7031)  loss_bbox_4: 0.4293 (0.4630)  loss_giou_4: 0.8990 (0.8469)  loss_ce_unscaled: 0.8542 (0.8749)  class_error_unscaled: 55.3191 (57.5813)  loss_bbox_unscaled: 0.0883 (0.0931)  loss_giou_unscaled: 0.4533 (0.4250)  cardinality_error_unscaled: 292.0000 (292.1331)  loss_ce_0_unscaled: 0.8202 (0.8531)  loss_bbox_0_unscaled: 0.0973 (0.1043)  loss_giou_0_unscaled: 0.4916 (0.4788)  cardinality_error_0_unscaled: 292.2500 (292.2684)  loss_ce_1_unscaled: 0.8388 (0.8615)  loss_bbox_1_unscaled: 0.0898 (0.0951)  loss_giou_1_unscaled: 0.4587 (0.4363)  cardinality_error_1_unscaled: 291.8333 (291.9975)  loss_ce_2_unscaled: 0.8258 (0.8654)  loss_bbox_2_unscaled: 0.0888 (0.0933)  loss_giou_2_unscaled: 0.4532 (0.4271)  cardinality_error_2_unscaled: 292.2500 (292.1403)  loss_ce_3_unscaled: 0.8357 (0.8670)  loss_bbox_3_unscaled: 0.0878 (0.0927)  loss_giou_3_unscaled: 0.4485 (0.4235)  cardinality_error_3_unscaled: 292.2500 (292.1548)  loss_ce_4_unscaled: 0.8250 (0.8516)  loss_bbox_4_unscaled: 0.0859 (0.0926)  loss_giou_4_unscaled: 0.4495 (0.4234)  cardinality_error_4_unscaled: 292.0833 (292.2074)  time: 2.9605  data: 0.0312  max mem: 8138\n",
            "Test:  [240/417]  eta: 0:08:52  class_error: 64.08  loss: 17.3765 (18.3849)  loss_ce: 1.5643 (1.7437)  loss_bbox: 0.4437 (0.4655)  loss_giou: 0.8076 (0.8484)  loss_ce_0: 1.6150 (1.7010)  loss_bbox_0: 0.4938 (0.5214)  loss_giou_0: 0.8984 (0.9557)  loss_ce_1: 1.6023 (1.7175)  loss_bbox_1: 0.4551 (0.4754)  loss_giou_1: 0.8171 (0.8710)  loss_ce_2: 1.5417 (1.7247)  loss_bbox_2: 0.4459 (0.4664)  loss_giou_2: 0.7979 (0.8526)  loss_ce_3: 1.5847 (1.7281)  loss_bbox_3: 0.4411 (0.4633)  loss_giou_3: 0.7969 (0.8452)  loss_ce_4: 1.5267 (1.6970)  loss_bbox_4: 0.4347 (0.4629)  loss_giou_4: 0.7975 (0.8453)  loss_ce_unscaled: 0.7821 (0.8718)  class_error_unscaled: 56.5217 (57.4333)  loss_bbox_unscaled: 0.0887 (0.0931)  loss_giou_unscaled: 0.4038 (0.4242)  cardinality_error_unscaled: 292.2500 (292.1262)  loss_ce_0_unscaled: 0.8075 (0.8505)  loss_bbox_0_unscaled: 0.0988 (0.1043)  loss_giou_0_unscaled: 0.4492 (0.4779)  cardinality_error_0_unscaled: 292.4167 (292.2673)  loss_ce_1_unscaled: 0.8011 (0.8587)  loss_bbox_1_unscaled: 0.0910 (0.0951)  loss_giou_1_unscaled: 0.4085 (0.4355)  cardinality_error_1_unscaled: 291.9167 (291.9962)  loss_ce_2_unscaled: 0.7708 (0.8623)  loss_bbox_2_unscaled: 0.0892 (0.0933)  loss_giou_2_unscaled: 0.3990 (0.4263)  cardinality_error_2_unscaled: 292.2500 (292.1349)  loss_ce_3_unscaled: 0.7924 (0.8640)  loss_bbox_3_unscaled: 0.0882 (0.0927)  loss_giou_3_unscaled: 0.3984 (0.4226)  cardinality_error_3_unscaled: 292.2500 (292.1487)  loss_ce_4_unscaled: 0.7634 (0.8485)  loss_bbox_4_unscaled: 0.0869 (0.0926)  loss_giou_4_unscaled: 0.3987 (0.4227)  cardinality_error_4_unscaled: 292.4167 (292.2006)  time: 2.9474  data: 0.0312  max mem: 8138\n",
            "Test:  [250/417]  eta: 0:08:22  class_error: 28.30  loss: 16.7552 (18.3369)  loss_ce: 1.5302 (1.7403)  loss_bbox: 0.4522 (0.4630)  loss_giou: 0.8076 (0.8462)  loss_ce_0: 1.5311 (1.6974)  loss_bbox_0: 0.4992 (0.5194)  loss_giou_0: 0.8914 (0.9545)  loss_ce_1: 1.5623 (1.7140)  loss_bbox_1: 0.4551 (0.4728)  loss_giou_1: 0.8171 (0.8690)  loss_ce_2: 1.5214 (1.7210)  loss_bbox_2: 0.4579 (0.4641)  loss_giou_2: 0.7979 (0.8502)  loss_ce_3: 1.5243 (1.7240)  loss_bbox_3: 0.4522 (0.4608)  loss_giou_3: 0.7969 (0.8430)  loss_ce_4: 1.4728 (1.6936)  loss_bbox_4: 0.4497 (0.4603)  loss_giou_4: 0.7975 (0.8431)  loss_ce_unscaled: 0.7651 (0.8702)  class_error_unscaled: 50.4762 (57.2714)  loss_bbox_unscaled: 0.0904 (0.0926)  loss_giou_unscaled: 0.4038 (0.4231)  cardinality_error_unscaled: 292.1667 (292.1491)  loss_ce_0_unscaled: 0.7655 (0.8487)  loss_bbox_0_unscaled: 0.0998 (0.1039)  loss_giou_0_unscaled: 0.4457 (0.4773)  cardinality_error_0_unscaled: 292.0833 (292.2895)  loss_ce_1_unscaled: 0.7812 (0.8570)  loss_bbox_1_unscaled: 0.0910 (0.0946)  loss_giou_1_unscaled: 0.4085 (0.4345)  cardinality_error_1_unscaled: 291.9167 (292.0229)  loss_ce_2_unscaled: 0.7607 (0.8605)  loss_bbox_2_unscaled: 0.0916 (0.0928)  loss_giou_2_unscaled: 0.3990 (0.4251)  cardinality_error_2_unscaled: 292.0833 (292.1634)  loss_ce_3_unscaled: 0.7621 (0.8620)  loss_bbox_3_unscaled: 0.0904 (0.0922)  loss_giou_3_unscaled: 0.3984 (0.4215)  cardinality_error_3_unscaled: 292.1667 (292.1770)  loss_ce_4_unscaled: 0.7364 (0.8468)  loss_bbox_4_unscaled: 0.0899 (0.0921)  loss_giou_4_unscaled: 0.3987 (0.4215)  cardinality_error_4_unscaled: 292.0833 (292.2241)  time: 3.0422  data: 0.0312  max mem: 8138\n",
            "Test:  [260/417]  eta: 0:07:52  class_error: 82.14  loss: 17.2228 (18.3605)  loss_ce: 1.6429 (1.7423)  loss_bbox: 0.4591 (0.4642)  loss_giou: 0.8157 (0.8460)  loss_ce_0: 1.5993 (1.7010)  loss_bbox_0: 0.5011 (0.5208)  loss_giou_0: 0.9637 (0.9547)  loss_ce_1: 1.6039 (1.7169)  loss_bbox_1: 0.4622 (0.4742)  loss_giou_1: 0.8797 (0.8691)  loss_ce_2: 1.6903 (1.7242)  loss_bbox_2: 0.4615 (0.4654)  loss_giou_2: 0.8297 (0.8502)  loss_ce_3: 1.6933 (1.7260)  loss_bbox_3: 0.4662 (0.4621)  loss_giou_3: 0.8107 (0.8429)  loss_ce_4: 1.6427 (1.6954)  loss_bbox_4: 0.4610 (0.4617)  loss_giou_4: 0.8150 (0.8430)  loss_ce_unscaled: 0.8214 (0.8711)  class_error_unscaled: 56.5217 (57.3910)  loss_bbox_unscaled: 0.0918 (0.0928)  loss_giou_unscaled: 0.4079 (0.4230)  cardinality_error_unscaled: 293.4167 (292.1769)  loss_ce_0_unscaled: 0.7996 (0.8505)  loss_bbox_0_unscaled: 0.1002 (0.1042)  loss_giou_0_unscaled: 0.4819 (0.4774)  cardinality_error_0_unscaled: 293.4167 (292.3078)  loss_ce_1_unscaled: 0.8020 (0.8585)  loss_bbox_1_unscaled: 0.0924 (0.0948)  loss_giou_1_unscaled: 0.4399 (0.4346)  cardinality_error_1_unscaled: 293.4167 (292.0511)  loss_ce_2_unscaled: 0.8452 (0.8621)  loss_bbox_2_unscaled: 0.0923 (0.0931)  loss_giou_2_unscaled: 0.4148 (0.4251)  cardinality_error_2_unscaled: 293.4167 (292.1897)  loss_ce_3_unscaled: 0.8467 (0.8630)  loss_bbox_3_unscaled: 0.0932 (0.0924)  loss_giou_3_unscaled: 0.4053 (0.4215)  cardinality_error_3_unscaled: 293.5833 (292.2040)  loss_ce_4_unscaled: 0.8214 (0.8477)  loss_bbox_4_unscaled: 0.0922 (0.0923)  loss_giou_4_unscaled: 0.4075 (0.4215)  cardinality_error_4_unscaled: 293.4167 (292.2510)  time: 3.0153  data: 0.0308  max mem: 8138\n",
            "Test:  [270/417]  eta: 0:07:23  class_error: 39.74  loss: 17.1601 (18.2758)  loss_ce: 1.5825 (1.7339)  loss_bbox: 0.4855 (0.4626)  loss_giou: 0.7344 (0.8418)  loss_ce_0: 1.5704 (1.6935)  loss_bbox_0: 0.5265 (0.5189)  loss_giou_0: 0.8728 (0.9504)  loss_ce_1: 1.5720 (1.7088)  loss_bbox_1: 0.4809 (0.4724)  loss_giou_1: 0.7823 (0.8648)  loss_ce_2: 1.5639 (1.7157)  loss_bbox_2: 0.4815 (0.4637)  loss_giou_2: 0.7676 (0.8459)  loss_ce_3: 1.6220 (1.7176)  loss_bbox_3: 0.4732 (0.4606)  loss_giou_3: 0.7443 (0.8388)  loss_ce_4: 1.5655 (1.6875)  loss_bbox_4: 0.4752 (0.4600)  loss_giou_4: 0.7452 (0.8389)  loss_ce_unscaled: 0.7912 (0.8670)  class_error_unscaled: 54.3307 (57.0918)  loss_bbox_unscaled: 0.0971 (0.0925)  loss_giou_unscaled: 0.3672 (0.4209)  cardinality_error_unscaled: 292.0833 (292.1178)  loss_ce_0_unscaled: 0.7852 (0.8468)  loss_bbox_0_unscaled: 0.1053 (0.1038)  loss_giou_0_unscaled: 0.4364 (0.4752)  cardinality_error_0_unscaled: 292.0833 (292.2512)  loss_ce_1_unscaled: 0.7860 (0.8544)  loss_bbox_1_unscaled: 0.0962 (0.0945)  loss_giou_1_unscaled: 0.3912 (0.4324)  cardinality_error_1_unscaled: 292.0833 (291.9619)  loss_ce_2_unscaled: 0.7820 (0.8579)  loss_bbox_2_unscaled: 0.0963 (0.0927)  loss_giou_2_unscaled: 0.3838 (0.4229)  cardinality_error_2_unscaled: 292.0833 (292.1390)  loss_ce_3_unscaled: 0.8110 (0.8588)  loss_bbox_3_unscaled: 0.0946 (0.0921)  loss_giou_3_unscaled: 0.3721 (0.4194)  cardinality_error_3_unscaled: 292.0833 (292.1473)  loss_ce_4_unscaled: 0.7828 (0.8438)  loss_bbox_4_unscaled: 0.0950 (0.0920)  loss_giou_4_unscaled: 0.3726 (0.4194)  cardinality_error_4_unscaled: 292.0833 (292.1965)  time: 3.0932  data: 0.0308  max mem: 8138\n",
            "Test:  [280/417]  eta: 0:06:52  class_error: 62.69  loss: 17.1601 (18.2690)  loss_ce: 1.5644 (1.7314)  loss_bbox: 0.4480 (0.4625)  loss_giou: 0.8191 (0.8436)  loss_ce_0: 1.5097 (1.6912)  loss_bbox_0: 0.5023 (0.5184)  loss_giou_0: 0.9352 (0.9524)  loss_ce_1: 1.5169 (1.7059)  loss_bbox_1: 0.4633 (0.4721)  loss_giou_1: 0.8447 (0.8666)  loss_ce_2: 1.5314 (1.7126)  loss_bbox_2: 0.4435 (0.4635)  loss_giou_2: 0.8262 (0.8477)  loss_ce_3: 1.5070 (1.7148)  loss_bbox_3: 0.4519 (0.4602)  loss_giou_3: 0.8376 (0.8405)  loss_ce_4: 1.5163 (1.6851)  loss_bbox_4: 0.4432 (0.4599)  loss_giou_4: 0.8259 (0.8406)  loss_ce_unscaled: 0.7822 (0.8657)  class_error_unscaled: 49.6552 (56.9796)  loss_bbox_unscaled: 0.0896 (0.0925)  loss_giou_unscaled: 0.4095 (0.4218)  cardinality_error_unscaled: 290.8333 (292.0682)  loss_ce_0_unscaled: 0.7548 (0.8456)  loss_bbox_0_unscaled: 0.1005 (0.1037)  loss_giou_0_unscaled: 0.4676 (0.4762)  cardinality_error_0_unscaled: 290.8333 (292.1919)  loss_ce_1_unscaled: 0.7584 (0.8530)  loss_bbox_1_unscaled: 0.0927 (0.0944)  loss_giou_1_unscaled: 0.4224 (0.4333)  cardinality_error_1_unscaled: 290.8333 (291.9149)  loss_ce_2_unscaled: 0.7657 (0.8563)  loss_bbox_2_unscaled: 0.0887 (0.0927)  loss_giou_2_unscaled: 0.4131 (0.4239)  cardinality_error_2_unscaled: 290.8333 (292.0893)  loss_ce_3_unscaled: 0.7535 (0.8574)  loss_bbox_3_unscaled: 0.0904 (0.0920)  loss_giou_3_unscaled: 0.4188 (0.4203)  cardinality_error_3_unscaled: 290.8333 (292.0964)  loss_ce_4_unscaled: 0.7581 (0.8425)  loss_bbox_4_unscaled: 0.0886 (0.0920)  loss_giou_4_unscaled: 0.4130 (0.4203)  cardinality_error_4_unscaled: 290.8333 (292.1438)  time: 3.0440  data: 0.0305  max mem: 8138\n",
            "Test:  [290/417]  eta: 0:06:22  class_error: 51.52  loss: 17.7218 (18.2780)  loss_ce: 1.6462 (1.7331)  loss_bbox: 0.4480 (0.4630)  loss_giou: 0.8666 (0.8426)  loss_ce_0: 1.6145 (1.6941)  loss_bbox_0: 0.5023 (0.5189)  loss_giou_0: 0.9738 (0.9512)  loss_ce_1: 1.6262 (1.7082)  loss_bbox_1: 0.4633 (0.4728)  loss_giou_1: 0.8912 (0.8655)  loss_ce_2: 1.6145 (1.7145)  loss_bbox_2: 0.4435 (0.4639)  loss_giou_2: 0.8738 (0.8468)  loss_ce_3: 1.6270 (1.7167)  loss_bbox_3: 0.4519 (0.4606)  loss_giou_3: 0.8675 (0.8395)  loss_ce_4: 1.6158 (1.6866)  loss_bbox_4: 0.4419 (0.4603)  loss_giou_4: 0.8665 (0.8396)  loss_ce_unscaled: 0.8231 (0.8666)  class_error_unscaled: 55.7823 (57.0566)  loss_bbox_unscaled: 0.0896 (0.0926)  loss_giou_unscaled: 0.4333 (0.4213)  cardinality_error_unscaled: 291.5833 (292.1140)  loss_ce_0_unscaled: 0.8073 (0.8470)  loss_bbox_0_unscaled: 0.1005 (0.1038)  loss_giou_0_unscaled: 0.4869 (0.4756)  cardinality_error_0_unscaled: 291.5833 (292.2325)  loss_ce_1_unscaled: 0.8131 (0.8541)  loss_bbox_1_unscaled: 0.0927 (0.0946)  loss_giou_1_unscaled: 0.4456 (0.4327)  cardinality_error_1_unscaled: 291.5833 (291.9631)  loss_ce_2_unscaled: 0.8073 (0.8573)  loss_bbox_2_unscaled: 0.0887 (0.0928)  loss_giou_2_unscaled: 0.4369 (0.4234)  cardinality_error_2_unscaled: 291.5833 (292.1326)  loss_ce_3_unscaled: 0.8135 (0.8584)  loss_bbox_3_unscaled: 0.0904 (0.0921)  loss_giou_3_unscaled: 0.4337 (0.4198)  cardinality_error_3_unscaled: 291.5833 (292.1400)  loss_ce_4_unscaled: 0.8079 (0.8433)  loss_bbox_4_unscaled: 0.0884 (0.0921)  loss_giou_4_unscaled: 0.4333 (0.4198)  cardinality_error_4_unscaled: 291.5833 (292.1870)  time: 2.9166  data: 0.0313  max mem: 8138\n",
            "Test:  [300/417]  eta: 0:05:51  class_error: 50.96  loss: 18.2882 (18.3048)  loss_ce: 1.6958 (1.7356)  loss_bbox: 0.4491 (0.4636)  loss_giou: 0.8666 (0.8446)  loss_ce_0: 1.7225 (1.6957)  loss_bbox_0: 0.5008 (0.5194)  loss_giou_0: 0.9858 (0.9528)  loss_ce_1: 1.6558 (1.7101)  loss_bbox_1: 0.4643 (0.4734)  loss_giou_1: 0.8912 (0.8676)  loss_ce_2: 1.7087 (1.7161)  loss_bbox_2: 0.4497 (0.4645)  loss_giou_2: 0.8738 (0.8490)  loss_ce_3: 1.6823 (1.7186)  loss_bbox_3: 0.4519 (0.4612)  loss_giou_3: 0.8675 (0.8416)  loss_ce_4: 1.6531 (1.6884)  loss_bbox_4: 0.4478 (0.4610)  loss_giou_4: 0.8665 (0.8417)  loss_ce_unscaled: 0.8479 (0.8678)  class_error_unscaled: 57.6923 (57.1347)  loss_bbox_unscaled: 0.0898 (0.0927)  loss_giou_unscaled: 0.4333 (0.4223)  cardinality_error_unscaled: 292.5833 (292.1119)  loss_ce_0_unscaled: 0.8612 (0.8478)  loss_bbox_0_unscaled: 0.1002 (0.1039)  loss_giou_0_unscaled: 0.4929 (0.4764)  cardinality_error_0_unscaled: 292.8333 (292.2309)  loss_ce_1_unscaled: 0.8279 (0.8550)  loss_bbox_1_unscaled: 0.0929 (0.0947)  loss_giou_1_unscaled: 0.4456 (0.4338)  cardinality_error_1_unscaled: 292.6667 (291.9637)  loss_ce_2_unscaled: 0.8544 (0.8580)  loss_bbox_2_unscaled: 0.0899 (0.0929)  loss_giou_2_unscaled: 0.4369 (0.4245)  cardinality_error_2_unscaled: 292.5000 (292.1296)  loss_ce_3_unscaled: 0.8412 (0.8593)  loss_bbox_3_unscaled: 0.0904 (0.0922)  loss_giou_3_unscaled: 0.4337 (0.4208)  cardinality_error_3_unscaled: 292.5833 (292.1387)  loss_ce_4_unscaled: 0.8265 (0.8442)  loss_bbox_4_unscaled: 0.0896 (0.0922)  loss_giou_4_unscaled: 0.4333 (0.4208)  cardinality_error_4_unscaled: 292.5833 (292.1836)  time: 2.9500  data: 0.0330  max mem: 8138\n",
            "Test:  [310/417]  eta: 0:05:21  class_error: 81.52  loss: 18.0632 (18.2981)  loss_ce: 1.6900 (1.7356)  loss_bbox: 0.4434 (0.4630)  loss_giou: 0.8554 (0.8441)  loss_ce_0: 1.6459 (1.6958)  loss_bbox_0: 0.4888 (0.5190)  loss_giou_0: 0.9720 (0.9525)  loss_ce_1: 1.7035 (1.7106)  loss_bbox_1: 0.4540 (0.4727)  loss_giou_1: 0.8565 (0.8669)  loss_ce_2: 1.6933 (1.7163)  loss_bbox_2: 0.4480 (0.4638)  loss_giou_2: 0.8604 (0.8484)  loss_ce_3: 1.6938 (1.7185)  loss_bbox_3: 0.4399 (0.4605)  loss_giou_3: 0.8540 (0.8409)  loss_ce_4: 1.6395 (1.6881)  loss_bbox_4: 0.4421 (0.4603)  loss_giou_4: 0.8541 (0.8411)  loss_ce_unscaled: 0.8450 (0.8678)  class_error_unscaled: 58.5859 (57.1763)  loss_bbox_unscaled: 0.0887 (0.0926)  loss_giou_unscaled: 0.4277 (0.4220)  cardinality_error_unscaled: 292.4167 (292.1490)  loss_ce_0_unscaled: 0.8229 (0.8479)  loss_bbox_0_unscaled: 0.0978 (0.1038)  loss_giou_0_unscaled: 0.4860 (0.4763)  cardinality_error_0_unscaled: 292.6667 (292.2664)  loss_ce_1_unscaled: 0.8518 (0.8553)  loss_bbox_1_unscaled: 0.0908 (0.0945)  loss_giou_1_unscaled: 0.4283 (0.4335)  cardinality_error_1_unscaled: 292.4167 (292.0070)  loss_ce_2_unscaled: 0.8467 (0.8581)  loss_bbox_2_unscaled: 0.0896 (0.0928)  loss_giou_2_unscaled: 0.4302 (0.4242)  cardinality_error_2_unscaled: 292.4167 (292.1688)  loss_ce_3_unscaled: 0.8469 (0.8593)  loss_bbox_3_unscaled: 0.0880 (0.0921)  loss_giou_3_unscaled: 0.4270 (0.4205)  cardinality_error_3_unscaled: 292.4167 (292.1761)  loss_ce_4_unscaled: 0.8198 (0.8441)  loss_bbox_4_unscaled: 0.0884 (0.0921)  loss_giou_4_unscaled: 0.4271 (0.4206)  cardinality_error_4_unscaled: 292.4167 (292.2200)  time: 2.9859  data: 0.0326  max mem: 8138\n",
            "Test:  [320/417]  eta: 0:04:51  class_error: 47.76  loss: 17.1315 (18.2821)  loss_ce: 1.6632 (1.7338)  loss_bbox: 0.4314 (0.4622)  loss_giou: 0.8496 (0.8437)  loss_ce_0: 1.6216 (1.6945)  loss_bbox_0: 0.4748 (0.5181)  loss_giou_0: 0.9379 (0.9525)  loss_ce_1: 1.6312 (1.7088)  loss_bbox_1: 0.4314 (0.4719)  loss_giou_1: 0.8565 (0.8667)  loss_ce_2: 1.6533 (1.7147)  loss_bbox_2: 0.4247 (0.4630)  loss_giou_2: 0.8481 (0.8482)  loss_ce_3: 1.6697 (1.7169)  loss_bbox_3: 0.4332 (0.4597)  loss_giou_3: 0.8435 (0.8407)  loss_ce_4: 1.6272 (1.6863)  loss_bbox_4: 0.4327 (0.4594)  loss_giou_4: 0.8447 (0.8408)  loss_ce_unscaled: 0.8316 (0.8669)  class_error_unscaled: 55.7823 (57.0889)  loss_bbox_unscaled: 0.0863 (0.0924)  loss_giou_unscaled: 0.4248 (0.4219)  cardinality_error_unscaled: 292.4167 (292.1379)  loss_ce_0_unscaled: 0.8108 (0.8473)  loss_bbox_0_unscaled: 0.0950 (0.1036)  loss_giou_0_unscaled: 0.4689 (0.4763)  cardinality_error_0_unscaled: 292.4167 (292.2557)  loss_ce_1_unscaled: 0.8156 (0.8544)  loss_bbox_1_unscaled: 0.0863 (0.0944)  loss_giou_1_unscaled: 0.4283 (0.4334)  cardinality_error_1_unscaled: 292.4167 (291.9974)  loss_ce_2_unscaled: 0.8266 (0.8574)  loss_bbox_2_unscaled: 0.0849 (0.0926)  loss_giou_2_unscaled: 0.4241 (0.4241)  cardinality_error_2_unscaled: 292.4167 (292.1547)  loss_ce_3_unscaled: 0.8348 (0.8585)  loss_bbox_3_unscaled: 0.0866 (0.0919)  loss_giou_3_unscaled: 0.4218 (0.4203)  cardinality_error_3_unscaled: 292.4167 (292.1659)  loss_ce_4_unscaled: 0.8136 (0.8432)  loss_bbox_4_unscaled: 0.0865 (0.0919)  loss_giou_4_unscaled: 0.4224 (0.4204)  cardinality_error_4_unscaled: 292.4167 (292.2077)  time: 2.9668  data: 0.0319  max mem: 8138\n",
            "Test:  [330/417]  eta: 0:04:20  class_error: 66.67  loss: 17.3426 (18.2775)  loss_ce: 1.5830 (1.7315)  loss_bbox: 0.4632 (0.4639)  loss_giou: 0.8204 (0.8429)  loss_ce_0: 1.6106 (1.6930)  loss_bbox_0: 0.5018 (0.5203)  loss_giou_0: 0.9280 (0.9518)  loss_ce_1: 1.6151 (1.7070)  loss_bbox_1: 0.4806 (0.4738)  loss_giou_1: 0.8468 (0.8662)  loss_ce_2: 1.6235 (1.7128)  loss_bbox_2: 0.4692 (0.4650)  loss_giou_2: 0.8204 (0.8476)  loss_ce_3: 1.5819 (1.7152)  loss_bbox_3: 0.4622 (0.4614)  loss_giou_3: 0.8070 (0.8398)  loss_ce_4: 1.5656 (1.6843)  loss_bbox_4: 0.4521 (0.4611)  loss_giou_4: 0.8170 (0.8400)  loss_ce_unscaled: 0.7915 (0.8657)  class_error_unscaled: 54.4643 (57.0139)  loss_bbox_unscaled: 0.0926 (0.0928)  loss_giou_unscaled: 0.4102 (0.4215)  cardinality_error_unscaled: 292.5000 (292.1657)  loss_ce_0_unscaled: 0.8053 (0.8465)  loss_bbox_0_unscaled: 0.1004 (0.1041)  loss_giou_0_unscaled: 0.4640 (0.4759)  cardinality_error_0_unscaled: 293.0833 (292.2837)  loss_ce_1_unscaled: 0.8075 (0.8535)  loss_bbox_1_unscaled: 0.0961 (0.0948)  loss_giou_1_unscaled: 0.4234 (0.4331)  cardinality_error_1_unscaled: 292.0000 (292.0285)  loss_ce_2_unscaled: 0.8117 (0.8564)  loss_bbox_2_unscaled: 0.0938 (0.0930)  loss_giou_2_unscaled: 0.4102 (0.4238)  cardinality_error_2_unscaled: 292.5000 (292.1823)  loss_ce_3_unscaled: 0.7910 (0.8576)  loss_bbox_3_unscaled: 0.0924 (0.0923)  loss_giou_3_unscaled: 0.4035 (0.4199)  cardinality_error_3_unscaled: 292.0833 (292.1911)  loss_ce_4_unscaled: 0.7828 (0.8421)  loss_bbox_4_unscaled: 0.0904 (0.0922)  loss_giou_4_unscaled: 0.4085 (0.4200)  cardinality_error_4_unscaled: 292.2500 (292.2326)  time: 2.8484  data: 0.0304  max mem: 8138\n",
            "Test:  [340/417]  eta: 0:03:50  class_error: 33.94  loss: 17.0886 (18.2379)  loss_ce: 1.4873 (1.7257)  loss_bbox: 0.4642 (0.4638)  loss_giou: 0.8550 (0.8419)  loss_ce_0: 1.4563 (1.6879)  loss_bbox_0: 0.5318 (0.5202)  loss_giou_0: 0.9760 (0.9510)  loss_ce_1: 1.4700 (1.7016)  loss_bbox_1: 0.4877 (0.4738)  loss_giou_1: 0.8855 (0.8652)  loss_ce_2: 1.4696 (1.7070)  loss_bbox_2: 0.4717 (0.4648)  loss_giou_2: 0.8589 (0.8466)  loss_ce_3: 1.4649 (1.7093)  loss_bbox_3: 0.4750 (0.4613)  loss_giou_3: 0.8505 (0.8388)  loss_ce_4: 1.4641 (1.6787)  loss_bbox_4: 0.4736 (0.4612)  loss_giou_4: 0.8530 (0.8390)  loss_ce_unscaled: 0.7436 (0.8628)  class_error_unscaled: 49.4624 (56.8332)  loss_bbox_unscaled: 0.0928 (0.0928)  loss_giou_unscaled: 0.4275 (0.4210)  cardinality_error_unscaled: 292.3333 (292.1364)  loss_ce_0_unscaled: 0.7281 (0.8440)  loss_bbox_0_unscaled: 0.1064 (0.1040)  loss_giou_0_unscaled: 0.4880 (0.4755)  cardinality_error_0_unscaled: 292.2500 (292.2537)  loss_ce_1_unscaled: 0.7350 (0.8508)  loss_bbox_1_unscaled: 0.0975 (0.0948)  loss_giou_1_unscaled: 0.4428 (0.4326)  cardinality_error_1_unscaled: 292.0000 (292.0020)  loss_ce_2_unscaled: 0.7348 (0.8535)  loss_bbox_2_unscaled: 0.0943 (0.0930)  loss_giou_2_unscaled: 0.4295 (0.4233)  cardinality_error_2_unscaled: 291.9167 (292.1545)  loss_ce_3_unscaled: 0.7325 (0.8547)  loss_bbox_3_unscaled: 0.0950 (0.0923)  loss_giou_3_unscaled: 0.4252 (0.4194)  cardinality_error_3_unscaled: 292.0000 (292.1611)  loss_ce_4_unscaled: 0.7321 (0.8393)  loss_bbox_4_unscaled: 0.0947 (0.0922)  loss_giou_4_unscaled: 0.4265 (0.4195)  cardinality_error_4_unscaled: 291.9167 (292.2028)  time: 2.8265  data: 0.0289  max mem: 8138\n",
            "Test:  [350/417]  eta: 0:03:20  class_error: 43.86  loss: 15.4329 (18.1778)  loss_ce: 1.4277 (1.7179)  loss_bbox: 0.4452 (0.4630)  loss_giou: 0.7940 (0.8405)  loss_ce_0: 1.3742 (1.6799)  loss_bbox_0: 0.4780 (0.5193)  loss_giou_0: 0.9465 (0.9497)  loss_ce_1: 1.3562 (1.6936)  loss_bbox_1: 0.4550 (0.4727)  loss_giou_1: 0.8192 (0.8637)  loss_ce_2: 1.3927 (1.6994)  loss_bbox_2: 0.4298 (0.4638)  loss_giou_2: 0.7927 (0.8450)  loss_ce_3: 1.3621 (1.7020)  loss_bbox_3: 0.4395 (0.4604)  loss_giou_3: 0.7932 (0.8374)  loss_ce_4: 1.3751 (1.6714)  loss_bbox_4: 0.4446 (0.4603)  loss_giou_4: 0.7961 (0.8377)  loss_ce_unscaled: 0.7139 (0.8590)  class_error_unscaled: 47.5610 (56.6189)  loss_bbox_unscaled: 0.0890 (0.0926)  loss_giou_unscaled: 0.3970 (0.4203)  cardinality_error_unscaled: 291.2500 (292.1394)  loss_ce_0_unscaled: 0.6871 (0.8400)  loss_bbox_0_unscaled: 0.0956 (0.1039)  loss_giou_0_unscaled: 0.4732 (0.4749)  cardinality_error_0_unscaled: 291.2500 (292.2569)  loss_ce_1_unscaled: 0.6781 (0.8468)  loss_bbox_1_unscaled: 0.0910 (0.0945)  loss_giou_1_unscaled: 0.4096 (0.4319)  cardinality_error_1_unscaled: 291.2500 (292.0093)  loss_ce_2_unscaled: 0.6964 (0.8497)  loss_bbox_2_unscaled: 0.0860 (0.0928)  loss_giou_2_unscaled: 0.3963 (0.4225)  cardinality_error_2_unscaled: 291.2500 (292.1593)  loss_ce_3_unscaled: 0.6811 (0.8510)  loss_bbox_3_unscaled: 0.0879 (0.0921)  loss_giou_3_unscaled: 0.3966 (0.4187)  cardinality_error_3_unscaled: 291.2500 (292.1664)  loss_ce_4_unscaled: 0.6876 (0.8357)  loss_bbox_4_unscaled: 0.0889 (0.0921)  loss_giou_4_unscaled: 0.3981 (0.4188)  cardinality_error_4_unscaled: 291.2500 (292.2082)  time: 2.9153  data: 0.0295  max mem: 8138\n",
            "Test:  [360/417]  eta: 0:02:50  class_error: 59.38  loss: 16.8915 (18.1710)  loss_ce: 1.5139 (1.7160)  loss_bbox: 0.4698 (0.4633)  loss_giou: 0.7940 (0.8407)  loss_ce_0: 1.5534 (1.6788)  loss_bbox_0: 0.4947 (0.5197)  loss_giou_0: 0.9064 (0.9501)  loss_ce_1: 1.4859 (1.6919)  loss_bbox_1: 0.4678 (0.4732)  loss_giou_1: 0.8078 (0.8642)  loss_ce_2: 1.4727 (1.6978)  loss_bbox_2: 0.4585 (0.4640)  loss_giou_2: 0.7927 (0.8452)  loss_ce_3: 1.4850 (1.7000)  loss_bbox_3: 0.4639 (0.4607)  loss_giou_3: 0.7932 (0.8377)  loss_ce_4: 1.4625 (1.6694)  loss_bbox_4: 0.4621 (0.4605)  loss_giou_4: 0.7961 (0.8379)  loss_ce_unscaled: 0.7570 (0.8580)  class_error_unscaled: 51.8519 (56.5678)  loss_bbox_unscaled: 0.0940 (0.0927)  loss_giou_unscaled: 0.3970 (0.4203)  cardinality_error_unscaled: 292.2500 (292.1272)  loss_ce_0_unscaled: 0.7767 (0.8394)  loss_bbox_0_unscaled: 0.0989 (0.1039)  loss_giou_0_unscaled: 0.4532 (0.4751)  cardinality_error_0_unscaled: 292.2500 (292.2456)  loss_ce_1_unscaled: 0.7429 (0.8459)  loss_bbox_1_unscaled: 0.0936 (0.0946)  loss_giou_1_unscaled: 0.4039 (0.4321)  cardinality_error_1_unscaled: 292.2500 (291.9956)  loss_ce_2_unscaled: 0.7363 (0.8489)  loss_bbox_2_unscaled: 0.0917 (0.0928)  loss_giou_2_unscaled: 0.3963 (0.4226)  cardinality_error_2_unscaled: 292.2500 (292.1477)  loss_ce_3_unscaled: 0.7425 (0.8500)  loss_bbox_3_unscaled: 0.0928 (0.0921)  loss_giou_3_unscaled: 0.3966 (0.4188)  cardinality_error_3_unscaled: 292.2500 (292.1547)  loss_ce_4_unscaled: 0.7313 (0.8347)  loss_bbox_4_unscaled: 0.0924 (0.0921)  loss_giou_4_unscaled: 0.3981 (0.4189)  cardinality_error_4_unscaled: 292.2500 (292.1937)  time: 3.0212  data: 0.0312  max mem: 8138\n",
            "Test:  [370/417]  eta: 0:02:20  class_error: 52.78  loss: 19.0330 (18.2156)  loss_ce: 1.7356 (1.7199)  loss_bbox: 0.4913 (0.4650)  loss_giou: 0.8811 (0.8425)  loss_ce_0: 1.7364 (1.6819)  loss_bbox_0: 0.5523 (0.5218)  loss_giou_0: 1.0097 (0.9519)  loss_ce_1: 1.7683 (1.6955)  loss_bbox_1: 0.5020 (0.4752)  loss_giou_1: 0.9447 (0.8663)  loss_ce_2: 1.7720 (1.7016)  loss_bbox_2: 0.4948 (0.4658)  loss_giou_2: 0.8938 (0.8473)  loss_ce_3: 1.7113 (1.7037)  loss_bbox_3: 0.4856 (0.4624)  loss_giou_3: 0.8786 (0.8396)  loss_ce_4: 1.7100 (1.6730)  loss_bbox_4: 0.4885 (0.4623)  loss_giou_4: 0.8809 (0.8398)  loss_ce_unscaled: 0.8678 (0.8600)  class_error_unscaled: 56.9231 (56.7058)  loss_bbox_unscaled: 0.0983 (0.0930)  loss_giou_unscaled: 0.4406 (0.4213)  cardinality_error_unscaled: 292.7500 (292.1496)  loss_ce_0_unscaled: 0.8682 (0.8410)  loss_bbox_0_unscaled: 0.1105 (0.1044)  loss_giou_0_unscaled: 0.5049 (0.4760)  cardinality_error_0_unscaled: 293.0833 (292.2671)  loss_ce_1_unscaled: 0.8841 (0.8477)  loss_bbox_1_unscaled: 0.1004 (0.0950)  loss_giou_1_unscaled: 0.4723 (0.4331)  cardinality_error_1_unscaled: 292.7500 (292.0209)  loss_ce_2_unscaled: 0.8860 (0.8508)  loss_bbox_2_unscaled: 0.0990 (0.0932)  loss_giou_2_unscaled: 0.4469 (0.4236)  cardinality_error_2_unscaled: 292.9167 (292.1698)  loss_ce_3_unscaled: 0.8556 (0.8518)  loss_bbox_3_unscaled: 0.0971 (0.0925)  loss_giou_3_unscaled: 0.4393 (0.4198)  cardinality_error_3_unscaled: 292.5833 (292.1770)  loss_ce_4_unscaled: 0.8550 (0.8365)  loss_bbox_4_unscaled: 0.0977 (0.0925)  loss_giou_4_unscaled: 0.4404 (0.4199)  cardinality_error_4_unscaled: 292.9167 (292.2177)  time: 3.0001  data: 0.0318  max mem: 8138\n",
            "Test:  [380/417]  eta: 0:01:50  class_error: 32.65  loss: 19.0330 (18.2140)  loss_ce: 1.8131 (1.7194)  loss_bbox: 0.4758 (0.4645)  loss_giou: 0.8967 (0.8433)  loss_ce_0: 1.7257 (1.6810)  loss_bbox_0: 0.5510 (0.5218)  loss_giou_0: 1.0097 (0.9528)  loss_ce_1: 1.8041 (1.6953)  loss_bbox_1: 0.4849 (0.4746)  loss_giou_1: 0.9296 (0.8671)  loss_ce_2: 1.8130 (1.7012)  loss_bbox_2: 0.4767 (0.4653)  loss_giou_2: 0.8960 (0.8479)  loss_ce_3: 1.7732 (1.7029)  loss_bbox_3: 0.4723 (0.4620)  loss_giou_3: 0.8993 (0.8404)  loss_ce_4: 1.7575 (1.6724)  loss_bbox_4: 0.4659 (0.4618)  loss_giou_4: 0.8993 (0.8406)  loss_ce_unscaled: 0.9066 (0.8597)  class_error_unscaled: 56.9231 (56.6473)  loss_bbox_unscaled: 0.0952 (0.0929)  loss_giou_unscaled: 0.4483 (0.4216)  cardinality_error_unscaled: 292.6667 (292.1350)  loss_ce_0_unscaled: 0.8628 (0.8405)  loss_bbox_0_unscaled: 0.1102 (0.1044)  loss_giou_0_unscaled: 0.5049 (0.4764)  cardinality_error_0_unscaled: 293.0833 (292.2507)  loss_ce_1_unscaled: 0.9021 (0.8476)  loss_bbox_1_unscaled: 0.0970 (0.0949)  loss_giou_1_unscaled: 0.4648 (0.4335)  cardinality_error_1_unscaled: 292.5833 (292.0050)  loss_ce_2_unscaled: 0.9065 (0.8506)  loss_bbox_2_unscaled: 0.0953 (0.0931)  loss_giou_2_unscaled: 0.4480 (0.4239)  cardinality_error_2_unscaled: 292.7500 (292.1573)  loss_ce_3_unscaled: 0.8866 (0.8514)  loss_bbox_3_unscaled: 0.0945 (0.0924)  loss_giou_3_unscaled: 0.4496 (0.4202)  cardinality_error_3_unscaled: 292.5833 (292.1592)  loss_ce_4_unscaled: 0.8787 (0.8362)  loss_bbox_4_unscaled: 0.0932 (0.0924)  loss_giou_4_unscaled: 0.4497 (0.4203)  cardinality_error_4_unscaled: 292.9167 (292.2034)  time: 3.0065  data: 0.0306  max mem: 8138\n",
            "Test:  [390/417]  eta: 0:01:20  class_error: 47.13  loss: 17.8356 (18.1973)  loss_ce: 1.6642 (1.7167)  loss_bbox: 0.4121 (0.4639)  loss_giou: 0.8555 (0.8433)  loss_ce_0: 1.6342 (1.6790)  loss_bbox_0: 0.4772 (0.5213)  loss_giou_0: 0.9422 (0.9531)  loss_ce_1: 1.6664 (1.6930)  loss_bbox_1: 0.4188 (0.4741)  loss_giou_1: 0.8650 (0.8671)  loss_ce_2: 1.6244 (1.6986)  loss_bbox_2: 0.4056 (0.4649)  loss_giou_2: 0.8779 (0.8481)  loss_ce_3: 1.6408 (1.7002)  loss_bbox_3: 0.4068 (0.4616)  loss_giou_3: 0.8460 (0.8405)  loss_ce_4: 1.6285 (1.6699)  loss_bbox_4: 0.4068 (0.4613)  loss_giou_4: 0.8518 (0.8406)  loss_ce_unscaled: 0.8321 (0.8584)  class_error_unscaled: 53.0769 (56.5977)  loss_bbox_unscaled: 0.0824 (0.0928)  loss_giou_unscaled: 0.4278 (0.4217)  cardinality_error_unscaled: 292.5833 (292.1479)  loss_ce_0_unscaled: 0.8171 (0.8395)  loss_bbox_0_unscaled: 0.0954 (0.1043)  loss_giou_0_unscaled: 0.4711 (0.4765)  cardinality_error_0_unscaled: 292.5000 (292.2598)  loss_ce_1_unscaled: 0.8332 (0.8465)  loss_bbox_1_unscaled: 0.0838 (0.0948)  loss_giou_1_unscaled: 0.4325 (0.4335)  cardinality_error_1_unscaled: 292.5000 (292.0149)  loss_ce_2_unscaled: 0.8122 (0.8493)  loss_bbox_2_unscaled: 0.0811 (0.0930)  loss_giou_2_unscaled: 0.4389 (0.4241)  cardinality_error_2_unscaled: 292.5000 (292.1656)  loss_ce_3_unscaled: 0.8204 (0.8501)  loss_bbox_3_unscaled: 0.0814 (0.0923)  loss_giou_3_unscaled: 0.4230 (0.4203)  cardinality_error_3_unscaled: 292.5000 (292.1677)  loss_ce_4_unscaled: 0.8142 (0.8350)  loss_bbox_4_unscaled: 0.0814 (0.0923)  loss_giou_4_unscaled: 0.4259 (0.4203)  cardinality_error_4_unscaled: 292.5000 (292.2144)  time: 2.9674  data: 0.0299  max mem: 8138\n",
            "Test:  [400/417]  eta: 0:00:50  class_error: 51.04  loss: 17.8356 (18.1973)  loss_ce: 1.6642 (1.7162)  loss_bbox: 0.4269 (0.4643)  loss_giou: 0.8366 (0.8436)  loss_ce_0: 1.6342 (1.6783)  loss_bbox_0: 0.4954 (0.5214)  loss_giou_0: 0.9494 (0.9534)  loss_ce_1: 1.6664 (1.6922)  loss_bbox_1: 0.4504 (0.4742)  loss_giou_1: 0.8580 (0.8673)  loss_ce_2: 1.6244 (1.6978)  loss_bbox_2: 0.4308 (0.4652)  loss_giou_2: 0.8191 (0.8484)  loss_ce_3: 1.6434 (1.6999)  loss_bbox_3: 0.4225 (0.4619)  loss_giou_3: 0.8336 (0.8409)  loss_ce_4: 1.6334 (1.6697)  loss_bbox_4: 0.4259 (0.4616)  loss_giou_4: 0.8366 (0.8410)  loss_ce_unscaled: 0.8321 (0.8581)  class_error_unscaled: 52.2727 (56.5733)  loss_bbox_unscaled: 0.0854 (0.0929)  loss_giou_unscaled: 0.4183 (0.4218)  cardinality_error_unscaled: 292.3333 (292.1600)  loss_ce_0_unscaled: 0.8171 (0.8391)  loss_bbox_0_unscaled: 0.0991 (0.1043)  loss_giou_0_unscaled: 0.4747 (0.4767)  cardinality_error_0_unscaled: 292.1667 (292.2706)  loss_ce_1_unscaled: 0.8332 (0.8461)  loss_bbox_1_unscaled: 0.0901 (0.0948)  loss_giou_1_unscaled: 0.4290 (0.4336)  cardinality_error_1_unscaled: 292.2500 (292.0295)  loss_ce_2_unscaled: 0.8122 (0.8489)  loss_bbox_2_unscaled: 0.0862 (0.0930)  loss_giou_2_unscaled: 0.4096 (0.4242)  cardinality_error_2_unscaled: 292.2500 (292.1794)  loss_ce_3_unscaled: 0.8217 (0.8500)  loss_bbox_3_unscaled: 0.0845 (0.0924)  loss_giou_3_unscaled: 0.4168 (0.4205)  cardinality_error_3_unscaled: 292.1667 (292.1800)  loss_ce_4_unscaled: 0.8167 (0.8348)  loss_bbox_4_unscaled: 0.0852 (0.0923)  loss_giou_4_unscaled: 0.4183 (0.4205)  cardinality_error_4_unscaled: 292.2500 (292.2272)  time: 2.8604  data: 0.0300  max mem: 8138\n",
            "Test:  [410/417]  eta: 0:00:20  class_error: 55.00  loss: 18.8608 (18.2279)  loss_ce: 1.7209 (1.7196)  loss_bbox: 0.4561 (0.4643)  loss_giou: 0.9022 (0.8455)  loss_ce_0: 1.6912 (1.6808)  loss_bbox_0: 0.4938 (0.5214)  loss_giou_0: 0.9952 (0.9551)  loss_ce_1: 1.6991 (1.6956)  loss_bbox_1: 0.4719 (0.4743)  loss_giou_1: 0.9142 (0.8690)  loss_ce_2: 1.7470 (1.7012)  loss_bbox_2: 0.4475 (0.4653)  loss_giou_2: 0.9120 (0.8503)  loss_ce_3: 1.7142 (1.7032)  loss_bbox_3: 0.4499 (0.4620)  loss_giou_3: 0.9032 (0.8429)  loss_ce_4: 1.6931 (1.6731)  loss_bbox_4: 0.4474 (0.4617)  loss_giou_4: 0.8975 (0.8428)  loss_ce_unscaled: 0.8605 (0.8598)  class_error_unscaled: 55.0000 (56.6821)  loss_bbox_unscaled: 0.0912 (0.0929)  loss_giou_unscaled: 0.4511 (0.4227)  cardinality_error_unscaled: 292.0000 (292.1547)  loss_ce_0_unscaled: 0.8456 (0.8404)  loss_bbox_0_unscaled: 0.0988 (0.1043)  loss_giou_0_unscaled: 0.4976 (0.4776)  cardinality_error_0_unscaled: 291.9167 (292.2591)  loss_ce_1_unscaled: 0.8496 (0.8478)  loss_bbox_1_unscaled: 0.0944 (0.0949)  loss_giou_1_unscaled: 0.4571 (0.4345)  cardinality_error_1_unscaled: 291.9167 (292.0245)  loss_ce_2_unscaled: 0.8735 (0.8506)  loss_bbox_2_unscaled: 0.0895 (0.0931)  loss_giou_2_unscaled: 0.4560 (0.4252)  cardinality_error_2_unscaled: 291.9167 (292.1738)  loss_ce_3_unscaled: 0.8571 (0.8516)  loss_bbox_3_unscaled: 0.0900 (0.0924)  loss_giou_3_unscaled: 0.4516 (0.4214)  cardinality_error_3_unscaled: 292.2500 (292.1748)  loss_ce_4_unscaled: 0.8465 (0.8365)  loss_bbox_4_unscaled: 0.0895 (0.0923)  loss_giou_4_unscaled: 0.4488 (0.4214)  cardinality_error_4_unscaled: 292.2500 (292.2239)  time: 2.8563  data: 0.0311  max mem: 8138\n",
            "Test:  [416/417]  eta: 0:00:02  class_error: 24.39  loss: 18.4857 (18.1943)  loss_ce: 1.6955 (1.7170)  loss_bbox: 0.4526 (0.4630)  loss_giou: 0.8259 (0.8437)  loss_ce_0: 1.6322 (1.6783)  loss_bbox_0: 0.4938 (0.5202)  loss_giou_0: 0.9576 (0.9535)  loss_ce_1: 1.6911 (1.6926)  loss_bbox_1: 0.4719 (0.4731)  loss_giou_1: 0.8651 (0.8673)  loss_ce_2: 1.7163 (1.6984)  loss_bbox_2: 0.4475 (0.4639)  loss_giou_2: 0.8464 (0.8486)  loss_ce_3: 1.7058 (1.7008)  loss_bbox_3: 0.4499 (0.4607)  loss_giou_3: 0.8307 (0.8411)  loss_ce_4: 1.6523 (1.6707)  loss_bbox_4: 0.4474 (0.4603)  loss_giou_4: 0.8326 (0.8411)  loss_ce_unscaled: 0.8477 (0.8585)  class_error_unscaled: 51.8519 (56.5761)  loss_bbox_unscaled: 0.0905 (0.0926)  loss_giou_unscaled: 0.4130 (0.4219)  cardinality_error_unscaled: 292.7500 (292.1798)  loss_ce_0_unscaled: 0.8161 (0.8391)  loss_bbox_0_unscaled: 0.0988 (0.1040)  loss_giou_0_unscaled: 0.4788 (0.4768)  cardinality_error_0_unscaled: 292.7500 (292.2817)  loss_ce_1_unscaled: 0.8456 (0.8463)  loss_bbox_1_unscaled: 0.0944 (0.0946)  loss_giou_1_unscaled: 0.4326 (0.4336)  cardinality_error_1_unscaled: 292.7500 (292.0503)  loss_ce_2_unscaled: 0.8582 (0.8492)  loss_bbox_2_unscaled: 0.0895 (0.0928)  loss_giou_2_unscaled: 0.4232 (0.4243)  cardinality_error_2_unscaled: 292.7500 (292.1980)  loss_ce_3_unscaled: 0.8529 (0.8504)  loss_bbox_3_unscaled: 0.0900 (0.0921)  loss_giou_3_unscaled: 0.4154 (0.4206)  cardinality_error_3_unscaled: 292.7500 (292.1992)  loss_ce_4_unscaled: 0.8261 (0.8353)  loss_bbox_4_unscaled: 0.0895 (0.0921)  loss_giou_4_unscaled: 0.4163 (0.4206)  cardinality_error_4_unscaled: 292.7500 (292.2479)  time: 2.8513  data: 0.0309  max mem: 8138\n",
            "Test: Total time: 0:20:44 (2.9846 s / it)\n",
            "Averaged stats: class_error: 24.39  loss: 18.4857 (18.1943)  loss_ce: 1.6955 (1.7170)  loss_bbox: 0.4526 (0.4630)  loss_giou: 0.8259 (0.8437)  loss_ce_0: 1.6322 (1.6783)  loss_bbox_0: 0.4938 (0.5202)  loss_giou_0: 0.9576 (0.9535)  loss_ce_1: 1.6911 (1.6926)  loss_bbox_1: 0.4719 (0.4731)  loss_giou_1: 0.8651 (0.8673)  loss_ce_2: 1.7163 (1.6984)  loss_bbox_2: 0.4475 (0.4639)  loss_giou_2: 0.8464 (0.8486)  loss_ce_3: 1.7058 (1.7008)  loss_bbox_3: 0.4499 (0.4607)  loss_giou_3: 0.8307 (0.8411)  loss_ce_4: 1.6523 (1.6707)  loss_bbox_4: 0.4474 (0.4603)  loss_giou_4: 0.8326 (0.8411)  loss_ce_unscaled: 0.8477 (0.8585)  class_error_unscaled: 51.8519 (56.5761)  loss_bbox_unscaled: 0.0905 (0.0926)  loss_giou_unscaled: 0.4130 (0.4219)  cardinality_error_unscaled: 292.7500 (292.1798)  loss_ce_0_unscaled: 0.8161 (0.8391)  loss_bbox_0_unscaled: 0.0988 (0.1040)  loss_giou_0_unscaled: 0.4788 (0.4768)  cardinality_error_0_unscaled: 292.7500 (292.2817)  loss_ce_1_unscaled: 0.8456 (0.8463)  loss_bbox_1_unscaled: 0.0944 (0.0946)  loss_giou_1_unscaled: 0.4326 (0.4336)  cardinality_error_1_unscaled: 292.7500 (292.0503)  loss_ce_2_unscaled: 0.8582 (0.8492)  loss_bbox_2_unscaled: 0.0895 (0.0928)  loss_giou_2_unscaled: 0.4232 (0.4243)  cardinality_error_2_unscaled: 292.7500 (292.1980)  loss_ce_3_unscaled: 0.8529 (0.8504)  loss_bbox_3_unscaled: 0.0900 (0.0921)  loss_giou_3_unscaled: 0.4154 (0.4206)  cardinality_error_3_unscaled: 292.7500 (292.1992)  loss_ce_4_unscaled: 0.8261 (0.8353)  loss_bbox_4_unscaled: 0.0895 (0.0921)  loss_giou_4_unscaled: 0.4163 (0.4206)  cardinality_error_4_unscaled: 292.7500 (292.2479)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=13.18s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.266\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.394\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.284\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.114\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.280\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.405\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.245\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.389\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.416\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.206\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.432\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py \\\n",
        "    --output_dir exps/deform \\\n",
        "    --coco_path ../COCODIR \\\n",
        "    --batch_size 12 \\\n",
        "    --resume ./pth/cd-detr-v2.pth \\\n",
        "    --with_box_refine \\\n",
        "    --eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLERRM8Wdd-S",
        "outputId": "617571ee-4e56-49b1-ab43-913d922371b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "git:\n",
            "  sha: 11169a60c33333af00a4849f1808023eba96a931, status: has uncommited changes, branch: main\n",
            "\n",
            "Namespace(lr=0.0002, lr_backbone_names=['backbone.0'], lr_backbone=2e-05, lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, batch_size=12, weight_decay=0.0001, epochs=50, lr_drop=40, lr_drop_epochs=None, clip_max_norm=0.1, sgd=False, with_box_refine=True, two_stage=False, frozen_weights=None, backbone='resnet50', dilation=False, position_embedding='sine', position_embedding_scale=6.283185307179586, num_feature_levels=4, enc_layers=6, dec_layers=6, dim_feedforward=1024, hidden_dim=256, dropout=0.1, nheads=8, num_queries=300, dec_n_points=4, enc_n_points=4, masks=False, aux_loss=True, set_cost_class=2, set_cost_bbox=5, set_cost_giou=2, mask_loss_coef=1, dice_loss_coef=1, cls_loss_coef=2, bbox_loss_coef=5, giou_loss_coef=2, focal_alpha=0.25, dataset_file='coco', coco_path='../COCODIR', coco_panoptic_path=None, remove_difficult=False, output_dir='exps/deform', device='cuda', seed=42, resume='./pth/cd-detr-v2.pth', start_epoch=0, eval=True, num_workers=2, cache_mode=False, distributed=False)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "number of params: 40627260\n",
            "loading annotations into memory...\n",
            "Done (t=14.78s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=1.86s)\n",
            "creating index...\n",
            "index created!\n",
            "transformer.level_embed\n",
            "transformer.encoder.layers.0.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.0.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.0.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.0.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.0.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.0.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.0.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.0.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.0.norm1.weight\n",
            "transformer.encoder.layers.0.norm1.bias\n",
            "transformer.encoder.layers.0.linear1.weight\n",
            "transformer.encoder.layers.0.linear1.bias\n",
            "transformer.encoder.layers.0.linear2.weight\n",
            "transformer.encoder.layers.0.linear2.bias\n",
            "transformer.encoder.layers.0.norm2.weight\n",
            "transformer.encoder.layers.0.norm2.bias\n",
            "transformer.encoder.layers.1.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.1.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.1.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.1.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.1.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.1.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.1.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.1.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.1.norm1.weight\n",
            "transformer.encoder.layers.1.norm1.bias\n",
            "transformer.encoder.layers.1.linear1.weight\n",
            "transformer.encoder.layers.1.linear1.bias\n",
            "transformer.encoder.layers.1.linear2.weight\n",
            "transformer.encoder.layers.1.linear2.bias\n",
            "transformer.encoder.layers.1.norm2.weight\n",
            "transformer.encoder.layers.1.norm2.bias\n",
            "transformer.encoder.layers.2.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.2.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.2.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.2.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.2.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.2.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.2.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.2.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.2.norm1.weight\n",
            "transformer.encoder.layers.2.norm1.bias\n",
            "transformer.encoder.layers.2.linear1.weight\n",
            "transformer.encoder.layers.2.linear1.bias\n",
            "transformer.encoder.layers.2.linear2.weight\n",
            "transformer.encoder.layers.2.linear2.bias\n",
            "transformer.encoder.layers.2.norm2.weight\n",
            "transformer.encoder.layers.2.norm2.bias\n",
            "transformer.encoder.layers.3.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.3.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.3.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.3.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.3.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.3.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.3.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.3.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.3.norm1.weight\n",
            "transformer.encoder.layers.3.norm1.bias\n",
            "transformer.encoder.layers.3.linear1.weight\n",
            "transformer.encoder.layers.3.linear1.bias\n",
            "transformer.encoder.layers.3.linear2.weight\n",
            "transformer.encoder.layers.3.linear2.bias\n",
            "transformer.encoder.layers.3.norm2.weight\n",
            "transformer.encoder.layers.3.norm2.bias\n",
            "transformer.encoder.layers.4.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.4.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.4.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.4.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.4.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.4.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.4.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.4.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.4.norm1.weight\n",
            "transformer.encoder.layers.4.norm1.bias\n",
            "transformer.encoder.layers.4.linear1.weight\n",
            "transformer.encoder.layers.4.linear1.bias\n",
            "transformer.encoder.layers.4.linear2.weight\n",
            "transformer.encoder.layers.4.linear2.bias\n",
            "transformer.encoder.layers.4.norm2.weight\n",
            "transformer.encoder.layers.4.norm2.bias\n",
            "transformer.encoder.layers.5.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.5.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.5.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.5.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.5.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.5.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.5.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.5.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.5.norm1.weight\n",
            "transformer.encoder.layers.5.norm1.bias\n",
            "transformer.encoder.layers.5.linear1.weight\n",
            "transformer.encoder.layers.5.linear1.bias\n",
            "transformer.encoder.layers.5.linear2.weight\n",
            "transformer.encoder.layers.5.linear2.bias\n",
            "transformer.encoder.layers.5.norm2.weight\n",
            "transformer.encoder.layers.5.norm2.bias\n",
            "transformer.decoder.layers.0.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.0.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.0.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.0.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.0.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.0.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.0.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.0.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.0.norm1.weight\n",
            "transformer.decoder.layers.0.norm1.bias\n",
            "transformer.decoder.layers.0.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.0.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.0.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.0.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.0.norm2.weight\n",
            "transformer.decoder.layers.0.norm2.bias\n",
            "transformer.decoder.layers.0.linear1.weight\n",
            "transformer.decoder.layers.0.linear1.bias\n",
            "transformer.decoder.layers.0.linear2.weight\n",
            "transformer.decoder.layers.0.linear2.bias\n",
            "transformer.decoder.layers.0.norm3.weight\n",
            "transformer.decoder.layers.0.norm3.bias\n",
            "transformer.decoder.layers.1.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.1.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.1.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.1.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.1.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.1.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.1.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.1.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.1.norm1.weight\n",
            "transformer.decoder.layers.1.norm1.bias\n",
            "transformer.decoder.layers.1.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.1.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.1.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.1.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.1.norm2.weight\n",
            "transformer.decoder.layers.1.norm2.bias\n",
            "transformer.decoder.layers.1.linear1.weight\n",
            "transformer.decoder.layers.1.linear1.bias\n",
            "transformer.decoder.layers.1.linear2.weight\n",
            "transformer.decoder.layers.1.linear2.bias\n",
            "transformer.decoder.layers.1.norm3.weight\n",
            "transformer.decoder.layers.1.norm3.bias\n",
            "transformer.decoder.layers.2.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.2.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.2.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.2.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.2.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.2.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.2.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.2.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.2.norm1.weight\n",
            "transformer.decoder.layers.2.norm1.bias\n",
            "transformer.decoder.layers.2.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.2.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.2.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.2.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.2.norm2.weight\n",
            "transformer.decoder.layers.2.norm2.bias\n",
            "transformer.decoder.layers.2.linear1.weight\n",
            "transformer.decoder.layers.2.linear1.bias\n",
            "transformer.decoder.layers.2.linear2.weight\n",
            "transformer.decoder.layers.2.linear2.bias\n",
            "transformer.decoder.layers.2.norm3.weight\n",
            "transformer.decoder.layers.2.norm3.bias\n",
            "transformer.decoder.layers.3.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.3.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.3.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.3.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.3.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.3.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.3.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.3.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.3.norm1.weight\n",
            "transformer.decoder.layers.3.norm1.bias\n",
            "transformer.decoder.layers.3.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.3.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.3.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.3.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.3.norm2.weight\n",
            "transformer.decoder.layers.3.norm2.bias\n",
            "transformer.decoder.layers.3.linear1.weight\n",
            "transformer.decoder.layers.3.linear1.bias\n",
            "transformer.decoder.layers.3.linear2.weight\n",
            "transformer.decoder.layers.3.linear2.bias\n",
            "transformer.decoder.layers.3.norm3.weight\n",
            "transformer.decoder.layers.3.norm3.bias\n",
            "transformer.decoder.layers.4.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.4.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.4.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.4.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.4.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.4.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.4.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.4.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.4.norm1.weight\n",
            "transformer.decoder.layers.4.norm1.bias\n",
            "transformer.decoder.layers.4.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.4.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.4.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.4.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.4.norm2.weight\n",
            "transformer.decoder.layers.4.norm2.bias\n",
            "transformer.decoder.layers.4.linear1.weight\n",
            "transformer.decoder.layers.4.linear1.bias\n",
            "transformer.decoder.layers.4.linear2.weight\n",
            "transformer.decoder.layers.4.linear2.bias\n",
            "transformer.decoder.layers.4.norm3.weight\n",
            "transformer.decoder.layers.4.norm3.bias\n",
            "transformer.decoder.layers.5.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.5.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.5.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.5.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.5.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.5.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.5.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.5.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.5.norm1.weight\n",
            "transformer.decoder.layers.5.norm1.bias\n",
            "transformer.decoder.layers.5.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.5.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.5.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.5.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.5.norm2.weight\n",
            "transformer.decoder.layers.5.norm2.bias\n",
            "transformer.decoder.layers.5.linear1.weight\n",
            "transformer.decoder.layers.5.linear1.bias\n",
            "transformer.decoder.layers.5.linear2.weight\n",
            "transformer.decoder.layers.5.linear2.bias\n",
            "transformer.decoder.layers.5.norm3.weight\n",
            "transformer.decoder.layers.5.norm3.bias\n",
            "transformer.decoder.bbox_embed.0.layers.0.weight\n",
            "transformer.decoder.bbox_embed.0.layers.0.bias\n",
            "transformer.decoder.bbox_embed.0.layers.1.weight\n",
            "transformer.decoder.bbox_embed.0.layers.1.bias\n",
            "transformer.decoder.bbox_embed.0.layers.2.weight\n",
            "transformer.decoder.bbox_embed.0.layers.2.bias\n",
            "transformer.decoder.bbox_embed.1.layers.0.weight\n",
            "transformer.decoder.bbox_embed.1.layers.0.bias\n",
            "transformer.decoder.bbox_embed.1.layers.1.weight\n",
            "transformer.decoder.bbox_embed.1.layers.1.bias\n",
            "transformer.decoder.bbox_embed.1.layers.2.weight\n",
            "transformer.decoder.bbox_embed.1.layers.2.bias\n",
            "transformer.decoder.bbox_embed.2.layers.0.weight\n",
            "transformer.decoder.bbox_embed.2.layers.0.bias\n",
            "transformer.decoder.bbox_embed.2.layers.1.weight\n",
            "transformer.decoder.bbox_embed.2.layers.1.bias\n",
            "transformer.decoder.bbox_embed.2.layers.2.weight\n",
            "transformer.decoder.bbox_embed.2.layers.2.bias\n",
            "transformer.decoder.bbox_embed.3.layers.0.weight\n",
            "transformer.decoder.bbox_embed.3.layers.0.bias\n",
            "transformer.decoder.bbox_embed.3.layers.1.weight\n",
            "transformer.decoder.bbox_embed.3.layers.1.bias\n",
            "transformer.decoder.bbox_embed.3.layers.2.weight\n",
            "transformer.decoder.bbox_embed.3.layers.2.bias\n",
            "transformer.decoder.bbox_embed.4.layers.0.weight\n",
            "transformer.decoder.bbox_embed.4.layers.0.bias\n",
            "transformer.decoder.bbox_embed.4.layers.1.weight\n",
            "transformer.decoder.bbox_embed.4.layers.1.bias\n",
            "transformer.decoder.bbox_embed.4.layers.2.weight\n",
            "transformer.decoder.bbox_embed.4.layers.2.bias\n",
            "transformer.decoder.bbox_embed.5.layers.0.weight\n",
            "transformer.decoder.bbox_embed.5.layers.0.bias\n",
            "transformer.decoder.bbox_embed.5.layers.1.weight\n",
            "transformer.decoder.bbox_embed.5.layers.1.bias\n",
            "transformer.decoder.bbox_embed.5.layers.2.weight\n",
            "transformer.decoder.bbox_embed.5.layers.2.bias\n",
            "transformer.reference_points.weight\n",
            "transformer.reference_points.bias\n",
            "class_embed.0.weight\n",
            "class_embed.0.bias\n",
            "class_embed.1.weight\n",
            "class_embed.1.bias\n",
            "class_embed.2.weight\n",
            "class_embed.2.bias\n",
            "class_embed.3.weight\n",
            "class_embed.3.bias\n",
            "class_embed.4.weight\n",
            "class_embed.4.bias\n",
            "class_embed.5.weight\n",
            "class_embed.5.bias\n",
            "query_embed.weight\n",
            "input_proj.0.0.weight\n",
            "input_proj.0.0.bias\n",
            "input_proj.0.1.weight\n",
            "input_proj.0.1.bias\n",
            "input_proj.1.0.weight\n",
            "input_proj.1.0.bias\n",
            "input_proj.1.1.weight\n",
            "input_proj.1.1.bias\n",
            "input_proj.2.0.weight\n",
            "input_proj.2.0.bias\n",
            "input_proj.2.1.weight\n",
            "input_proj.2.1.bias\n",
            "input_proj.3.0.weight\n",
            "input_proj.3.0.bias\n",
            "input_proj.3.1.weight\n",
            "input_proj.3.1.bias\n",
            "backbone.0.body.conv1.weight\n",
            "backbone.0.body.layer1.0.conv1.weight\n",
            "backbone.0.body.layer1.0.conv2.weight\n",
            "backbone.0.body.layer1.0.conv3.weight\n",
            "backbone.0.body.layer1.0.downsample.0.weight\n",
            "backbone.0.body.layer1.1.conv1.weight\n",
            "backbone.0.body.layer1.1.conv2.weight\n",
            "backbone.0.body.layer1.1.conv3.weight\n",
            "backbone.0.body.layer1.2.conv1.weight\n",
            "backbone.0.body.layer1.2.conv2.weight\n",
            "backbone.0.body.layer1.2.conv3.weight\n",
            "backbone.0.body.layer2.0.conv1.weight\n",
            "backbone.0.body.layer2.0.conv2.weight\n",
            "backbone.0.body.layer2.0.conv3.weight\n",
            "backbone.0.body.layer2.0.downsample.0.weight\n",
            "backbone.0.body.layer2.1.conv1.weight\n",
            "backbone.0.body.layer2.1.conv2.weight\n",
            "backbone.0.body.layer2.1.conv3.weight\n",
            "backbone.0.body.layer2.2.conv1.weight\n",
            "backbone.0.body.layer2.2.conv2.weight\n",
            "backbone.0.body.layer2.2.conv3.weight\n",
            "backbone.0.body.layer2.3.conv1.weight\n",
            "backbone.0.body.layer2.3.conv2.weight\n",
            "backbone.0.body.layer2.3.conv3.weight\n",
            "backbone.0.body.layer3.0.conv1.weight\n",
            "backbone.0.body.layer3.0.conv2.weight\n",
            "backbone.0.body.layer3.0.conv3.weight\n",
            "backbone.0.body.layer3.0.downsample.0.weight\n",
            "backbone.0.body.layer3.1.conv1.weight\n",
            "backbone.0.body.layer3.1.conv2.weight\n",
            "backbone.0.body.layer3.1.conv3.weight\n",
            "backbone.0.body.layer3.2.conv1.weight\n",
            "backbone.0.body.layer3.2.conv2.weight\n",
            "backbone.0.body.layer3.2.conv3.weight\n",
            "backbone.0.body.layer3.3.conv1.weight\n",
            "backbone.0.body.layer3.3.conv2.weight\n",
            "backbone.0.body.layer3.3.conv3.weight\n",
            "backbone.0.body.layer3.4.conv1.weight\n",
            "backbone.0.body.layer3.4.conv2.weight\n",
            "backbone.0.body.layer3.4.conv3.weight\n",
            "backbone.0.body.layer3.5.conv1.weight\n",
            "backbone.0.body.layer3.5.conv2.weight\n",
            "backbone.0.body.layer3.5.conv3.weight\n",
            "backbone.0.body.layer4.0.conv1.weight\n",
            "backbone.0.body.layer4.0.conv2.weight\n",
            "backbone.0.body.layer4.0.conv3.weight\n",
            "backbone.0.body.layer4.0.downsample.0.weight\n",
            "backbone.0.body.layer4.1.conv1.weight\n",
            "backbone.0.body.layer4.1.conv2.weight\n",
            "backbone.0.body.layer4.1.conv3.weight\n",
            "backbone.0.body.layer4.2.conv1.weight\n",
            "backbone.0.body.layer4.2.conv2.weight\n",
            "backbone.0.body.layer4.2.conv3.weight\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Test:  [  0/417]  eta: 0:27:18  class_error: 54.74  loss: 19.3367 (19.3367)  loss_ce: 1.6908 (1.6908)  loss_bbox: 0.5543 (0.5543)  loss_giou: 0.9307 (0.9307)  loss_ce_0: 1.7009 (1.7009)  loss_bbox_0: 0.6122 (0.6122)  loss_giou_0: 1.0613 (1.0613)  loss_ce_1: 1.7162 (1.7162)  loss_bbox_1: 0.5647 (0.5647)  loss_giou_1: 0.9617 (0.9617)  loss_ce_2: 1.6960 (1.6960)  loss_bbox_2: 0.5610 (0.5610)  loss_giou_2: 0.9319 (0.9319)  loss_ce_3: 1.7149 (1.7149)  loss_bbox_3: 0.5501 (0.5501)  loss_giou_3: 0.9217 (0.9217)  loss_ce_4: 1.6822 (1.6822)  loss_bbox_4: 0.5565 (0.5565)  loss_giou_4: 0.9295 (0.9295)  loss_ce_unscaled: 0.8454 (0.8454)  class_error_unscaled: 54.7368 (54.7368)  loss_bbox_unscaled: 0.1109 (0.1109)  loss_giou_unscaled: 0.4654 (0.4654)  cardinality_error_unscaled: 292.0833 (292.0833)  loss_ce_0_unscaled: 0.8505 (0.8505)  loss_bbox_0_unscaled: 0.1224 (0.1224)  loss_giou_0_unscaled: 0.5306 (0.5306)  cardinality_error_0_unscaled: 292.0833 (292.0833)  loss_ce_1_unscaled: 0.8581 (0.8581)  loss_bbox_1_unscaled: 0.1129 (0.1129)  loss_giou_1_unscaled: 0.4808 (0.4808)  cardinality_error_1_unscaled: 292.0833 (292.0833)  loss_ce_2_unscaled: 0.8480 (0.8480)  loss_bbox_2_unscaled: 0.1122 (0.1122)  loss_giou_2_unscaled: 0.4660 (0.4660)  cardinality_error_2_unscaled: 292.0833 (292.0833)  loss_ce_3_unscaled: 0.8574 (0.8574)  loss_bbox_3_unscaled: 0.1100 (0.1100)  loss_giou_3_unscaled: 0.4609 (0.4609)  cardinality_error_3_unscaled: 292.0833 (292.0833)  loss_ce_4_unscaled: 0.8411 (0.8411)  loss_bbox_4_unscaled: 0.1113 (0.1113)  loss_giou_4_unscaled: 0.4648 (0.4648)  cardinality_error_4_unscaled: 291.9167 (291.9167)  time: 3.9284  data: 0.8978  max mem: 6678\n",
            "Test:  [ 10/417]  eta: 0:19:46  class_error: 73.74  loss: 19.3367 (19.9589)  loss_ce: 1.8187 (1.9023)  loss_bbox: 0.4804 (0.5234)  loss_giou: 0.8656 (0.8597)  loss_ce_0: 1.8518 (1.9145)  loss_bbox_0: 0.5660 (0.5904)  loss_giou_0: 0.9965 (0.9937)  loss_ce_1: 1.8714 (1.9100)  loss_bbox_1: 0.5045 (0.5284)  loss_giou_1: 0.8850 (0.8813)  loss_ce_2: 1.8675 (1.8888)  loss_bbox_2: 0.4928 (0.5275)  loss_giou_2: 0.8816 (0.8641)  loss_ce_3: 1.8520 (1.9036)  loss_bbox_3: 0.4921 (0.5250)  loss_giou_3: 0.8640 (0.8562)  loss_ce_4: 1.8613 (1.9061)  loss_bbox_4: 0.4858 (0.5247)  loss_giou_4: 0.8640 (0.8593)  loss_ce_unscaled: 0.9094 (0.9512)  class_error_unscaled: 63.1579 (63.0597)  loss_bbox_unscaled: 0.0961 (0.1047)  loss_giou_unscaled: 0.4328 (0.4298)  cardinality_error_unscaled: 292.0833 (292.5758)  loss_ce_0_unscaled: 0.9259 (0.9573)  loss_bbox_0_unscaled: 0.1132 (0.1181)  loss_giou_0_unscaled: 0.4983 (0.4969)  cardinality_error_0_unscaled: 292.2500 (292.5909)  loss_ce_1_unscaled: 0.9357 (0.9550)  loss_bbox_1_unscaled: 0.1009 (0.1057)  loss_giou_1_unscaled: 0.4425 (0.4407)  cardinality_error_1_unscaled: 292.0833 (292.5530)  loss_ce_2_unscaled: 0.9337 (0.9444)  loss_bbox_2_unscaled: 0.0986 (0.1055)  loss_giou_2_unscaled: 0.4408 (0.4321)  cardinality_error_2_unscaled: 292.1667 (292.5076)  loss_ce_3_unscaled: 0.9260 (0.9518)  loss_bbox_3_unscaled: 0.0984 (0.1050)  loss_giou_3_unscaled: 0.4320 (0.4281)  cardinality_error_3_unscaled: 292.1667 (292.4849)  loss_ce_4_unscaled: 0.9306 (0.9531)  loss_bbox_4_unscaled: 0.0972 (0.1049)  loss_giou_4_unscaled: 0.4320 (0.4296)  cardinality_error_4_unscaled: 292.1667 (292.5909)  time: 2.9158  data: 0.1086  max mem: 7374\n",
            "Test:  [ 20/417]  eta: 0:18:58  class_error: 58.51  loss: 19.1567 (19.2427)  loss_ce: 1.7911 (1.8348)  loss_bbox: 0.4670 (0.5025)  loss_giou: 0.8160 (0.8316)  loss_ce_0: 1.8492 (1.8540)  loss_bbox_0: 0.5523 (0.5636)  loss_giou_0: 0.9226 (0.9572)  loss_ce_1: 1.8281 (1.8386)  loss_bbox_1: 0.4802 (0.5080)  loss_giou_1: 0.8646 (0.8558)  loss_ce_2: 1.8304 (1.8277)  loss_bbox_2: 0.4724 (0.5027)  loss_giou_2: 0.8241 (0.8349)  loss_ce_3: 1.8099 (1.8347)  loss_bbox_3: 0.4705 (0.5009)  loss_giou_3: 0.8067 (0.8279)  loss_ce_4: 1.8021 (1.8364)  loss_bbox_4: 0.4665 (0.5011)  loss_giou_4: 0.8078 (0.8302)  loss_ce_unscaled: 0.8955 (0.9174)  class_error_unscaled: 60.3175 (61.0956)  loss_bbox_unscaled: 0.0934 (0.1005)  loss_giou_unscaled: 0.4080 (0.4158)  cardinality_error_unscaled: 292.0833 (292.6032)  loss_ce_0_unscaled: 0.9246 (0.9270)  loss_bbox_0_unscaled: 0.1105 (0.1127)  loss_giou_0_unscaled: 0.4613 (0.4786)  cardinality_error_0_unscaled: 292.2500 (292.7460)  loss_ce_1_unscaled: 0.9141 (0.9193)  loss_bbox_1_unscaled: 0.0960 (0.1016)  loss_giou_1_unscaled: 0.4323 (0.4279)  cardinality_error_1_unscaled: 292.0833 (292.6310)  loss_ce_2_unscaled: 0.9152 (0.9139)  loss_bbox_2_unscaled: 0.0945 (0.1005)  loss_giou_2_unscaled: 0.4120 (0.4174)  cardinality_error_2_unscaled: 292.1667 (292.6508)  loss_ce_3_unscaled: 0.9049 (0.9173)  loss_bbox_3_unscaled: 0.0941 (0.1002)  loss_giou_3_unscaled: 0.4033 (0.4139)  cardinality_error_3_unscaled: 292.1667 (292.5635)  loss_ce_4_unscaled: 0.9010 (0.9182)  loss_bbox_4_unscaled: 0.0933 (0.1002)  loss_giou_4_unscaled: 0.4039 (0.4151)  cardinality_error_4_unscaled: 292.1667 (292.6667)  time: 2.8150  data: 0.0290  max mem: 7755\n",
            "Test:  [ 30/417]  eta: 0:18:56  class_error: 59.15  loss: 17.8572 (18.4410)  loss_ce: 1.6185 (1.7456)  loss_bbox: 0.4295 (0.4592)  loss_giou: 0.7928 (0.8354)  loss_ce_0: 1.6271 (1.7612)  loss_bbox_0: 0.4669 (0.5185)  loss_giou_0: 0.9226 (0.9604)  loss_ce_1: 1.5627 (1.7355)  loss_bbox_1: 0.4329 (0.4644)  loss_giou_1: 0.8259 (0.8603)  loss_ce_2: 1.5668 (1.7286)  loss_bbox_2: 0.4167 (0.4595)  loss_giou_2: 0.7945 (0.8385)  loss_ce_3: 1.6180 (1.7435)  loss_bbox_3: 0.4351 (0.4589)  loss_giou_3: 0.7941 (0.8325)  loss_ce_4: 1.6123 (1.7466)  loss_bbox_4: 0.4325 (0.4582)  loss_giou_4: 0.7935 (0.8342)  loss_ce_unscaled: 0.8092 (0.8728)  class_error_unscaled: 53.9007 (58.2732)  loss_bbox_unscaled: 0.0859 (0.0918)  loss_giou_unscaled: 0.3964 (0.4177)  cardinality_error_unscaled: 291.7500 (292.0027)  loss_ce_0_unscaled: 0.8135 (0.8806)  loss_bbox_0_unscaled: 0.0934 (0.1037)  loss_giou_0_unscaled: 0.4613 (0.4802)  cardinality_error_0_unscaled: 291.4167 (292.2581)  loss_ce_1_unscaled: 0.7814 (0.8678)  loss_bbox_1_unscaled: 0.0866 (0.0929)  loss_giou_1_unscaled: 0.4129 (0.4301)  cardinality_error_1_unscaled: 291.5833 (292.1022)  loss_ce_2_unscaled: 0.7834 (0.8643)  loss_bbox_2_unscaled: 0.0833 (0.0919)  loss_giou_2_unscaled: 0.3972 (0.4193)  cardinality_error_2_unscaled: 291.6667 (292.2124)  loss_ce_3_unscaled: 0.8090 (0.8718)  loss_bbox_3_unscaled: 0.0870 (0.0918)  loss_giou_3_unscaled: 0.3971 (0.4163)  cardinality_error_3_unscaled: 291.1667 (292.1317)  loss_ce_4_unscaled: 0.8061 (0.8733)  loss_bbox_4_unscaled: 0.0865 (0.0916)  loss_giou_4_unscaled: 0.3967 (0.4171)  cardinality_error_4_unscaled: 291.6667 (292.1290)  time: 2.9494  data: 0.0292  max mem: 8137\n",
            "Test:  [ 40/417]  eta: 0:18:55  class_error: 70.59  loss: 17.8757 (18.4328)  loss_ce: 1.8363 (1.7600)  loss_bbox: 0.3995 (0.4541)  loss_giou: 0.7928 (0.8246)  loss_ce_0: 1.6616 (1.7697)  loss_bbox_0: 0.4586 (0.5197)  loss_giou_0: 0.9277 (0.9488)  loss_ce_1: 1.7025 (1.7448)  loss_bbox_1: 0.4021 (0.4636)  loss_giou_1: 0.8259 (0.8491)  loss_ce_2: 1.7699 (1.7397)  loss_bbox_2: 0.3998 (0.4563)  loss_giou_2: 0.7945 (0.8282)  loss_ce_3: 1.8193 (1.7580)  loss_bbox_3: 0.3996 (0.4540)  loss_giou_3: 0.7941 (0.8219)  loss_ce_4: 1.8312 (1.7633)  loss_bbox_4: 0.3971 (0.4537)  loss_giou_4: 0.7935 (0.8236)  loss_ce_unscaled: 0.9182 (0.8800)  class_error_unscaled: 62.1622 (58.9942)  loss_bbox_unscaled: 0.0799 (0.0908)  loss_giou_unscaled: 0.3964 (0.4123)  cardinality_error_unscaled: 292.7500 (292.2947)  loss_ce_0_unscaled: 0.8308 (0.8848)  loss_bbox_0_unscaled: 0.0917 (0.1039)  loss_giou_0_unscaled: 0.4638 (0.4744)  cardinality_error_0_unscaled: 293.0000 (292.6016)  loss_ce_1_unscaled: 0.8512 (0.8724)  loss_bbox_1_unscaled: 0.0804 (0.0927)  loss_giou_1_unscaled: 0.4129 (0.4245)  cardinality_error_1_unscaled: 292.6667 (292.4004)  loss_ce_2_unscaled: 0.8850 (0.8698)  loss_bbox_2_unscaled: 0.0800 (0.0913)  loss_giou_2_unscaled: 0.3972 (0.4141)  cardinality_error_2_unscaled: 292.5000 (292.4025)  loss_ce_3_unscaled: 0.9097 (0.8790)  loss_bbox_3_unscaled: 0.0799 (0.0908)  loss_giou_3_unscaled: 0.3971 (0.4109)  cardinality_error_3_unscaled: 292.7500 (292.4085)  loss_ce_4_unscaled: 0.9156 (0.8816)  loss_bbox_4_unscaled: 0.0794 (0.0907)  loss_giou_4_unscaled: 0.3967 (0.4118)  cardinality_error_4_unscaled: 293.2500 (292.4248)  time: 3.1629  data: 0.0307  max mem: 8137\n",
            "Test:  [ 50/417]  eta: 0:18:31  class_error: 48.89  loss: 19.1471 (18.6961)  loss_ce: 1.8705 (1.7822)  loss_bbox: 0.4621 (0.4612)  loss_giou: 0.8106 (0.8344)  loss_ce_0: 1.8736 (1.8000)  loss_bbox_0: 0.5397 (0.5265)  loss_giou_0: 0.9460 (0.9587)  loss_ce_1: 1.8016 (1.7763)  loss_bbox_1: 0.4714 (0.4723)  loss_giou_1: 0.8442 (0.8623)  loss_ce_2: 1.8195 (1.7676)  loss_bbox_2: 0.4664 (0.4637)  loss_giou_2: 0.8088 (0.8375)  loss_ce_3: 1.8328 (1.7813)  loss_bbox_3: 0.4635 (0.4616)  loss_giou_3: 0.8068 (0.8324)  loss_ce_4: 1.8570 (1.7836)  loss_bbox_4: 0.4635 (0.4609)  loss_giou_4: 0.8062 (0.8336)  loss_ce_unscaled: 0.9352 (0.8911)  class_error_unscaled: 62.8571 (59.6720)  loss_bbox_unscaled: 0.0924 (0.0922)  loss_giou_unscaled: 0.4053 (0.4172)  cardinality_error_unscaled: 292.3333 (291.8611)  loss_ce_0_unscaled: 0.9368 (0.9000)  loss_bbox_0_unscaled: 0.1079 (0.1053)  loss_giou_0_unscaled: 0.4730 (0.4793)  cardinality_error_0_unscaled: 292.5000 (292.2222)  loss_ce_1_unscaled: 0.9008 (0.8881)  loss_bbox_1_unscaled: 0.0943 (0.0945)  loss_giou_1_unscaled: 0.4221 (0.4311)  cardinality_error_1_unscaled: 292.3333 (291.9967)  loss_ce_2_unscaled: 0.9098 (0.8838)  loss_bbox_2_unscaled: 0.0933 (0.0927)  loss_giou_2_unscaled: 0.4044 (0.4188)  cardinality_error_2_unscaled: 292.1667 (292.0131)  loss_ce_3_unscaled: 0.9164 (0.8907)  loss_bbox_3_unscaled: 0.0927 (0.0923)  loss_giou_3_unscaled: 0.4034 (0.4162)  cardinality_error_3_unscaled: 292.1667 (292.0392)  loss_ce_4_unscaled: 0.9285 (0.8918)  loss_bbox_4_unscaled: 0.0927 (0.0922)  loss_giou_4_unscaled: 0.4031 (0.4168)  cardinality_error_4_unscaled: 292.5000 (292.0409)  time: 3.1724  data: 0.0344  max mem: 8138\n",
            "Test:  [ 60/417]  eta: 0:18:06  class_error: 48.86  loss: 19.1316 (18.9674)  loss_ce: 1.8392 (1.8162)  loss_bbox: 0.4652 (0.4651)  loss_giou: 0.8736 (0.8418)  loss_ce_0: 1.8191 (1.8340)  loss_bbox_0: 0.5397 (0.5285)  loss_giou_0: 0.9982 (0.9660)  loss_ce_1: 1.8137 (1.8107)  loss_bbox_1: 0.4814 (0.4756)  loss_giou_1: 0.9168 (0.8693)  loss_ce_2: 1.8195 (1.8030)  loss_bbox_2: 0.4682 (0.4664)  loss_giou_2: 0.9017 (0.8459)  loss_ce_3: 1.8245 (1.8175)  loss_bbox_3: 0.4662 (0.4642)  loss_giou_3: 0.8747 (0.8404)  loss_ce_4: 1.8040 (1.8176)  loss_bbox_4: 0.4658 (0.4637)  loss_giou_4: 0.8730 (0.8412)  loss_ce_unscaled: 0.9196 (0.9081)  class_error_unscaled: 64.5161 (60.6758)  loss_bbox_unscaled: 0.0930 (0.0930)  loss_giou_unscaled: 0.4368 (0.4209)  cardinality_error_unscaled: 290.5000 (291.6571)  loss_ce_0_unscaled: 0.9095 (0.9170)  loss_bbox_0_unscaled: 0.1079 (0.1057)  loss_giou_0_unscaled: 0.4991 (0.4830)  cardinality_error_0_unscaled: 290.7500 (291.9303)  loss_ce_1_unscaled: 0.9069 (0.9054)  loss_bbox_1_unscaled: 0.0963 (0.0951)  loss_giou_1_unscaled: 0.4584 (0.4346)  cardinality_error_1_unscaled: 290.6667 (291.7760)  loss_ce_2_unscaled: 0.9098 (0.9015)  loss_bbox_2_unscaled: 0.0936 (0.0933)  loss_giou_2_unscaled: 0.4509 (0.4229)  cardinality_error_2_unscaled: 291.0000 (291.8251)  loss_ce_3_unscaled: 0.9122 (0.9088)  loss_bbox_3_unscaled: 0.0932 (0.0928)  loss_giou_3_unscaled: 0.4373 (0.4202)  cardinality_error_3_unscaled: 290.8333 (291.8279)  loss_ce_4_unscaled: 0.9020 (0.9088)  loss_bbox_4_unscaled: 0.0932 (0.0927)  loss_giou_4_unscaled: 0.4365 (0.4206)  cardinality_error_4_unscaled: 290.5833 (291.8347)  time: 3.1083  data: 0.0348  max mem: 8138\n",
            "Test:  [ 70/417]  eta: 0:17:46  class_error: 59.14  loss: 18.9374 (18.7895)  loss_ce: 1.8672 (1.7965)  loss_bbox: 0.4480 (0.4620)  loss_giou: 0.8265 (0.8358)  loss_ce_0: 1.8782 (1.8144)  loss_bbox_0: 0.5059 (0.5250)  loss_giou_0: 0.9554 (0.9604)  loss_ce_1: 1.8378 (1.7897)  loss_bbox_1: 0.4560 (0.4718)  loss_giou_1: 0.8489 (0.8630)  loss_ce_2: 1.8504 (1.7820)  loss_bbox_2: 0.4424 (0.4637)  loss_giou_2: 0.8328 (0.8400)  loss_ce_3: 1.8544 (1.7968)  loss_bbox_3: 0.4414 (0.4614)  loss_giou_3: 0.8301 (0.8342)  loss_ce_4: 1.8596 (1.7973)  loss_bbox_4: 0.4454 (0.4606)  loss_giou_4: 0.8292 (0.8352)  loss_ce_unscaled: 0.9336 (0.8982)  class_error_unscaled: 59.8131 (59.9030)  loss_bbox_unscaled: 0.0896 (0.0924)  loss_giou_unscaled: 0.4133 (0.4179)  cardinality_error_unscaled: 290.5000 (291.7113)  loss_ce_0_unscaled: 0.9391 (0.9072)  loss_bbox_0_unscaled: 0.1012 (0.1050)  loss_giou_0_unscaled: 0.4777 (0.4802)  cardinality_error_0_unscaled: 290.8333 (292.0082)  loss_ce_1_unscaled: 0.9189 (0.8948)  loss_bbox_1_unscaled: 0.0912 (0.0944)  loss_giou_1_unscaled: 0.4244 (0.4315)  cardinality_error_1_unscaled: 290.9167 (291.8486)  loss_ce_2_unscaled: 0.9252 (0.8910)  loss_bbox_2_unscaled: 0.0885 (0.0927)  loss_giou_2_unscaled: 0.4164 (0.4200)  cardinality_error_2_unscaled: 291.0833 (291.9002)  loss_ce_3_unscaled: 0.9272 (0.8984)  loss_bbox_3_unscaled: 0.0883 (0.0923)  loss_giou_3_unscaled: 0.4151 (0.4171)  cardinality_error_3_unscaled: 290.9167 (291.8991)  loss_ce_4_unscaled: 0.9298 (0.8986)  loss_bbox_4_unscaled: 0.0891 (0.0921)  loss_giou_4_unscaled: 0.4146 (0.4176)  cardinality_error_4_unscaled: 290.8333 (291.8979)  time: 3.1861  data: 0.0312  max mem: 8138\n",
            "Test:  [ 80/417]  eta: 0:17:12  class_error: 67.19  loss: 17.6181 (18.6865)  loss_ce: 1.7917 (1.7921)  loss_bbox: 0.4409 (0.4584)  loss_giou: 0.7597 (0.8236)  loss_ce_0: 1.8122 (1.8140)  loss_bbox_0: 0.4970 (0.5236)  loss_giou_0: 0.8926 (0.9485)  loss_ce_1: 1.8001 (1.7898)  loss_bbox_1: 0.4403 (0.4682)  loss_giou_1: 0.7888 (0.8505)  loss_ce_2: 1.7904 (1.7812)  loss_bbox_2: 0.4423 (0.4603)  loss_giou_2: 0.7691 (0.8278)  loss_ce_3: 1.8007 (1.7938)  loss_bbox_3: 0.4330 (0.4580)  loss_giou_3: 0.7604 (0.8223)  loss_ce_4: 1.8029 (1.7936)  loss_bbox_4: 0.4377 (0.4575)  loss_giou_4: 0.7613 (0.8233)  loss_ce_unscaled: 0.8958 (0.8961)  class_error_unscaled: 59.1398 (59.5824)  loss_bbox_unscaled: 0.0882 (0.0917)  loss_giou_unscaled: 0.3798 (0.4118)  cardinality_error_unscaled: 292.7500 (291.8961)  loss_ce_0_unscaled: 0.9061 (0.9070)  loss_bbox_0_unscaled: 0.0994 (0.1047)  loss_giou_0_unscaled: 0.4463 (0.4742)  cardinality_error_0_unscaled: 292.8333 (292.2017)  loss_ce_1_unscaled: 0.9000 (0.8949)  loss_bbox_1_unscaled: 0.0881 (0.0936)  loss_giou_1_unscaled: 0.3944 (0.4253)  cardinality_error_1_unscaled: 292.5000 (292.0196)  loss_ce_2_unscaled: 0.8952 (0.8906)  loss_bbox_2_unscaled: 0.0885 (0.0921)  loss_giou_2_unscaled: 0.3846 (0.4139)  cardinality_error_2_unscaled: 292.3333 (292.0648)  loss_ce_3_unscaled: 0.9003 (0.8969)  loss_bbox_3_unscaled: 0.0866 (0.0916)  loss_giou_3_unscaled: 0.3802 (0.4112)  cardinality_error_3_unscaled: 292.8333 (292.0823)  loss_ce_4_unscaled: 0.9015 (0.8968)  loss_bbox_4_unscaled: 0.0875 (0.0915)  loss_giou_4_unscaled: 0.3807 (0.4116)  cardinality_error_4_unscaled: 292.5000 (292.0741)  time: 3.1284  data: 0.0299  max mem: 8138\n",
            "Test:  [ 90/417]  eta: 0:16:43  class_error: 43.75  loss: 17.4817 (18.6852)  loss_ce: 1.7310 (1.7895)  loss_bbox: 0.4409 (0.4573)  loss_giou: 0.8118 (0.8275)  loss_ce_0: 1.7860 (1.8116)  loss_bbox_0: 0.4955 (0.5216)  loss_giou_0: 0.9224 (0.9518)  loss_ce_1: 1.6993 (1.7870)  loss_bbox_1: 0.4419 (0.4670)  loss_giou_1: 0.8405 (0.8548)  loss_ce_2: 1.7156 (1.7777)  loss_bbox_2: 0.4499 (0.4590)  loss_giou_2: 0.8096 (0.8326)  loss_ce_3: 1.7214 (1.7906)  loss_bbox_3: 0.4444 (0.4566)  loss_giou_3: 0.8157 (0.8266)  loss_ce_4: 1.7402 (1.7907)  loss_bbox_4: 0.4377 (0.4564)  loss_giou_4: 0.8194 (0.8271)  loss_ce_unscaled: 0.8655 (0.8947)  class_error_unscaled: 63.6364 (59.5842)  loss_bbox_unscaled: 0.0882 (0.0915)  loss_giou_unscaled: 0.4059 (0.4137)  cardinality_error_unscaled: 292.4167 (291.8059)  loss_ce_0_unscaled: 0.8930 (0.9058)  loss_bbox_0_unscaled: 0.0991 (0.1043)  loss_giou_0_unscaled: 0.4612 (0.4759)  cardinality_error_0_unscaled: 292.7500 (292.1227)  loss_ce_1_unscaled: 0.8496 (0.8935)  loss_bbox_1_unscaled: 0.0884 (0.0934)  loss_giou_1_unscaled: 0.4203 (0.4274)  cardinality_error_1_unscaled: 292.5000 (291.9670)  loss_ce_2_unscaled: 0.8578 (0.8889)  loss_bbox_2_unscaled: 0.0900 (0.0918)  loss_giou_2_unscaled: 0.4048 (0.4163)  cardinality_error_2_unscaled: 292.3333 (291.9918)  loss_ce_3_unscaled: 0.8607 (0.8953)  loss_bbox_3_unscaled: 0.0889 (0.0913)  loss_giou_3_unscaled: 0.4079 (0.4133)  cardinality_error_3_unscaled: 292.4167 (292.0137)  loss_ce_4_unscaled: 0.8701 (0.8953)  loss_bbox_4_unscaled: 0.0875 (0.0913)  loss_giou_4_unscaled: 0.4097 (0.4136)  cardinality_error_4_unscaled: 292.5000 (291.9744)  time: 3.0446  data: 0.0295  max mem: 8138\n",
            "Test:  [100/417]  eta: 0:16:10  class_error: 70.42  loss: 19.2659 (18.7600)  loss_ce: 1.8464 (1.7998)  loss_bbox: 0.4707 (0.4585)  loss_giou: 0.8181 (0.8276)  loss_ce_0: 1.9372 (1.8205)  loss_bbox_0: 0.5014 (0.5226)  loss_giou_0: 0.9740 (0.9527)  loss_ce_1: 1.8724 (1.7971)  loss_bbox_1: 0.4621 (0.4682)  loss_giou_1: 0.8547 (0.8559)  loss_ce_2: 1.8305 (1.7905)  loss_bbox_2: 0.4516 (0.4596)  loss_giou_2: 0.8157 (0.8336)  loss_ce_3: 1.8649 (1.8021)  loss_bbox_3: 0.4559 (0.4577)  loss_giou_3: 0.8203 (0.8273)  loss_ce_4: 1.8286 (1.8008)  loss_bbox_4: 0.4697 (0.4577)  loss_giou_4: 0.8238 (0.8277)  loss_ce_unscaled: 0.9232 (0.8999)  class_error_unscaled: 64.9351 (60.0437)  loss_bbox_unscaled: 0.0941 (0.0917)  loss_giou_unscaled: 0.4090 (0.4138)  cardinality_error_unscaled: 291.1667 (291.8111)  loss_ce_0_unscaled: 0.9686 (0.9103)  loss_bbox_0_unscaled: 0.1003 (0.1045)  loss_giou_0_unscaled: 0.4870 (0.4764)  cardinality_error_0_unscaled: 291.6667 (292.1221)  loss_ce_1_unscaled: 0.9362 (0.8985)  loss_bbox_1_unscaled: 0.0924 (0.0936)  loss_giou_1_unscaled: 0.4274 (0.4280)  cardinality_error_1_unscaled: 291.5000 (291.9596)  loss_ce_2_unscaled: 0.9152 (0.8953)  loss_bbox_2_unscaled: 0.0903 (0.0919)  loss_giou_2_unscaled: 0.4079 (0.4168)  cardinality_error_2_unscaled: 291.5833 (291.9744)  loss_ce_3_unscaled: 0.9325 (0.9011)  loss_bbox_3_unscaled: 0.0912 (0.0915)  loss_giou_3_unscaled: 0.4102 (0.4136)  cardinality_error_3_unscaled: 291.5833 (292.0140)  loss_ce_4_unscaled: 0.9143 (0.9004)  loss_bbox_4_unscaled: 0.0939 (0.0915)  loss_giou_4_unscaled: 0.4119 (0.4138)  cardinality_error_4_unscaled: 291.4167 (291.9538)  time: 3.0556  data: 0.0304  max mem: 8138\n",
            "Test:  [110/417]  eta: 0:15:39  class_error: 54.29  loss: 20.4027 (18.9487)  loss_ce: 1.9564 (1.8131)  loss_bbox: 0.5096 (0.4666)  loss_giou: 0.8523 (0.8370)  loss_ce_0: 1.9729 (1.8376)  loss_bbox_0: 0.5691 (0.5301)  loss_giou_0: 0.9945 (0.9611)  loss_ce_1: 1.9941 (1.8124)  loss_bbox_1: 0.5213 (0.4766)  loss_giou_1: 0.9136 (0.8650)  loss_ce_2: 1.9928 (1.8044)  loss_bbox_2: 0.5176 (0.4683)  loss_giou_2: 0.8731 (0.8426)  loss_ce_3: 1.9604 (1.8150)  loss_bbox_3: 0.5186 (0.4663)  loss_giou_3: 0.8587 (0.8363)  loss_ce_4: 1.9656 (1.8138)  loss_bbox_4: 0.5142 (0.4660)  loss_giou_4: 0.8553 (0.8366)  loss_ce_unscaled: 0.9782 (0.9065)  class_error_unscaled: 65.2174 (60.4557)  loss_bbox_unscaled: 0.1019 (0.0933)  loss_giou_unscaled: 0.4262 (0.4185)  cardinality_error_unscaled: 291.4167 (291.8048)  loss_ce_0_unscaled: 0.9865 (0.9188)  loss_bbox_0_unscaled: 0.1138 (0.1060)  loss_giou_0_unscaled: 0.4973 (0.4805)  cardinality_error_0_unscaled: 291.6667 (292.1096)  loss_ce_1_unscaled: 0.9971 (0.9062)  loss_bbox_1_unscaled: 0.1043 (0.0953)  loss_giou_1_unscaled: 0.4568 (0.4325)  cardinality_error_1_unscaled: 291.5000 (291.9437)  loss_ce_2_unscaled: 0.9964 (0.9022)  loss_bbox_2_unscaled: 0.1035 (0.0937)  loss_giou_2_unscaled: 0.4365 (0.4213)  cardinality_error_2_unscaled: 291.5833 (291.9677)  loss_ce_3_unscaled: 0.9802 (0.9075)  loss_bbox_3_unscaled: 0.1037 (0.0933)  loss_giou_3_unscaled: 0.4293 (0.4182)  cardinality_error_3_unscaled: 291.5833 (292.0105)  loss_ce_4_unscaled: 0.9828 (0.9069)  loss_bbox_4_unscaled: 0.1028 (0.0932)  loss_giou_4_unscaled: 0.4277 (0.4183)  cardinality_error_4_unscaled: 291.4167 (291.9617)  time: 3.0253  data: 0.0310  max mem: 8138\n",
            "Test:  [120/417]  eta: 0:15:05  class_error: 60.00  loss: 20.4027 (18.9689)  loss_ce: 1.9564 (1.8189)  loss_bbox: 0.4776 (0.4667)  loss_giou: 0.8508 (0.8371)  loss_ce_0: 1.9867 (1.8390)  loss_bbox_0: 0.5283 (0.5288)  loss_giou_0: 0.9883 (0.9609)  loss_ce_1: 1.9246 (1.8125)  loss_bbox_1: 0.4782 (0.4768)  loss_giou_1: 0.8912 (0.8651)  loss_ce_2: 1.9285 (1.8066)  loss_bbox_2: 0.4703 (0.4684)  loss_giou_2: 0.8547 (0.8429)  loss_ce_3: 1.9604 (1.8194)  loss_bbox_3: 0.4741 (0.4665)  loss_giou_3: 0.8545 (0.8366)  loss_ce_4: 1.9656 (1.8205)  loss_bbox_4: 0.4665 (0.4660)  loss_giou_4: 0.8553 (0.8363)  loss_ce_unscaled: 0.9782 (0.9094)  class_error_unscaled: 65.2174 (60.8132)  loss_bbox_unscaled: 0.0955 (0.0933)  loss_giou_unscaled: 0.4254 (0.4185)  cardinality_error_unscaled: 290.9167 (291.6784)  loss_ce_0_unscaled: 0.9934 (0.9195)  loss_bbox_0_unscaled: 0.1057 (0.1058)  loss_giou_0_unscaled: 0.4942 (0.4804)  cardinality_error_0_unscaled: 291.3333 (292.0159)  loss_ce_1_unscaled: 0.9623 (0.9062)  loss_bbox_1_unscaled: 0.0956 (0.0954)  loss_giou_1_unscaled: 0.4456 (0.4326)  cardinality_error_1_unscaled: 291.0833 (291.8320)  loss_ce_2_unscaled: 0.9643 (0.9033)  loss_bbox_2_unscaled: 0.0941 (0.0937)  loss_giou_2_unscaled: 0.4273 (0.4214)  cardinality_error_2_unscaled: 291.3333 (291.8636)  loss_ce_3_unscaled: 0.9802 (0.9097)  loss_bbox_3_unscaled: 0.0948 (0.0933)  loss_giou_3_unscaled: 0.4273 (0.4183)  cardinality_error_3_unscaled: 291.3333 (291.9208)  loss_ce_4_unscaled: 0.9828 (0.9102)  loss_bbox_4_unscaled: 0.0933 (0.0932)  loss_giou_4_unscaled: 0.4277 (0.4181)  cardinality_error_4_unscaled: 291.3333 (291.8333)  time: 2.9708  data: 0.0298  max mem: 8138\n",
            "Test:  [130/417]  eta: 0:14:32  class_error: 59.02  loss: 19.0142 (19.0001)  loss_ce: 1.8656 (1.8187)  loss_bbox: 0.4730 (0.4704)  loss_giou: 0.8506 (0.8396)  loss_ce_0: 1.7576 (1.8404)  loss_bbox_0: 0.5276 (0.5316)  loss_giou_0: 0.9629 (0.9611)  loss_ce_1: 1.7759 (1.8119)  loss_bbox_1: 0.4816 (0.4802)  loss_giou_1: 0.8772 (0.8673)  loss_ce_2: 1.7912 (1.8061)  loss_bbox_2: 0.4703 (0.4720)  loss_giou_2: 0.8547 (0.8453)  loss_ce_3: 1.8495 (1.8186)  loss_bbox_3: 0.4741 (0.4701)  loss_giou_3: 0.8444 (0.8387)  loss_ce_4: 1.8812 (1.8200)  loss_bbox_4: 0.4665 (0.4695)  loss_giou_4: 0.8482 (0.8388)  loss_ce_unscaled: 0.9328 (0.9093)  class_error_unscaled: 62.0253 (60.8631)  loss_bbox_unscaled: 0.0946 (0.0941)  loss_giou_unscaled: 0.4253 (0.4198)  cardinality_error_unscaled: 290.6667 (291.6597)  loss_ce_0_unscaled: 0.8788 (0.9202)  loss_bbox_0_unscaled: 0.1055 (0.1063)  loss_giou_0_unscaled: 0.4815 (0.4805)  cardinality_error_0_unscaled: 291.3333 (291.9911)  loss_ce_1_unscaled: 0.8879 (0.9060)  loss_bbox_1_unscaled: 0.0963 (0.0960)  loss_giou_1_unscaled: 0.4386 (0.4336)  cardinality_error_1_unscaled: 291.0833 (291.7996)  loss_ce_2_unscaled: 0.8956 (0.9030)  loss_bbox_2_unscaled: 0.0941 (0.0944)  loss_giou_2_unscaled: 0.4273 (0.4227)  cardinality_error_2_unscaled: 291.3333 (291.8295)  loss_ce_3_unscaled: 0.9248 (0.9093)  loss_bbox_3_unscaled: 0.0948 (0.0940)  loss_giou_3_unscaled: 0.4222 (0.4193)  cardinality_error_3_unscaled: 291.3333 (291.8906)  loss_ce_4_unscaled: 0.9406 (0.9100)  loss_bbox_4_unscaled: 0.0933 (0.0939)  loss_giou_4_unscaled: 0.4241 (0.4194)  cardinality_error_4_unscaled: 291.3333 (291.7964)  time: 2.9376  data: 0.0286  max mem: 8138\n",
            "Test:  [140/417]  eta: 0:14:03  class_error: 67.00  loss: 19.3652 (19.0532)  loss_ce: 1.8453 (1.8243)  loss_bbox: 0.4932 (0.4711)  loss_giou: 0.8618 (0.8413)  loss_ce_0: 1.9216 (1.8489)  loss_bbox_0: 0.5630 (0.5314)  loss_giou_0: 0.9875 (0.9626)  loss_ce_1: 1.8548 (1.8201)  loss_bbox_1: 0.5046 (0.4809)  loss_giou_1: 0.8829 (0.8687)  loss_ce_2: 1.8163 (1.8125)  loss_bbox_2: 0.4964 (0.4725)  loss_giou_2: 0.8613 (0.8470)  loss_ce_3: 1.8366 (1.8242)  loss_bbox_3: 0.4911 (0.4709)  loss_giou_3: 0.8548 (0.8405)  loss_ce_4: 1.8696 (1.8255)  loss_bbox_4: 0.4879 (0.4702)  loss_giou_4: 0.8573 (0.8406)  loss_ce_unscaled: 0.9226 (0.9121)  class_error_unscaled: 59.6330 (60.9531)  loss_bbox_unscaled: 0.0986 (0.0942)  loss_giou_unscaled: 0.4309 (0.4207)  cardinality_error_unscaled: 291.6667 (291.6413)  loss_ce_0_unscaled: 0.9608 (0.9245)  loss_bbox_0_unscaled: 0.1126 (0.1063)  loss_giou_0_unscaled: 0.4938 (0.4813)  cardinality_error_0_unscaled: 292.0000 (291.9498)  loss_ce_1_unscaled: 0.9274 (0.9100)  loss_bbox_1_unscaled: 0.1009 (0.0962)  loss_giou_1_unscaled: 0.4415 (0.4343)  cardinality_error_1_unscaled: 291.6667 (291.7849)  loss_ce_2_unscaled: 0.9082 (0.9063)  loss_bbox_2_unscaled: 0.0993 (0.0945)  loss_giou_2_unscaled: 0.4306 (0.4235)  cardinality_error_2_unscaled: 291.9167 (291.8174)  loss_ce_3_unscaled: 0.9183 (0.9121)  loss_bbox_3_unscaled: 0.0982 (0.0942)  loss_giou_3_unscaled: 0.4274 (0.4202)  cardinality_error_3_unscaled: 291.5833 (291.8718)  loss_ce_4_unscaled: 0.9348 (0.9128)  loss_bbox_4_unscaled: 0.0976 (0.0940)  loss_giou_4_unscaled: 0.4287 (0.4203)  cardinality_error_4_unscaled: 291.5833 (291.7766)  time: 3.0382  data: 0.0308  max mem: 8138\n",
            "Test:  [150/417]  eta: 0:13:32  class_error: 53.00  loss: 19.3652 (18.9908)  loss_ce: 1.7231 (1.8189)  loss_bbox: 0.4656 (0.4702)  loss_giou: 0.8228 (0.8381)  loss_ce_0: 1.8164 (1.8415)  loss_bbox_0: 0.5437 (0.5308)  loss_giou_0: 0.9647 (0.9599)  loss_ce_1: 1.7390 (1.8123)  loss_bbox_1: 0.4695 (0.4802)  loss_giou_1: 0.8500 (0.8658)  loss_ce_2: 1.6916 (1.8053)  loss_bbox_2: 0.4528 (0.4713)  loss_giou_2: 0.8337 (0.8438)  loss_ce_3: 1.6933 (1.8183)  loss_bbox_3: 0.4736 (0.4699)  loss_giou_3: 0.8270 (0.8372)  loss_ce_4: 1.7369 (1.8205)  loss_bbox_4: 0.4656 (0.4694)  loss_giou_4: 0.8186 (0.8374)  loss_ce_unscaled: 0.8615 (0.9094)  class_error_unscaled: 59.6330 (60.8631)  loss_bbox_unscaled: 0.0931 (0.0940)  loss_giou_unscaled: 0.4114 (0.4191)  cardinality_error_unscaled: 291.6667 (291.6203)  loss_ce_0_unscaled: 0.9082 (0.9207)  loss_bbox_0_unscaled: 0.1087 (0.1062)  loss_giou_0_unscaled: 0.4824 (0.4800)  cardinality_error_0_unscaled: 292.4167 (291.9007)  loss_ce_1_unscaled: 0.8695 (0.9062)  loss_bbox_1_unscaled: 0.0939 (0.0960)  loss_giou_1_unscaled: 0.4250 (0.4329)  cardinality_error_1_unscaled: 291.9167 (291.7506)  loss_ce_2_unscaled: 0.8458 (0.9026)  loss_bbox_2_unscaled: 0.0906 (0.0943)  loss_giou_2_unscaled: 0.4168 (0.4219)  cardinality_error_2_unscaled: 291.6667 (291.7875)  loss_ce_3_unscaled: 0.8467 (0.9091)  loss_bbox_3_unscaled: 0.0947 (0.0940)  loss_giou_3_unscaled: 0.4135 (0.4186)  cardinality_error_3_unscaled: 291.5833 (291.8510)  loss_ce_4_unscaled: 0.8684 (0.9102)  loss_bbox_4_unscaled: 0.0931 (0.0939)  loss_giou_4_unscaled: 0.4093 (0.4187)  cardinality_error_4_unscaled: 291.6667 (291.7517)  time: 3.0631  data: 0.0320  max mem: 8138\n",
            "Test:  [160/417]  eta: 0:13:01  class_error: 44.00  loss: 18.4829 (18.9085)  loss_ce: 1.6192 (1.8124)  loss_bbox: 0.4579 (0.4665)  loss_giou: 0.7740 (0.8351)  loss_ce_0: 1.6697 (1.8323)  loss_bbox_0: 0.4841 (0.5278)  loss_giou_0: 0.9121 (0.9578)  loss_ce_1: 1.6279 (1.8041)  loss_bbox_1: 0.4514 (0.4766)  loss_giou_1: 0.8050 (0.8630)  loss_ce_2: 1.6488 (1.7982)  loss_bbox_2: 0.4387 (0.4675)  loss_giou_2: 0.7744 (0.8409)  loss_ce_3: 1.6390 (1.8114)  loss_bbox_3: 0.4419 (0.4663)  loss_giou_3: 0.7848 (0.8345)  loss_ce_4: 1.6505 (1.8138)  loss_bbox_4: 0.4568 (0.4657)  loss_giou_4: 0.7803 (0.8347)  loss_ce_unscaled: 0.8096 (0.9062)  class_error_unscaled: 59.6154 (60.5639)  loss_bbox_unscaled: 0.0916 (0.0933)  loss_giou_unscaled: 0.3870 (0.4175)  cardinality_error_unscaled: 291.6667 (291.6486)  loss_ce_0_unscaled: 0.8349 (0.9162)  loss_bbox_0_unscaled: 0.0968 (0.1056)  loss_giou_0_unscaled: 0.4561 (0.4789)  cardinality_error_0_unscaled: 292.7500 (291.9260)  loss_ce_1_unscaled: 0.8139 (0.9021)  loss_bbox_1_unscaled: 0.0903 (0.0953)  loss_giou_1_unscaled: 0.4025 (0.4315)  cardinality_error_1_unscaled: 291.9167 (291.7821)  loss_ce_2_unscaled: 0.8244 (0.8991)  loss_bbox_2_unscaled: 0.0877 (0.0935)  loss_giou_2_unscaled: 0.3872 (0.4204)  cardinality_error_2_unscaled: 291.6667 (291.8178)  loss_ce_3_unscaled: 0.8195 (0.9057)  loss_bbox_3_unscaled: 0.0884 (0.0933)  loss_giou_3_unscaled: 0.3924 (0.4172)  cardinality_error_3_unscaled: 291.5833 (291.8763)  loss_ce_4_unscaled: 0.8252 (0.9069)  loss_bbox_4_unscaled: 0.0914 (0.0931)  loss_giou_4_unscaled: 0.3902 (0.4173)  cardinality_error_4_unscaled: 291.6667 (291.7774)  time: 3.0171  data: 0.0320  max mem: 8138\n",
            "Test:  [170/417]  eta: 0:12:31  class_error: 59.55  loss: 19.5324 (18.9171)  loss_ce: 1.8060 (1.8128)  loss_bbox: 0.4066 (0.4663)  loss_giou: 0.8481 (0.8360)  loss_ce_0: 1.7957 (1.8330)  loss_bbox_0: 0.4841 (0.5281)  loss_giou_0: 0.9262 (0.9590)  loss_ce_1: 1.8064 (1.8045)  loss_bbox_1: 0.4332 (0.4767)  loss_giou_1: 0.8784 (0.8641)  loss_ce_2: 1.7768 (1.7989)  loss_bbox_2: 0.4204 (0.4674)  loss_giou_2: 0.8494 (0.8420)  loss_ce_3: 1.7778 (1.8115)  loss_bbox_3: 0.4100 (0.4662)  loss_giou_3: 0.8479 (0.8357)  loss_ce_4: 1.8131 (1.8138)  loss_bbox_4: 0.4103 (0.4655)  loss_giou_4: 0.8543 (0.8356)  loss_ce_unscaled: 0.9030 (0.9064)  class_error_unscaled: 61.9048 (60.4415)  loss_bbox_unscaled: 0.0813 (0.0933)  loss_giou_unscaled: 0.4241 (0.4180)  cardinality_error_unscaled: 291.1667 (291.5819)  loss_ce_0_unscaled: 0.8978 (0.9165)  loss_bbox_0_unscaled: 0.0968 (0.1056)  loss_giou_0_unscaled: 0.4631 (0.4795)  cardinality_error_0_unscaled: 291.5000 (291.8718)  loss_ce_1_unscaled: 0.9032 (0.9023)  loss_bbox_1_unscaled: 0.0866 (0.0953)  loss_giou_1_unscaled: 0.4392 (0.4321)  cardinality_error_1_unscaled: 291.5000 (291.7149)  loss_ce_2_unscaled: 0.8884 (0.8994)  loss_bbox_2_unscaled: 0.0841 (0.0935)  loss_giou_2_unscaled: 0.4247 (0.4210)  cardinality_error_2_unscaled: 291.5000 (291.7476)  loss_ce_3_unscaled: 0.8889 (0.9058)  loss_bbox_3_unscaled: 0.0820 (0.0932)  loss_giou_3_unscaled: 0.4240 (0.4178)  cardinality_error_3_unscaled: 291.3333 (291.7978)  loss_ce_4_unscaled: 0.9065 (0.9069)  loss_bbox_4_unscaled: 0.0821 (0.0931)  loss_giou_4_unscaled: 0.4271 (0.4178)  cardinality_error_4_unscaled: 291.2500 (291.7027)  time: 3.0227  data: 0.0301  max mem: 8138\n",
            "Test:  [180/417]  eta: 0:12:00  class_error: 51.61  loss: 17.2211 (18.7563)  loss_ce: 1.7450 (1.7960)  loss_bbox: 0.4236 (0.4638)  loss_giou: 0.7794 (0.8294)  loss_ce_0: 1.6771 (1.8146)  loss_bbox_0: 0.4801 (0.5262)  loss_giou_0: 0.8895 (0.9524)  loss_ce_1: 1.6792 (1.7862)  loss_bbox_1: 0.4390 (0.4745)  loss_giou_1: 0.7953 (0.8574)  loss_ce_2: 1.6687 (1.7805)  loss_bbox_2: 0.4235 (0.4650)  loss_giou_2: 0.7837 (0.8356)  loss_ce_3: 1.6885 (1.7937)  loss_bbox_3: 0.4190 (0.4637)  loss_giou_3: 0.7824 (0.8291)  loss_ce_4: 1.7106 (1.7962)  loss_bbox_4: 0.4223 (0.4630)  loss_giou_4: 0.7792 (0.8290)  loss_ce_unscaled: 0.8725 (0.8980)  class_error_unscaled: 57.1429 (59.9229)  loss_bbox_unscaled: 0.0847 (0.0928)  loss_giou_unscaled: 0.3897 (0.4147)  cardinality_error_unscaled: 291.4167 (291.5645)  loss_ce_0_unscaled: 0.8386 (0.9073)  loss_bbox_0_unscaled: 0.0960 (0.1052)  loss_giou_0_unscaled: 0.4448 (0.4762)  cardinality_error_0_unscaled: 291.6667 (291.8780)  loss_ce_1_unscaled: 0.8396 (0.8931)  loss_bbox_1_unscaled: 0.0878 (0.0949)  loss_giou_1_unscaled: 0.3976 (0.4287)  cardinality_error_1_unscaled: 291.5833 (291.6938)  loss_ce_2_unscaled: 0.8344 (0.8902)  loss_bbox_2_unscaled: 0.0847 (0.0930)  loss_giou_2_unscaled: 0.3919 (0.4178)  cardinality_error_2_unscaled: 291.7500 (291.7339)  loss_ce_3_unscaled: 0.8442 (0.8969)  loss_bbox_3_unscaled: 0.0838 (0.0927)  loss_giou_3_unscaled: 0.3912 (0.4146)  cardinality_error_3_unscaled: 291.4167 (291.7901)  loss_ce_4_unscaled: 0.8553 (0.8981)  loss_bbox_4_unscaled: 0.0845 (0.0926)  loss_giou_4_unscaled: 0.3896 (0.4145)  cardinality_error_4_unscaled: 291.8333 (291.6754)  time: 3.0258  data: 0.0287  max mem: 8138\n",
            "Test:  [190/417]  eta: 0:11:29  class_error: 75.00  loss: 17.2211 (18.8808)  loss_ce: 1.7913 (1.8121)  loss_bbox: 0.4382 (0.4659)  loss_giou: 0.7738 (0.8315)  loss_ce_0: 1.7222 (1.8314)  loss_bbox_0: 0.4981 (0.5285)  loss_giou_0: 0.8804 (0.9535)  loss_ce_1: 1.7261 (1.8039)  loss_bbox_1: 0.4512 (0.4764)  loss_giou_1: 0.7926 (0.8588)  loss_ce_2: 1.7427 (1.7983)  loss_bbox_2: 0.4365 (0.4671)  loss_giou_2: 0.7816 (0.8374)  loss_ce_3: 1.8215 (1.8109)  loss_bbox_3: 0.4288 (0.4658)  loss_giou_3: 0.7676 (0.8311)  loss_ce_4: 1.8279 (1.8124)  loss_bbox_4: 0.4330 (0.4651)  loss_giou_4: 0.7690 (0.8310)  loss_ce_unscaled: 0.8957 (0.9060)  class_error_unscaled: 62.9870 (60.4865)  loss_bbox_unscaled: 0.0876 (0.0932)  loss_giou_unscaled: 0.3869 (0.4157)  cardinality_error_unscaled: 293.0000 (291.6073)  loss_ce_0_unscaled: 0.8611 (0.9157)  loss_bbox_0_unscaled: 0.0996 (0.1057)  loss_giou_0_unscaled: 0.4402 (0.4768)  cardinality_error_0_unscaled: 293.1667 (291.9289)  loss_ce_1_unscaled: 0.8630 (0.9019)  loss_bbox_1_unscaled: 0.0902 (0.0953)  loss_giou_1_unscaled: 0.3963 (0.4294)  cardinality_error_1_unscaled: 292.5000 (291.7404)  loss_ce_2_unscaled: 0.8714 (0.8991)  loss_bbox_2_unscaled: 0.0873 (0.0934)  loss_giou_2_unscaled: 0.3908 (0.4187)  cardinality_error_2_unscaled: 293.0833 (291.7880)  loss_ce_3_unscaled: 0.9108 (0.9055)  loss_bbox_3_unscaled: 0.0858 (0.0932)  loss_giou_3_unscaled: 0.3838 (0.4155)  cardinality_error_3_unscaled: 293.1667 (291.8333)  loss_ce_4_unscaled: 0.9140 (0.9062)  loss_bbox_4_unscaled: 0.0866 (0.0930)  loss_giou_4_unscaled: 0.3845 (0.4155)  cardinality_error_4_unscaled: 293.0000 (291.7308)  time: 3.0158  data: 0.0298  max mem: 8138\n",
            "Test:  [200/417]  eta: 0:10:58  class_error: 73.64  loss: 19.3447 (18.8624)  loss_ce: 1.8809 (1.8077)  loss_bbox: 0.4826 (0.4675)  loss_giou: 0.8472 (0.8319)  loss_ce_0: 1.8614 (1.8252)  loss_bbox_0: 0.5627 (0.5300)  loss_giou_0: 0.9749 (0.9544)  loss_ce_1: 1.8666 (1.7975)  loss_bbox_1: 0.5054 (0.4779)  loss_giou_1: 0.8661 (0.8593)  loss_ce_2: 1.8900 (1.7925)  loss_bbox_2: 0.4840 (0.4689)  loss_giou_2: 0.8438 (0.8381)  loss_ce_3: 1.9079 (1.8063)  loss_bbox_3: 0.4834 (0.4674)  loss_giou_3: 0.8488 (0.8317)  loss_ce_4: 1.8626 (1.8078)  loss_bbox_4: 0.4825 (0.4668)  loss_giou_4: 0.8412 (0.8314)  loss_ce_unscaled: 0.9404 (0.9038)  class_error_unscaled: 63.6364 (60.3262)  loss_bbox_unscaled: 0.0965 (0.0935)  loss_giou_unscaled: 0.4236 (0.4160)  cardinality_error_unscaled: 292.0833 (291.6086)  loss_ce_0_unscaled: 0.9307 (0.9126)  loss_bbox_0_unscaled: 0.1125 (0.1060)  loss_giou_0_unscaled: 0.4875 (0.4772)  cardinality_error_0_unscaled: 292.7500 (291.9324)  loss_ce_1_unscaled: 0.9333 (0.8987)  loss_bbox_1_unscaled: 0.1011 (0.0956)  loss_giou_1_unscaled: 0.4330 (0.4296)  cardinality_error_1_unscaled: 292.2500 (291.7301)  loss_ce_2_unscaled: 0.9450 (0.8963)  loss_bbox_2_unscaled: 0.0968 (0.0938)  loss_giou_2_unscaled: 0.4219 (0.4191)  cardinality_error_2_unscaled: 292.6667 (291.7857)  loss_ce_3_unscaled: 0.9540 (0.9032)  loss_bbox_3_unscaled: 0.0967 (0.0935)  loss_giou_3_unscaled: 0.4244 (0.4159)  cardinality_error_3_unscaled: 292.4167 (291.8350)  loss_ce_4_unscaled: 0.9313 (0.9039)  loss_bbox_4_unscaled: 0.0965 (0.0934)  loss_giou_4_unscaled: 0.4206 (0.4157)  cardinality_error_4_unscaled: 292.7500 (291.7264)  time: 2.9772  data: 0.0293  max mem: 8138\n",
            "Test:  [210/417]  eta: 0:10:27  class_error: 63.51  loss: 18.3800 (18.9448)  loss_ce: 1.7812 (1.8148)  loss_bbox: 0.4931 (0.4688)  loss_giou: 0.8472 (0.8368)  loss_ce_0: 1.7463 (1.8334)  loss_bbox_0: 0.5433 (0.5312)  loss_giou_0: 0.9749 (0.9593)  loss_ce_1: 1.7552 (1.8049)  loss_bbox_1: 0.5054 (0.4794)  loss_giou_1: 0.8711 (0.8644)  loss_ce_2: 1.7191 (1.8000)  loss_bbox_2: 0.4954 (0.4704)  loss_giou_2: 0.8438 (0.8431)  loss_ce_3: 1.7869 (1.8133)  loss_bbox_3: 0.4994 (0.4688)  loss_giou_3: 0.8488 (0.8369)  loss_ce_4: 1.7811 (1.8147)  loss_bbox_4: 0.4910 (0.4680)  loss_giou_4: 0.8412 (0.8365)  loss_ce_unscaled: 0.8906 (0.9074)  class_error_unscaled: 60.0000 (60.4481)  loss_bbox_unscaled: 0.0986 (0.0938)  loss_giou_unscaled: 0.4236 (0.4184)  cardinality_error_unscaled: 290.8333 (291.5845)  loss_ce_0_unscaled: 0.8732 (0.9167)  loss_bbox_0_unscaled: 0.1087 (0.1062)  loss_giou_0_unscaled: 0.4875 (0.4796)  cardinality_error_0_unscaled: 291.3333 (291.9111)  loss_ce_1_unscaled: 0.8776 (0.9025)  loss_bbox_1_unscaled: 0.1011 (0.0959)  loss_giou_1_unscaled: 0.4356 (0.4322)  cardinality_error_1_unscaled: 291.4167 (291.7145)  loss_ce_2_unscaled: 0.8595 (0.9000)  loss_bbox_2_unscaled: 0.0991 (0.0941)  loss_giou_2_unscaled: 0.4219 (0.4216)  cardinality_error_2_unscaled: 291.4167 (291.7611)  loss_ce_3_unscaled: 0.8934 (0.9067)  loss_bbox_3_unscaled: 0.0999 (0.0938)  loss_giou_3_unscaled: 0.4244 (0.4184)  cardinality_error_3_unscaled: 290.8333 (291.8085)  loss_ce_4_unscaled: 0.8906 (0.9074)  loss_bbox_4_unscaled: 0.0982 (0.0936)  loss_giou_4_unscaled: 0.4206 (0.4183)  cardinality_error_4_unscaled: 290.8333 (291.6975)  time: 2.9599  data: 0.0294  max mem: 8138\n",
            "Test:  [220/417]  eta: 0:09:57  class_error: 54.21  loss: 19.9145 (18.9534)  loss_ce: 1.8272 (1.8122)  loss_bbox: 0.4919 (0.4688)  loss_giou: 0.8789 (0.8408)  loss_ce_0: 1.7824 (1.8301)  loss_bbox_0: 0.5377 (0.5311)  loss_giou_0: 1.0031 (0.9638)  loss_ce_1: 1.7894 (1.8024)  loss_bbox_1: 0.4901 (0.4792)  loss_giou_1: 0.8854 (0.8686)  loss_ce_2: 1.7972 (1.7976)  loss_bbox_2: 0.4832 (0.4703)  loss_giou_2: 0.8722 (0.8471)  loss_ce_3: 1.8111 (1.8110)  loss_bbox_3: 0.4743 (0.4687)  loss_giou_3: 0.8801 (0.8408)  loss_ce_4: 1.8067 (1.8124)  loss_bbox_4: 0.4690 (0.4679)  loss_giou_4: 0.8722 (0.8405)  loss_ce_unscaled: 0.9136 (0.9061)  class_error_unscaled: 60.0000 (60.3194)  loss_bbox_unscaled: 0.0984 (0.0938)  loss_giou_unscaled: 0.4395 (0.4204)  cardinality_error_unscaled: 290.6667 (291.5456)  loss_ce_0_unscaled: 0.8912 (0.9151)  loss_bbox_0_unscaled: 0.1075 (0.1062)  loss_giou_0_unscaled: 0.5016 (0.4819)  cardinality_error_0_unscaled: 291.1667 (291.8744)  loss_ce_1_unscaled: 0.8947 (0.9012)  loss_bbox_1_unscaled: 0.0980 (0.0958)  loss_giou_1_unscaled: 0.4427 (0.4343)  cardinality_error_1_unscaled: 291.4167 (291.6791)  loss_ce_2_unscaled: 0.8986 (0.8988)  loss_bbox_2_unscaled: 0.0966 (0.0941)  loss_giou_2_unscaled: 0.4361 (0.4235)  cardinality_error_2_unscaled: 291.3333 (291.7232)  loss_ce_3_unscaled: 0.9055 (0.9055)  loss_bbox_3_unscaled: 0.0949 (0.0937)  loss_giou_3_unscaled: 0.4400 (0.4204)  cardinality_error_3_unscaled: 290.5833 (291.7681)  loss_ce_4_unscaled: 0.9034 (0.9062)  loss_bbox_4_unscaled: 0.0938 (0.0936)  loss_giou_4_unscaled: 0.4361 (0.4203)  cardinality_error_4_unscaled: 290.7500 (291.6569)  time: 3.0259  data: 0.0303  max mem: 8138\n",
            "Test:  [230/417]  eta: 0:09:26  class_error: 60.00  loss: 18.4736 (18.9353)  loss_ce: 1.6583 (1.8108)  loss_bbox: 0.4368 (0.4667)  loss_giou: 0.8591 (0.8409)  loss_ce_0: 1.7182 (1.8296)  loss_bbox_0: 0.5012 (0.5295)  loss_giou_0: 1.0033 (0.9640)  loss_ce_1: 1.6851 (1.8009)  loss_bbox_1: 0.4508 (0.4774)  loss_giou_1: 0.8912 (0.8686)  loss_ce_2: 1.6724 (1.7958)  loss_bbox_2: 0.4383 (0.4685)  loss_giou_2: 0.8706 (0.8473)  loss_ce_3: 1.6518 (1.8092)  loss_bbox_3: 0.4396 (0.4668)  loss_giou_3: 0.8719 (0.8413)  loss_ce_4: 1.6733 (1.8114)  loss_bbox_4: 0.4352 (0.4659)  loss_giou_4: 0.8634 (0.8406)  loss_ce_unscaled: 0.8291 (0.9054)  class_error_unscaled: 55.5556 (60.1620)  loss_bbox_unscaled: 0.0874 (0.0933)  loss_giou_unscaled: 0.4295 (0.4204)  cardinality_error_unscaled: 290.8333 (291.5249)  loss_ce_0_unscaled: 0.8591 (0.9148)  loss_bbox_0_unscaled: 0.1002 (0.1059)  loss_giou_0_unscaled: 0.5016 (0.4820)  cardinality_error_0_unscaled: 291.0833 (291.8406)  loss_ce_1_unscaled: 0.8426 (0.9005)  loss_bbox_1_unscaled: 0.0902 (0.0955)  loss_giou_1_unscaled: 0.4456 (0.4343)  cardinality_error_1_unscaled: 291.5833 (291.6652)  loss_ce_2_unscaled: 0.8362 (0.8979)  loss_bbox_2_unscaled: 0.0877 (0.0937)  loss_giou_2_unscaled: 0.4353 (0.4237)  cardinality_error_2_unscaled: 291.3333 (291.7190)  loss_ce_3_unscaled: 0.8259 (0.9046)  loss_bbox_3_unscaled: 0.0879 (0.0934)  loss_giou_3_unscaled: 0.4360 (0.4206)  cardinality_error_3_unscaled: 291.0833 (291.7691)  loss_ce_4_unscaled: 0.8367 (0.9057)  loss_bbox_4_unscaled: 0.0870 (0.0932)  loss_giou_4_unscaled: 0.4317 (0.4203)  cardinality_error_4_unscaled: 291.0833 (291.6436)  time: 2.9896  data: 0.0302  max mem: 8138\n",
            "Test:  [240/417]  eta: 0:08:56  class_error: 71.84  loss: 17.9049 (18.8955)  loss_ce: 1.7539 (1.8049)  loss_bbox: 0.4419 (0.4674)  loss_giou: 0.7762 (0.8395)  loss_ce_0: 1.7730 (1.8239)  loss_bbox_0: 0.5001 (0.5294)  loss_giou_0: 0.9286 (0.9626)  loss_ce_1: 1.7265 (1.7948)  loss_bbox_1: 0.4527 (0.4779)  loss_giou_1: 0.8511 (0.8677)  loss_ce_2: 1.7599 (1.7896)  loss_bbox_2: 0.4385 (0.4692)  loss_giou_2: 0.8213 (0.8465)  loss_ce_3: 1.7584 (1.8033)  loss_bbox_3: 0.4572 (0.4674)  loss_giou_3: 0.7805 (0.8400)  loss_ce_4: 1.7551 (1.8055)  loss_bbox_4: 0.4465 (0.4665)  loss_giou_4: 0.7784 (0.8394)  loss_ce_unscaled: 0.8769 (0.9024)  class_error_unscaled: 58.7629 (60.0251)  loss_bbox_unscaled: 0.0884 (0.0935)  loss_giou_unscaled: 0.3881 (0.4198)  cardinality_error_unscaled: 291.4167 (291.5297)  loss_ce_0_unscaled: 0.8865 (0.9119)  loss_bbox_0_unscaled: 0.1000 (0.1059)  loss_giou_0_unscaled: 0.4643 (0.4813)  cardinality_error_0_unscaled: 291.8333 (291.8399)  loss_ce_1_unscaled: 0.8633 (0.8974)  loss_bbox_1_unscaled: 0.0905 (0.0956)  loss_giou_1_unscaled: 0.4256 (0.4338)  cardinality_error_1_unscaled: 291.6667 (291.6726)  loss_ce_2_unscaled: 0.8799 (0.8948)  loss_bbox_2_unscaled: 0.0877 (0.0938)  loss_giou_2_unscaled: 0.4106 (0.4233)  cardinality_error_2_unscaled: 291.8333 (291.7244)  loss_ce_3_unscaled: 0.8792 (0.9017)  loss_bbox_3_unscaled: 0.0914 (0.0935)  loss_giou_3_unscaled: 0.3903 (0.4200)  cardinality_error_3_unscaled: 291.6667 (291.7777)  loss_ce_4_unscaled: 0.8775 (0.9028)  loss_bbox_4_unscaled: 0.0893 (0.0933)  loss_giou_4_unscaled: 0.3892 (0.4197)  cardinality_error_4_unscaled: 291.8333 (291.6508)  time: 2.9835  data: 0.0304  max mem: 8138\n",
            "Test:  [250/417]  eta: 0:08:26  class_error: 33.96  loss: 17.0285 (18.8351)  loss_ce: 1.6352 (1.7995)  loss_bbox: 0.4208 (0.4646)  loss_giou: 0.7762 (0.8377)  loss_ce_0: 1.6405 (1.8177)  loss_bbox_0: 0.4913 (0.5269)  loss_giou_0: 0.9286 (0.9609)  loss_ce_1: 1.5782 (1.7891)  loss_bbox_1: 0.4310 (0.4753)  loss_giou_1: 0.8511 (0.8659)  loss_ce_2: 1.5293 (1.7843)  loss_bbox_2: 0.4341 (0.4665)  loss_giou_2: 0.8213 (0.8447)  loss_ce_3: 1.6297 (1.7979)  loss_bbox_3: 0.4281 (0.4647)  loss_giou_3: 0.7805 (0.8382)  loss_ce_4: 1.6430 (1.8002)  loss_bbox_4: 0.4207 (0.4637)  loss_giou_4: 0.7784 (0.8374)  loss_ce_unscaled: 0.8176 (0.8997)  class_error_unscaled: 55.3191 (59.8901)  loss_bbox_unscaled: 0.0842 (0.0929)  loss_giou_unscaled: 0.3881 (0.4189)  cardinality_error_unscaled: 291.7500 (291.5677)  loss_ce_0_unscaled: 0.8203 (0.9088)  loss_bbox_0_unscaled: 0.0983 (0.1054)  loss_giou_0_unscaled: 0.4643 (0.4805)  cardinality_error_0_unscaled: 292.0000 (291.8606)  loss_ce_1_unscaled: 0.7891 (0.8945)  loss_bbox_1_unscaled: 0.0862 (0.0951)  loss_giou_1_unscaled: 0.4256 (0.4329)  cardinality_error_1_unscaled: 291.8333 (291.7108)  loss_ce_2_unscaled: 0.7646 (0.8921)  loss_bbox_2_unscaled: 0.0868 (0.0933)  loss_giou_2_unscaled: 0.4106 (0.4224)  cardinality_error_2_unscaled: 291.8333 (291.7616)  loss_ce_3_unscaled: 0.8148 (0.8989)  loss_bbox_3_unscaled: 0.0856 (0.0929)  loss_giou_3_unscaled: 0.3903 (0.4191)  cardinality_error_3_unscaled: 291.6667 (291.8151)  loss_ce_4_unscaled: 0.8215 (0.9001)  loss_bbox_4_unscaled: 0.0841 (0.0927)  loss_giou_4_unscaled: 0.3892 (0.4187)  cardinality_error_4_unscaled: 291.9167 (291.6932)  time: 3.0801  data: 0.0311  max mem: 8138\n",
            "Test:  [260/417]  eta: 0:07:55  class_error: 82.14  loss: 17.2415 (18.8565)  loss_ce: 1.8009 (1.8019)  loss_bbox: 0.4208 (0.4652)  loss_giou: 0.7376 (0.8368)  loss_ce_0: 1.8831 (1.8230)  loss_bbox_0: 0.4927 (0.5280)  loss_giou_0: 0.8935 (0.9602)  loss_ce_1: 1.7867 (1.7933)  loss_bbox_1: 0.4457 (0.4762)  loss_giou_1: 0.7819 (0.8652)  loss_ce_2: 1.8163 (1.7880)  loss_bbox_2: 0.4341 (0.4673)  loss_giou_2: 0.7548 (0.8442)  loss_ce_3: 1.8391 (1.8005)  loss_bbox_3: 0.4281 (0.4655)  loss_giou_3: 0.7432 (0.8374)  loss_ce_4: 1.8076 (1.8029)  loss_bbox_4: 0.4207 (0.4642)  loss_giou_4: 0.7392 (0.8366)  loss_ce_unscaled: 0.9004 (0.9010)  class_error_unscaled: 60.3774 (60.0527)  loss_bbox_unscaled: 0.0842 (0.0930)  loss_giou_unscaled: 0.3688 (0.4184)  cardinality_error_unscaled: 292.6667 (291.5872)  loss_ce_0_unscaled: 0.9416 (0.9115)  loss_bbox_0_unscaled: 0.0985 (0.1056)  loss_giou_0_unscaled: 0.4468 (0.4801)  cardinality_error_0_unscaled: 293.1667 (291.8819)  loss_ce_1_unscaled: 0.8933 (0.8967)  loss_bbox_1_unscaled: 0.0891 (0.0952)  loss_giou_1_unscaled: 0.3910 (0.4326)  cardinality_error_1_unscaled: 293.1667 (291.7404)  loss_ce_2_unscaled: 0.9082 (0.8940)  loss_bbox_2_unscaled: 0.0868 (0.0935)  loss_giou_2_unscaled: 0.3774 (0.4221)  cardinality_error_2_unscaled: 293.3333 (291.7941)  loss_ce_3_unscaled: 0.9195 (0.9002)  loss_bbox_3_unscaled: 0.0856 (0.0931)  loss_giou_3_unscaled: 0.3716 (0.4187)  cardinality_error_3_unscaled: 293.2500 (291.8496)  loss_ce_4_unscaled: 0.9038 (0.9014)  loss_bbox_4_unscaled: 0.0841 (0.0928)  loss_giou_4_unscaled: 0.3696 (0.4183)  cardinality_error_4_unscaled: 292.1667 (291.7133)  time: 3.0497  data: 0.0312  max mem: 8138\n",
            "Test:  [270/417]  eta: 0:07:26  class_error: 42.31  loss: 17.2415 (18.7770)  loss_ce: 1.7523 (1.7933)  loss_bbox: 0.4602 (0.4642)  loss_giou: 0.7255 (0.8329)  loss_ce_0: 1.7271 (1.8150)  loss_bbox_0: 0.5311 (0.5269)  loss_giou_0: 0.8187 (0.9566)  loss_ce_1: 1.7164 (1.7855)  loss_bbox_1: 0.4689 (0.4750)  loss_giou_1: 0.7442 (0.8610)  loss_ce_2: 1.6818 (1.7795)  loss_bbox_2: 0.4565 (0.4662)  loss_giou_2: 0.7420 (0.8402)  loss_ce_3: 1.7415 (1.7924)  loss_bbox_3: 0.4554 (0.4643)  loss_giou_3: 0.7247 (0.8334)  loss_ce_4: 1.7613 (1.7947)  loss_bbox_4: 0.4541 (0.4632)  loss_giou_4: 0.7182 (0.8326)  loss_ce_unscaled: 0.8762 (0.8967)  class_error_unscaled: 59.4828 (59.7120)  loss_bbox_unscaled: 0.0920 (0.0928)  loss_giou_unscaled: 0.3627 (0.4165)  cardinality_error_unscaled: 292.0833 (291.5440)  loss_ce_0_unscaled: 0.8636 (0.9075)  loss_bbox_0_unscaled: 0.1062 (0.1054)  loss_giou_0_unscaled: 0.4093 (0.4783)  cardinality_error_0_unscaled: 292.0833 (291.8373)  loss_ce_1_unscaled: 0.8582 (0.8928)  loss_bbox_1_unscaled: 0.0938 (0.0950)  loss_giou_1_unscaled: 0.3721 (0.4305)  cardinality_error_1_unscaled: 292.0833 (291.6857)  loss_ce_2_unscaled: 0.8409 (0.8898)  loss_bbox_2_unscaled: 0.0913 (0.0932)  loss_giou_2_unscaled: 0.3710 (0.4201)  cardinality_error_2_unscaled: 292.0833 (291.7682)  loss_ce_3_unscaled: 0.8707 (0.8962)  loss_bbox_3_unscaled: 0.0911 (0.0929)  loss_giou_3_unscaled: 0.3624 (0.4167)  cardinality_error_3_unscaled: 292.0000 (291.8084)  loss_ce_4_unscaled: 0.8807 (0.8974)  loss_bbox_4_unscaled: 0.0908 (0.0926)  loss_giou_4_unscaled: 0.3591 (0.4163)  cardinality_error_4_unscaled: 292.0000 (291.6784)  time: 3.1291  data: 0.0312  max mem: 8138\n",
            "Test:  [280/417]  eta: 0:06:55  class_error: 61.19  loss: 17.0275 (18.7695)  loss_ce: 1.5247 (1.7899)  loss_bbox: 0.4547 (0.4645)  loss_giou: 0.8034 (0.8350)  loss_ce_0: 1.5460 (1.8117)  loss_bbox_0: 0.5083 (0.5265)  loss_giou_0: 0.9539 (0.9584)  loss_ce_1: 1.5424 (1.7821)  loss_bbox_1: 0.4560 (0.4749)  loss_giou_1: 0.8398 (0.8630)  loss_ce_2: 1.5622 (1.7760)  loss_bbox_2: 0.4528 (0.4664)  loss_giou_2: 0.8268 (0.8424)  loss_ce_3: 1.5536 (1.7892)  loss_bbox_3: 0.4565 (0.4645)  loss_giou_3: 0.8226 (0.8355)  loss_ce_4: 1.5474 (1.7915)  loss_bbox_4: 0.4473 (0.4633)  loss_giou_4: 0.8026 (0.8347)  loss_ce_unscaled: 0.7624 (0.8949)  class_error_unscaled: 53.5433 (59.5605)  loss_bbox_unscaled: 0.0909 (0.0929)  loss_giou_unscaled: 0.4017 (0.4175)  cardinality_error_unscaled: 290.8333 (291.5051)  loss_ce_0_unscaled: 0.7730 (0.9059)  loss_bbox_0_unscaled: 0.1017 (0.1053)  loss_giou_0_unscaled: 0.4770 (0.4792)  cardinality_error_0_unscaled: 290.7500 (291.7897)  loss_ce_1_unscaled: 0.7712 (0.8911)  loss_bbox_1_unscaled: 0.0912 (0.0950)  loss_giou_1_unscaled: 0.4199 (0.4315)  cardinality_error_1_unscaled: 290.5000 (291.6424)  loss_ce_2_unscaled: 0.7811 (0.8880)  loss_bbox_2_unscaled: 0.0906 (0.0933)  loss_giou_2_unscaled: 0.4134 (0.4212)  cardinality_error_2_unscaled: 290.9167 (291.7284)  loss_ce_3_unscaled: 0.7768 (0.8946)  loss_bbox_3_unscaled: 0.0913 (0.0929)  loss_giou_3_unscaled: 0.4113 (0.4177)  cardinality_error_3_unscaled: 290.8333 (291.7660)  loss_ce_4_unscaled: 0.7737 (0.8958)  loss_bbox_4_unscaled: 0.0895 (0.0927)  loss_giou_4_unscaled: 0.4013 (0.4173)  cardinality_error_4_unscaled: 290.8333 (291.6415)  time: 3.0828  data: 0.0308  max mem: 8138\n",
            "Test:  [290/417]  eta: 0:06:25  class_error: 45.45  loss: 18.3193 (18.7973)  loss_ce: 1.7660 (1.7924)  loss_bbox: 0.4539 (0.4657)  loss_giou: 0.8632 (0.8355)  loss_ce_0: 1.7526 (1.8156)  loss_bbox_0: 0.5142 (0.5278)  loss_giou_0: 0.9814 (0.9586)  loss_ce_1: 1.7467 (1.7853)  loss_bbox_1: 0.4574 (0.4765)  loss_giou_1: 0.8858 (0.8636)  loss_ce_2: 1.7827 (1.7788)  loss_bbox_2: 0.4572 (0.4677)  loss_giou_2: 0.8645 (0.8430)  loss_ce_3: 1.7675 (1.7915)  loss_bbox_3: 0.4565 (0.4656)  loss_giou_3: 0.8634 (0.8360)  loss_ce_4: 1.7619 (1.7942)  loss_bbox_4: 0.4492 (0.4645)  loss_giou_4: 0.8608 (0.8351)  loss_ce_unscaled: 0.8830 (0.8962)  class_error_unscaled: 56.2500 (59.5838)  loss_bbox_unscaled: 0.0908 (0.0931)  loss_giou_unscaled: 0.4316 (0.4178)  cardinality_error_unscaled: 291.1667 (291.5421)  loss_ce_0_unscaled: 0.8763 (0.9078)  loss_bbox_0_unscaled: 0.1028 (0.1056)  loss_giou_0_unscaled: 0.4907 (0.4793)  cardinality_error_0_unscaled: 290.9167 (291.8299)  loss_ce_1_unscaled: 0.8733 (0.8926)  loss_bbox_1_unscaled: 0.0915 (0.0953)  loss_giou_1_unscaled: 0.4429 (0.4318)  cardinality_error_1_unscaled: 290.7500 (291.6684)  loss_ce_2_unscaled: 0.8913 (0.8894)  loss_bbox_2_unscaled: 0.0914 (0.0935)  loss_giou_2_unscaled: 0.4322 (0.4215)  cardinality_error_2_unscaled: 291.2500 (291.7658)  loss_ce_3_unscaled: 0.8837 (0.8957)  loss_bbox_3_unscaled: 0.0913 (0.0931)  loss_giou_3_unscaled: 0.4317 (0.4180)  cardinality_error_3_unscaled: 291.0833 (291.7993)  loss_ce_4_unscaled: 0.8809 (0.8971)  loss_bbox_4_unscaled: 0.0898 (0.0929)  loss_giou_4_unscaled: 0.4304 (0.4176)  cardinality_error_4_unscaled: 291.1667 (291.6804)  time: 2.9510  data: 0.0315  max mem: 8138\n",
            "Test:  [300/417]  eta: 0:05:54  class_error: 51.92  loss: 18.5625 (18.8181)  loss_ce: 1.8015 (1.7939)  loss_bbox: 0.4539 (0.4661)  loss_giou: 0.8457 (0.8375)  loss_ce_0: 1.8750 (1.8175)  loss_bbox_0: 0.5142 (0.5277)  loss_giou_0: 0.9814 (0.9597)  loss_ce_1: 1.8284 (1.7863)  loss_bbox_1: 0.4737 (0.4771)  loss_giou_1: 0.8856 (0.8653)  loss_ce_2: 1.8347 (1.7800)  loss_bbox_2: 0.4644 (0.4683)  loss_giou_2: 0.8603 (0.8447)  loss_ce_3: 1.8377 (1.7927)  loss_bbox_3: 0.4551 (0.4661)  loss_giou_3: 0.8506 (0.8379)  loss_ce_4: 1.8276 (1.7951)  loss_bbox_4: 0.4574 (0.4650)  loss_giou_4: 0.8469 (0.8372)  loss_ce_unscaled: 0.9007 (0.8969)  class_error_unscaled: 58.6538 (59.6506)  loss_bbox_unscaled: 0.0908 (0.0932)  loss_giou_unscaled: 0.4229 (0.4187)  cardinality_error_unscaled: 291.4167 (291.5302)  loss_ce_0_unscaled: 0.9375 (0.9088)  loss_bbox_0_unscaled: 0.1028 (0.1055)  loss_giou_0_unscaled: 0.4907 (0.4798)  cardinality_error_0_unscaled: 292.0000 (291.8245)  loss_ce_1_unscaled: 0.9142 (0.8931)  loss_bbox_1_unscaled: 0.0947 (0.0954)  loss_giou_1_unscaled: 0.4428 (0.4327)  cardinality_error_1_unscaled: 291.5833 (291.6495)  loss_ce_2_unscaled: 0.9174 (0.8900)  loss_bbox_2_unscaled: 0.0929 (0.0937)  loss_giou_2_unscaled: 0.4301 (0.4224)  cardinality_error_2_unscaled: 291.5833 (291.7555)  loss_ce_3_unscaled: 0.9189 (0.8964)  loss_bbox_3_unscaled: 0.0910 (0.0932)  loss_giou_3_unscaled: 0.4253 (0.4190)  cardinality_error_3_unscaled: 291.5833 (291.7874)  loss_ce_4_unscaled: 0.9138 (0.8976)  loss_bbox_4_unscaled: 0.0915 (0.0930)  loss_giou_4_unscaled: 0.4235 (0.4186)  cardinality_error_4_unscaled: 291.9167 (291.6742)  time: 2.9753  data: 0.0314  max mem: 8138\n",
            "Test:  [310/417]  eta: 0:05:24  class_error: 83.70  loss: 18.4772 (18.8069)  loss_ce: 1.7786 (1.7933)  loss_bbox: 0.4494 (0.4652)  loss_giou: 0.8303 (0.8369)  loss_ce_0: 1.7989 (1.8174)  loss_bbox_0: 0.4997 (0.5268)  loss_giou_0: 0.9760 (0.9592)  loss_ce_1: 1.7463 (1.7861)  loss_bbox_1: 0.4702 (0.4761)  loss_giou_1: 0.8804 (0.8646)  loss_ce_2: 1.7612 (1.7797)  loss_bbox_2: 0.4644 (0.4673)  loss_giou_2: 0.8488 (0.8442)  loss_ce_3: 1.7855 (1.7924)  loss_bbox_3: 0.4545 (0.4652)  loss_giou_3: 0.8277 (0.8373)  loss_ce_4: 1.7600 (1.7947)  loss_bbox_4: 0.4563 (0.4641)  loss_giou_4: 0.8271 (0.8365)  loss_ce_unscaled: 0.8893 (0.8966)  class_error_unscaled: 60.0000 (59.6826)  loss_bbox_unscaled: 0.0899 (0.0930)  loss_giou_unscaled: 0.4151 (0.4184)  cardinality_error_unscaled: 292.0833 (291.5801)  loss_ce_0_unscaled: 0.8994 (0.9087)  loss_bbox_0_unscaled: 0.0999 (0.1054)  loss_giou_0_unscaled: 0.4880 (0.4796)  cardinality_error_0_unscaled: 292.0000 (291.8679)  loss_ce_1_unscaled: 0.8731 (0.8930)  loss_bbox_1_unscaled: 0.0940 (0.0952)  loss_giou_1_unscaled: 0.4402 (0.4323)  cardinality_error_1_unscaled: 291.9167 (291.6996)  loss_ce_2_unscaled: 0.8806 (0.8898)  loss_bbox_2_unscaled: 0.0929 (0.0935)  loss_giou_2_unscaled: 0.4244 (0.4221)  cardinality_error_2_unscaled: 292.3333 (291.8020)  loss_ce_3_unscaled: 0.8927 (0.8962)  loss_bbox_3_unscaled: 0.0909 (0.0930)  loss_giou_3_unscaled: 0.4138 (0.4186)  cardinality_error_3_unscaled: 292.2500 (291.8274)  loss_ce_4_unscaled: 0.8800 (0.8973)  loss_bbox_4_unscaled: 0.0913 (0.0928)  loss_giou_4_unscaled: 0.4136 (0.4183)  cardinality_error_4_unscaled: 292.0000 (291.7179)  time: 3.0143  data: 0.0317  max mem: 8138\n",
            "Test:  [320/417]  eta: 0:04:53  class_error: 40.30  loss: 18.2360 (18.7961)  loss_ce: 1.7451 (1.7929)  loss_bbox: 0.4372 (0.4644)  loss_giou: 0.8303 (0.8364)  loss_ce_0: 1.7841 (1.8166)  loss_bbox_0: 0.5063 (0.5260)  loss_giou_0: 0.9760 (0.9593)  loss_ce_1: 1.7463 (1.7854)  loss_bbox_1: 0.4454 (0.4752)  loss_giou_1: 0.8557 (0.8639)  loss_ce_2: 1.7026 (1.7793)  loss_bbox_2: 0.4473 (0.4664)  loss_giou_2: 0.8213 (0.8436)  loss_ce_3: 1.7407 (1.7921)  loss_bbox_3: 0.4356 (0.4644)  loss_giou_3: 0.8277 (0.8367)  loss_ce_4: 1.7600 (1.7944)  loss_bbox_4: 0.4373 (0.4633)  loss_giou_4: 0.8271 (0.8360)  loss_ce_unscaled: 0.8725 (0.8964)  class_error_unscaled: 60.4396 (59.6329)  loss_bbox_unscaled: 0.0874 (0.0929)  loss_giou_unscaled: 0.4151 (0.4182)  cardinality_error_unscaled: 292.2500 (291.5846)  loss_ce_0_unscaled: 0.8921 (0.9083)  loss_bbox_0_unscaled: 0.1013 (0.1052)  loss_giou_0_unscaled: 0.4880 (0.4796)  cardinality_error_0_unscaled: 292.3333 (291.8624)  loss_ce_1_unscaled: 0.8731 (0.8927)  loss_bbox_1_unscaled: 0.0891 (0.0950)  loss_giou_1_unscaled: 0.4278 (0.4319)  cardinality_error_1_unscaled: 292.4167 (291.6994)  loss_ce_2_unscaled: 0.8513 (0.8896)  loss_bbox_2_unscaled: 0.0895 (0.0933)  loss_giou_2_unscaled: 0.4107 (0.4218)  cardinality_error_2_unscaled: 292.4167 (291.8066)  loss_ce_3_unscaled: 0.8704 (0.8961)  loss_bbox_3_unscaled: 0.0871 (0.0929)  loss_giou_3_unscaled: 0.4138 (0.4183)  cardinality_error_3_unscaled: 292.2500 (291.8284)  loss_ce_4_unscaled: 0.8800 (0.8972)  loss_bbox_4_unscaled: 0.0875 (0.0927)  loss_giou_4_unscaled: 0.4136 (0.4180)  cardinality_error_4_unscaled: 292.4167 (291.7222)  time: 2.9975  data: 0.0323  max mem: 8138\n",
            "Test:  [330/417]  eta: 0:04:22  class_error: 64.20  loss: 18.5989 (18.7900)  loss_ce: 1.7451 (1.7907)  loss_bbox: 0.4603 (0.4659)  loss_giou: 0.8345 (0.8355)  loss_ce_0: 1.7841 (1.8149)  loss_bbox_0: 0.5114 (0.5279)  loss_giou_0: 0.9807 (0.9582)  loss_ce_1: 1.7495 (1.7843)  loss_bbox_1: 0.4638 (0.4766)  loss_giou_1: 0.8557 (0.8628)  loss_ce_2: 1.6842 (1.7783)  loss_bbox_2: 0.4616 (0.4679)  loss_giou_2: 0.8213 (0.8426)  loss_ce_3: 1.7407 (1.7901)  loss_bbox_3: 0.4616 (0.4659)  loss_giou_3: 0.8359 (0.8358)  loss_ce_4: 1.7776 (1.7926)  loss_bbox_4: 0.4603 (0.4648)  loss_giou_4: 0.8352 (0.8351)  loss_ce_unscaled: 0.8725 (0.8953)  class_error_unscaled: 59.8214 (59.5608)  loss_bbox_unscaled: 0.0921 (0.0932)  loss_giou_unscaled: 0.4172 (0.4178)  cardinality_error_unscaled: 292.3333 (291.6236)  loss_ce_0_unscaled: 0.8921 (0.9075)  loss_bbox_0_unscaled: 0.1023 (0.1056)  loss_giou_0_unscaled: 0.4903 (0.4791)  cardinality_error_0_unscaled: 293.0000 (291.8983)  loss_ce_1_unscaled: 0.8747 (0.8922)  loss_bbox_1_unscaled: 0.0928 (0.0953)  loss_giou_1_unscaled: 0.4278 (0.4314)  cardinality_error_1_unscaled: 292.7500 (291.7354)  loss_ce_2_unscaled: 0.8421 (0.8892)  loss_bbox_2_unscaled: 0.0923 (0.0936)  loss_giou_2_unscaled: 0.4107 (0.4213)  cardinality_error_2_unscaled: 292.9167 (291.8391)  loss_ce_3_unscaled: 0.8704 (0.8950)  loss_bbox_3_unscaled: 0.0923 (0.0932)  loss_giou_3_unscaled: 0.4179 (0.4179)  cardinality_error_3_unscaled: 292.8333 (291.8618)  loss_ce_4_unscaled: 0.8888 (0.8963)  loss_bbox_4_unscaled: 0.0921 (0.0930)  loss_giou_4_unscaled: 0.4176 (0.4176)  cardinality_error_4_unscaled: 292.7500 (291.7578)  time: 2.8790  data: 0.0316  max mem: 8138\n",
            "Test:  [340/417]  eta: 0:03:52  class_error: 32.11  loss: 17.5147 (18.7516)  loss_ce: 1.5764 (1.7851)  loss_bbox: 0.4743 (0.4660)  loss_giou: 0.8642 (0.8347)  loss_ce_0: 1.5535 (1.8095)  loss_bbox_0: 0.5767 (0.5282)  loss_giou_0: 0.9807 (0.9577)  loss_ce_1: 1.5454 (1.7781)  loss_bbox_1: 0.4959 (0.4768)  loss_giou_1: 0.8809 (0.8622)  loss_ce_2: 1.5465 (1.7725)  loss_bbox_2: 0.4750 (0.4680)  loss_giou_2: 0.8765 (0.8419)  loss_ce_3: 1.5484 (1.7839)  loss_bbox_3: 0.4883 (0.4661)  loss_giou_3: 0.8633 (0.8349)  loss_ce_4: 1.5679 (1.7867)  loss_bbox_4: 0.4748 (0.4649)  loss_giou_4: 0.8609 (0.8343)  loss_ce_unscaled: 0.7882 (0.8925)  class_error_unscaled: 53.7634 (59.3544)  loss_bbox_unscaled: 0.0949 (0.0932)  loss_giou_unscaled: 0.4321 (0.4174)  cardinality_error_unscaled: 291.4167 (291.5936)  loss_ce_0_unscaled: 0.7768 (0.9048)  loss_bbox_0_unscaled: 0.1153 (0.1056)  loss_giou_0_unscaled: 0.4903 (0.4789)  cardinality_error_0_unscaled: 292.0833 (291.8685)  loss_ce_1_unscaled: 0.7727 (0.8891)  loss_bbox_1_unscaled: 0.0992 (0.0954)  loss_giou_1_unscaled: 0.4404 (0.4311)  cardinality_error_1_unscaled: 291.4167 (291.7070)  loss_ce_2_unscaled: 0.7732 (0.8862)  loss_bbox_2_unscaled: 0.0950 (0.0936)  loss_giou_2_unscaled: 0.4382 (0.4209)  cardinality_error_2_unscaled: 291.2500 (291.8065)  loss_ce_3_unscaled: 0.7742 (0.8919)  loss_bbox_3_unscaled: 0.0977 (0.0932)  loss_giou_3_unscaled: 0.4316 (0.4175)  cardinality_error_3_unscaled: 291.5833 (291.8272)  loss_ce_4_unscaled: 0.7839 (0.8933)  loss_bbox_4_unscaled: 0.0950 (0.0930)  loss_giou_4_unscaled: 0.4304 (0.4171)  cardinality_error_4_unscaled: 291.2500 (291.7258)  time: 2.8561  data: 0.0302  max mem: 8138\n",
            "Test:  [350/417]  eta: 0:03:22  class_error: 40.35  loss: 15.9770 (18.6910)  loss_ce: 1.4435 (1.7773)  loss_bbox: 0.4245 (0.4650)  loss_giou: 0.8414 (0.8335)  loss_ce_0: 1.4414 (1.8008)  loss_bbox_0: 0.4849 (0.5272)  loss_giou_0: 0.9607 (0.9571)  loss_ce_1: 1.4070 (1.7700)  loss_bbox_1: 0.4314 (0.4757)  loss_giou_1: 0.8692 (0.8609)  loss_ce_2: 1.4377 (1.7646)  loss_bbox_2: 0.4313 (0.4670)  loss_giou_2: 0.8484 (0.8406)  loss_ce_3: 1.4309 (1.7763)  loss_bbox_3: 0.4259 (0.4651)  loss_giou_3: 0.8409 (0.8339)  loss_ce_4: 1.4454 (1.7791)  loss_bbox_4: 0.4236 (0.4639)  loss_giou_4: 0.8375 (0.8331)  loss_ce_unscaled: 0.7218 (0.8886)  class_error_unscaled: 48.9583 (59.0916)  loss_bbox_unscaled: 0.0849 (0.0930)  loss_giou_unscaled: 0.4207 (0.4167)  cardinality_error_unscaled: 290.9167 (291.6040)  loss_ce_0_unscaled: 0.7207 (0.9004)  loss_bbox_0_unscaled: 0.0970 (0.1054)  loss_giou_0_unscaled: 0.4803 (0.4785)  cardinality_error_0_unscaled: 291.0833 (291.8706)  loss_ce_1_unscaled: 0.7035 (0.8850)  loss_bbox_1_unscaled: 0.0863 (0.0951)  loss_giou_1_unscaled: 0.4346 (0.4305)  cardinality_error_1_unscaled: 291.1667 (291.7130)  loss_ce_2_unscaled: 0.7188 (0.8823)  loss_bbox_2_unscaled: 0.0863 (0.0934)  loss_giou_2_unscaled: 0.4242 (0.4203)  cardinality_error_2_unscaled: 291.2500 (291.8127)  loss_ce_3_unscaled: 0.7154 (0.8881)  loss_bbox_3_unscaled: 0.0852 (0.0930)  loss_giou_3_unscaled: 0.4205 (0.4169)  cardinality_error_3_unscaled: 291.2500 (291.8319)  loss_ce_4_unscaled: 0.7227 (0.8895)  loss_bbox_4_unscaled: 0.0847 (0.0928)  loss_giou_4_unscaled: 0.4187 (0.4166)  cardinality_error_4_unscaled: 291.0833 (291.7343)  time: 2.9373  data: 0.0293  max mem: 8138\n",
            "Test:  [360/417]  eta: 0:02:52  class_error: 63.54  loss: 16.9795 (18.6786)  loss_ce: 1.5913 (1.7743)  loss_bbox: 0.4505 (0.4654)  loss_giou: 0.8414 (0.8336)  loss_ce_0: 1.5830 (1.7986)  loss_bbox_0: 0.4966 (0.5276)  loss_giou_0: 0.9607 (0.9572)  loss_ce_1: 1.5351 (1.7675)  loss_bbox_1: 0.4703 (0.4761)  loss_giou_1: 0.8692 (0.8613)  loss_ce_2: 1.5703 (1.7620)  loss_bbox_2: 0.4474 (0.4673)  loss_giou_2: 0.8484 (0.8409)  loss_ce_3: 1.5946 (1.7736)  loss_bbox_3: 0.4533 (0.4654)  loss_giou_3: 0.8409 (0.8340)  loss_ce_4: 1.5831 (1.7763)  loss_bbox_4: 0.4401 (0.4643)  loss_giou_4: 0.8375 (0.8333)  loss_ce_unscaled: 0.7957 (0.8872)  class_error_unscaled: 50.9434 (59.0046)  loss_bbox_unscaled: 0.0901 (0.0931)  loss_giou_unscaled: 0.4207 (0.4168)  cardinality_error_unscaled: 292.0000 (291.5863)  loss_ce_0_unscaled: 0.7915 (0.8993)  loss_bbox_0_unscaled: 0.0993 (0.1055)  loss_giou_0_unscaled: 0.4803 (0.4786)  cardinality_error_0_unscaled: 292.1667 (291.8518)  loss_ce_1_unscaled: 0.7675 (0.8837)  loss_bbox_1_unscaled: 0.0941 (0.0952)  loss_giou_1_unscaled: 0.4346 (0.4306)  cardinality_error_1_unscaled: 292.2500 (291.7006)  loss_ce_2_unscaled: 0.7852 (0.8810)  loss_bbox_2_unscaled: 0.0895 (0.0935)  loss_giou_2_unscaled: 0.4242 (0.4204)  cardinality_error_2_unscaled: 292.0000 (291.8001)  loss_ce_3_unscaled: 0.7973 (0.8868)  loss_bbox_3_unscaled: 0.0907 (0.0931)  loss_giou_3_unscaled: 0.4205 (0.4170)  cardinality_error_3_unscaled: 292.0833 (291.8153)  loss_ce_4_unscaled: 0.7916 (0.8882)  loss_bbox_4_unscaled: 0.0880 (0.0929)  loss_giou_4_unscaled: 0.4187 (0.4167)  cardinality_error_4_unscaled: 292.0000 (291.7223)  time: 3.0468  data: 0.0306  max mem: 8138\n",
            "Test:  [370/417]  eta: 0:02:21  class_error: 55.56  loss: 18.8425 (18.7101)  loss_ce: 1.7523 (1.7786)  loss_bbox: 0.4663 (0.4661)  loss_giou: 0.8558 (0.8341)  loss_ce_0: 1.7688 (1.8019)  loss_bbox_0: 0.5393 (0.5285)  loss_giou_0: 0.9947 (0.9578)  loss_ce_1: 1.7301 (1.7716)  loss_bbox_1: 0.4851 (0.4767)  loss_giou_1: 0.8893 (0.8616)  loss_ce_2: 1.7445 (1.7660)  loss_bbox_2: 0.4751 (0.4681)  loss_giou_2: 0.8758 (0.8413)  loss_ce_3: 1.7622 (1.7779)  loss_bbox_3: 0.4639 (0.4661)  loss_giou_3: 0.8505 (0.8345)  loss_ce_4: 1.7734 (1.7806)  loss_bbox_4: 0.4655 (0.4650)  loss_giou_4: 0.8497 (0.8337)  loss_ce_unscaled: 0.8761 (0.8893)  class_error_unscaled: 58.1197 (59.1728)  loss_bbox_unscaled: 0.0933 (0.0932)  loss_giou_unscaled: 0.4279 (0.4170)  cardinality_error_unscaled: 292.1667 (291.6132)  loss_ce_0_unscaled: 0.8844 (0.9010)  loss_bbox_0_unscaled: 0.1079 (0.1057)  loss_giou_0_unscaled: 0.4973 (0.4789)  cardinality_error_0_unscaled: 292.7500 (291.8715)  loss_ce_1_unscaled: 0.8651 (0.8858)  loss_bbox_1_unscaled: 0.0970 (0.0953)  loss_giou_1_unscaled: 0.4447 (0.4308)  cardinality_error_1_unscaled: 292.1667 (291.7206)  loss_ce_2_unscaled: 0.8722 (0.8830)  loss_bbox_2_unscaled: 0.0950 (0.0936)  loss_giou_2_unscaled: 0.4379 (0.4207)  cardinality_error_2_unscaled: 292.3333 (291.8217)  loss_ce_3_unscaled: 0.8811 (0.8889)  loss_bbox_3_unscaled: 0.0928 (0.0932)  loss_giou_3_unscaled: 0.4253 (0.4172)  cardinality_error_3_unscaled: 292.6667 (291.8412)  loss_ce_4_unscaled: 0.8867 (0.8903)  loss_bbox_4_unscaled: 0.0931 (0.0930)  loss_giou_4_unscaled: 0.4249 (0.4169)  cardinality_error_4_unscaled: 292.1667 (291.7399)  time: 3.0323  data: 0.0306  max mem: 8138\n",
            "Test:  [380/417]  eta: 0:01:51  class_error: 29.25  loss: 18.8425 (18.6982)  loss_ce: 1.7837 (1.7772)  loss_bbox: 0.4190 (0.4652)  loss_giou: 0.8328 (0.8346)  loss_ce_0: 1.7713 (1.8003)  loss_bbox_0: 0.5014 (0.5276)  loss_giou_0: 0.9537 (0.9580)  loss_ce_1: 1.7301 (1.7706)  loss_bbox_1: 0.4268 (0.4755)  loss_giou_1: 0.8715 (0.8617)  loss_ce_2: 1.7445 (1.7646)  loss_bbox_2: 0.4277 (0.4672)  loss_giou_2: 0.8343 (0.8416)  loss_ce_3: 1.7666 (1.7763)  loss_bbox_3: 0.4304 (0.4651)  loss_giou_3: 0.8380 (0.8350)  loss_ce_4: 1.7857 (1.7793)  loss_bbox_4: 0.4321 (0.4641)  loss_giou_4: 0.8334 (0.8342)  loss_ce_unscaled: 0.8918 (0.8886)  class_error_unscaled: 61.4286 (59.0770)  loss_bbox_unscaled: 0.0838 (0.0930)  loss_giou_unscaled: 0.4164 (0.4173)  cardinality_error_unscaled: 292.6667 (291.6037)  loss_ce_0_unscaled: 0.8856 (0.9001)  loss_bbox_0_unscaled: 0.1003 (0.1055)  loss_giou_0_unscaled: 0.4769 (0.4790)  cardinality_error_0_unscaled: 292.5000 (291.8552)  loss_ce_1_unscaled: 0.8651 (0.8853)  loss_bbox_1_unscaled: 0.0854 (0.0951)  loss_giou_1_unscaled: 0.4357 (0.4309)  cardinality_error_1_unscaled: 292.5000 (291.7152)  loss_ce_2_unscaled: 0.8722 (0.8823)  loss_bbox_2_unscaled: 0.0855 (0.0934)  loss_giou_2_unscaled: 0.4172 (0.4208)  cardinality_error_2_unscaled: 292.5000 (291.8222)  loss_ce_3_unscaled: 0.8833 (0.8882)  loss_bbox_3_unscaled: 0.0861 (0.0930)  loss_giou_3_unscaled: 0.4190 (0.4175)  cardinality_error_3_unscaled: 292.6667 (291.8397)  loss_ce_4_unscaled: 0.8929 (0.8897)  loss_bbox_4_unscaled: 0.0864 (0.0928)  loss_giou_4_unscaled: 0.4167 (0.4171)  cardinality_error_4_unscaled: 292.5000 (291.7426)  time: 2.9397  data: 0.0291  max mem: 8138\n",
            "Test:  [390/417]  eta: 0:01:21  class_error: 49.43  loss: 18.0527 (18.6786)  loss_ce: 1.6711 (1.7742)  loss_bbox: 0.4028 (0.4649)  loss_giou: 0.8013 (0.8345)  loss_ce_0: 1.6603 (1.7972)  loss_bbox_0: 0.4672 (0.5274)  loss_giou_0: 0.9295 (0.9580)  loss_ce_1: 1.6983 (1.7680)  loss_bbox_1: 0.3954 (0.4750)  loss_giou_1: 0.8484 (0.8618)  loss_ce_2: 1.6724 (1.7620)  loss_bbox_2: 0.4016 (0.4668)  loss_giou_2: 0.8051 (0.8414)  loss_ce_3: 1.6994 (1.7737)  loss_bbox_3: 0.3915 (0.4648)  loss_giou_3: 0.8011 (0.8350)  loss_ce_4: 1.6709 (1.7761)  loss_bbox_4: 0.4036 (0.4638)  loss_giou_4: 0.8044 (0.8342)  loss_ce_unscaled: 0.8356 (0.8871)  class_error_unscaled: 56.9620 (59.0288)  loss_bbox_unscaled: 0.0806 (0.0930)  loss_giou_unscaled: 0.4006 (0.4173)  cardinality_error_unscaled: 292.1667 (291.6079)  loss_ce_0_unscaled: 0.8301 (0.8986)  loss_bbox_0_unscaled: 0.0934 (0.1055)  loss_giou_0_unscaled: 0.4648 (0.4790)  cardinality_error_0_unscaled: 291.6667 (291.8525)  loss_ce_1_unscaled: 0.8491 (0.8840)  loss_bbox_1_unscaled: 0.0791 (0.0950)  loss_giou_1_unscaled: 0.4242 (0.4309)  cardinality_error_1_unscaled: 292.1667 (291.7159)  loss_ce_2_unscaled: 0.8362 (0.8810)  loss_bbox_2_unscaled: 0.0803 (0.0934)  loss_giou_2_unscaled: 0.4025 (0.4207)  cardinality_error_2_unscaled: 292.5000 (291.8261)  loss_ce_3_unscaled: 0.8497 (0.8868)  loss_bbox_3_unscaled: 0.0783 (0.0930)  loss_giou_3_unscaled: 0.4005 (0.4175)  cardinality_error_3_unscaled: 292.1667 (291.8436)  loss_ce_4_unscaled: 0.8354 (0.8881)  loss_bbox_4_unscaled: 0.0807 (0.0928)  loss_giou_4_unscaled: 0.4022 (0.4171)  cardinality_error_4_unscaled: 292.5000 (291.7496)  time: 2.9905  data: 0.0283  max mem: 8138\n",
            "Test:  [400/417]  eta: 0:00:51  class_error: 47.92  loss: 18.0527 (18.6717)  loss_ce: 1.6657 (1.7728)  loss_bbox: 0.4326 (0.4653)  loss_giou: 0.8013 (0.8346)  loss_ce_0: 1.6603 (1.7955)  loss_bbox_0: 0.4675 (0.5276)  loss_giou_0: 0.9629 (0.9580)  loss_ce_1: 1.6772 (1.7662)  loss_bbox_1: 0.4346 (0.4755)  loss_giou_1: 0.8658 (0.8618)  loss_ce_2: 1.6577 (1.7603)  loss_bbox_2: 0.4361 (0.4672)  loss_giou_2: 0.8051 (0.8413)  loss_ce_3: 1.6449 (1.7722)  loss_bbox_3: 0.4334 (0.4651)  loss_giou_3: 0.8011 (0.8350)  loss_ce_4: 1.6992 (1.7745)  loss_bbox_4: 0.4372 (0.4643)  loss_giou_4: 0.8044 (0.8343)  loss_ce_unscaled: 0.8328 (0.8864)  class_error_unscaled: 56.8182 (58.9529)  loss_bbox_unscaled: 0.0865 (0.0931)  loss_giou_unscaled: 0.4006 (0.4173)  cardinality_error_unscaled: 291.8333 (291.6193)  loss_ce_0_unscaled: 0.8301 (0.8978)  loss_bbox_0_unscaled: 0.0935 (0.1055)  loss_giou_0_unscaled: 0.4815 (0.4790)  cardinality_error_0_unscaled: 291.6667 (291.8647)  loss_ce_1_unscaled: 0.8386 (0.8831)  loss_bbox_1_unscaled: 0.0869 (0.0951)  loss_giou_1_unscaled: 0.4329 (0.4309)  cardinality_error_1_unscaled: 291.6667 (291.7284)  loss_ce_2_unscaled: 0.8289 (0.8802)  loss_bbox_2_unscaled: 0.0872 (0.0934)  loss_giou_2_unscaled: 0.4025 (0.4207)  cardinality_error_2_unscaled: 291.8333 (291.8350)  loss_ce_3_unscaled: 0.8224 (0.8861)  loss_bbox_3_unscaled: 0.0867 (0.0930)  loss_giou_3_unscaled: 0.4005 (0.4175)  cardinality_error_3_unscaled: 291.9167 (291.8491)  loss_ce_4_unscaled: 0.8496 (0.8873)  loss_bbox_4_unscaled: 0.0874 (0.0929)  loss_giou_4_unscaled: 0.4022 (0.4172)  cardinality_error_4_unscaled: 291.8333 (291.7594)  time: 2.9792  data: 0.0293  max mem: 8138\n",
            "Test:  [410/417]  eta: 0:00:21  class_error: 51.67  loss: 18.6826 (18.6958)  loss_ce: 1.7369 (1.7749)  loss_bbox: 0.4705 (0.4654)  loss_giou: 0.8877 (0.8365)  loss_ce_0: 1.7814 (1.7969)  loss_bbox_0: 0.5414 (0.5276)  loss_giou_0: 0.9818 (0.9603)  loss_ce_1: 1.7166 (1.7679)  loss_bbox_1: 0.4705 (0.4757)  loss_giou_1: 0.9058 (0.8637)  loss_ce_2: 1.7076 (1.7624)  loss_bbox_2: 0.4725 (0.4674)  loss_giou_2: 0.8918 (0.8433)  loss_ce_3: 1.7214 (1.7743)  loss_bbox_3: 0.4669 (0.4654)  loss_giou_3: 0.8807 (0.8369)  loss_ce_4: 1.7161 (1.7767)  loss_bbox_4: 0.4552 (0.4645)  loss_giou_4: 0.8940 (0.8362)  loss_ce_unscaled: 0.8685 (0.8874)  class_error_unscaled: 56.8182 (59.0295)  loss_bbox_unscaled: 0.0941 (0.0931)  loss_giou_unscaled: 0.4439 (0.4182)  cardinality_error_unscaled: 291.6667 (291.6196)  loss_ce_0_unscaled: 0.8907 (0.8985)  loss_bbox_0_unscaled: 0.1083 (0.1055)  loss_giou_0_unscaled: 0.4909 (0.4801)  cardinality_error_0_unscaled: 291.5000 (291.8613)  loss_ce_1_unscaled: 0.8583 (0.8839)  loss_bbox_1_unscaled: 0.0941 (0.0951)  loss_giou_1_unscaled: 0.4529 (0.4318)  cardinality_error_1_unscaled: 291.5000 (291.7259)  loss_ce_2_unscaled: 0.8538 (0.8812)  loss_bbox_2_unscaled: 0.0945 (0.0935)  loss_giou_2_unscaled: 0.4459 (0.4216)  cardinality_error_2_unscaled: 292.0000 (291.8344)  loss_ce_3_unscaled: 0.8607 (0.8872)  loss_bbox_3_unscaled: 0.0934 (0.0931)  loss_giou_3_unscaled: 0.4404 (0.4184)  cardinality_error_3_unscaled: 292.0000 (291.8500)  loss_ce_4_unscaled: 0.8581 (0.8884)  loss_bbox_4_unscaled: 0.0910 (0.0929)  loss_giou_4_unscaled: 0.4470 (0.4181)  cardinality_error_4_unscaled: 291.6667 (291.7573)  time: 2.8860  data: 0.0296  max mem: 8138\n",
            "Test:  [416/417]  eta: 0:00:03  class_error: 43.90  loss: 18.6397 (18.6679)  loss_ce: 1.7128 (1.7728)  loss_bbox: 0.4598 (0.4645)  loss_giou: 0.8419 (0.8350)  loss_ce_0: 1.7354 (1.7951)  loss_bbox_0: 0.5401 (0.5265)  loss_giou_0: 0.9381 (0.9585)  loss_ce_1: 1.6933 (1.7655)  loss_bbox_1: 0.4705 (0.4748)  loss_giou_1: 0.8591 (0.8620)  loss_ce_2: 1.6969 (1.7602)  loss_bbox_2: 0.4707 (0.4665)  loss_giou_2: 0.8526 (0.8418)  loss_ce_3: 1.6874 (1.7721)  loss_bbox_3: 0.4629 (0.4644)  loss_giou_3: 0.8370 (0.8354)  loss_ce_4: 1.6805 (1.7745)  loss_bbox_4: 0.4552 (0.4635)  loss_giou_4: 0.8430 (0.8348)  loss_ce_unscaled: 0.8564 (0.8864)  class_error_unscaled: 51.6667 (58.9588)  loss_bbox_unscaled: 0.0920 (0.0929)  loss_giou_unscaled: 0.4210 (0.4175)  cardinality_error_unscaled: 291.8333 (291.6430)  loss_ce_0_unscaled: 0.8677 (0.8976)  loss_bbox_0_unscaled: 0.1080 (0.1053)  loss_giou_0_unscaled: 0.4691 (0.4793)  cardinality_error_0_unscaled: 292.3333 (291.8886)  loss_ce_1_unscaled: 0.8467 (0.8827)  loss_bbox_1_unscaled: 0.0941 (0.0950)  loss_giou_1_unscaled: 0.4296 (0.4310)  cardinality_error_1_unscaled: 292.1667 (291.7523)  loss_ce_2_unscaled: 0.8484 (0.8801)  loss_bbox_2_unscaled: 0.0941 (0.0933)  loss_giou_2_unscaled: 0.4263 (0.4209)  cardinality_error_2_unscaled: 292.2500 (291.8620)  loss_ce_3_unscaled: 0.8437 (0.8861)  loss_bbox_3_unscaled: 0.0926 (0.0929)  loss_giou_3_unscaled: 0.4185 (0.4177)  cardinality_error_3_unscaled: 292.2500 (291.8730)  loss_ce_4_unscaled: 0.8403 (0.8873)  loss_bbox_4_unscaled: 0.0910 (0.0927)  loss_giou_4_unscaled: 0.4215 (0.4174)  cardinality_error_4_unscaled: 292.0000 (291.7842)  time: 2.8812  data: 0.0284  max mem: 8138\n",
            "Test: Total time: 0:20:54 (3.0087 s / it)\n",
            "Averaged stats: class_error: 43.90  loss: 18.6397 (18.6679)  loss_ce: 1.7128 (1.7728)  loss_bbox: 0.4598 (0.4645)  loss_giou: 0.8419 (0.8350)  loss_ce_0: 1.7354 (1.7951)  loss_bbox_0: 0.5401 (0.5265)  loss_giou_0: 0.9381 (0.9585)  loss_ce_1: 1.6933 (1.7655)  loss_bbox_1: 0.4705 (0.4748)  loss_giou_1: 0.8591 (0.8620)  loss_ce_2: 1.6969 (1.7602)  loss_bbox_2: 0.4707 (0.4665)  loss_giou_2: 0.8526 (0.8418)  loss_ce_3: 1.6874 (1.7721)  loss_bbox_3: 0.4629 (0.4644)  loss_giou_3: 0.8370 (0.8354)  loss_ce_4: 1.6805 (1.7745)  loss_bbox_4: 0.4552 (0.4635)  loss_giou_4: 0.8430 (0.8348)  loss_ce_unscaled: 0.8564 (0.8864)  class_error_unscaled: 51.6667 (58.9588)  loss_bbox_unscaled: 0.0920 (0.0929)  loss_giou_unscaled: 0.4210 (0.4175)  cardinality_error_unscaled: 291.8333 (291.6430)  loss_ce_0_unscaled: 0.8677 (0.8976)  loss_bbox_0_unscaled: 0.1080 (0.1053)  loss_giou_0_unscaled: 0.4691 (0.4793)  cardinality_error_0_unscaled: 292.3333 (291.8886)  loss_ce_1_unscaled: 0.8467 (0.8827)  loss_bbox_1_unscaled: 0.0941 (0.0950)  loss_giou_1_unscaled: 0.4296 (0.4310)  cardinality_error_1_unscaled: 292.1667 (291.7523)  loss_ce_2_unscaled: 0.8484 (0.8801)  loss_bbox_2_unscaled: 0.0941 (0.0933)  loss_giou_2_unscaled: 0.4263 (0.4209)  cardinality_error_2_unscaled: 292.2500 (291.8620)  loss_ce_3_unscaled: 0.8437 (0.8861)  loss_bbox_3_unscaled: 0.0926 (0.0929)  loss_giou_3_unscaled: 0.4185 (0.4177)  cardinality_error_3_unscaled: 292.2500 (291.8730)  loss_ce_4_unscaled: 0.8403 (0.8873)  loss_bbox_4_unscaled: 0.0910 (0.0927)  loss_giou_4_unscaled: 0.4215 (0.4174)  cardinality_error_4_unscaled: 292.0000 (291.7842)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=12.82s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.261\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.387\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.279\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.117\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.270\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.395\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.238\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.379\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.404\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.216\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.418\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparision with Others"
      ],
      "metadata": {
        "id": "-2QP3J_JFjFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py \\\n",
        "    --output_dir exps/deform \\\n",
        "    --coco_path ../COCODIR \\\n",
        "    --batch_size 12 \\\n",
        "    --resume ./pth/underbound.pth \\\n",
        "    --with_box_refine \\\n",
        "    --eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uoWh6YHRVLL",
        "outputId": "ef52ff4f-3494-48c3-f7a9-c3839e4a0c4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "git:\n",
            "  sha: 11169a60c33333af00a4849f1808023eba96a931, status: has uncommited changes, branch: main\n",
            "\n",
            "Namespace(lr=0.0002, lr_backbone_names=['backbone.0'], lr_backbone=2e-05, lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, batch_size=12, weight_decay=0.0001, epochs=50, lr_drop=40, lr_drop_epochs=None, clip_max_norm=0.1, sgd=False, with_box_refine=True, two_stage=False, frozen_weights=None, backbone='resnet50', dilation=False, position_embedding='sine', position_embedding_scale=6.283185307179586, num_feature_levels=4, enc_layers=6, dec_layers=6, dim_feedforward=1024, hidden_dim=256, dropout=0.1, nheads=8, num_queries=300, dec_n_points=4, enc_n_points=4, masks=False, aux_loss=True, set_cost_class=2, set_cost_bbox=5, set_cost_giou=2, mask_loss_coef=1, dice_loss_coef=1, cls_loss_coef=2, bbox_loss_coef=5, giou_loss_coef=2, focal_alpha=0.25, dataset_file='coco', coco_path='../COCODIR', coco_panoptic_path=None, remove_difficult=False, output_dir='exps/deform', device='cuda', seed=42, resume='./pth/underbound.pth', start_epoch=0, eval=True, num_workers=2, cache_mode=False, distributed=False)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "number of params: 40627260\n",
            "loading annotations into memory...\n",
            "Done (t=14.79s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=1.87s)\n",
            "creating index...\n",
            "index created!\n",
            "transformer.level_embed\n",
            "transformer.encoder.layers.0.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.0.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.0.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.0.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.0.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.0.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.0.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.0.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.0.norm1.weight\n",
            "transformer.encoder.layers.0.norm1.bias\n",
            "transformer.encoder.layers.0.linear1.weight\n",
            "transformer.encoder.layers.0.linear1.bias\n",
            "transformer.encoder.layers.0.linear2.weight\n",
            "transformer.encoder.layers.0.linear2.bias\n",
            "transformer.encoder.layers.0.norm2.weight\n",
            "transformer.encoder.layers.0.norm2.bias\n",
            "transformer.encoder.layers.1.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.1.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.1.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.1.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.1.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.1.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.1.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.1.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.1.norm1.weight\n",
            "transformer.encoder.layers.1.norm1.bias\n",
            "transformer.encoder.layers.1.linear1.weight\n",
            "transformer.encoder.layers.1.linear1.bias\n",
            "transformer.encoder.layers.1.linear2.weight\n",
            "transformer.encoder.layers.1.linear2.bias\n",
            "transformer.encoder.layers.1.norm2.weight\n",
            "transformer.encoder.layers.1.norm2.bias\n",
            "transformer.encoder.layers.2.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.2.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.2.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.2.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.2.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.2.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.2.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.2.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.2.norm1.weight\n",
            "transformer.encoder.layers.2.norm1.bias\n",
            "transformer.encoder.layers.2.linear1.weight\n",
            "transformer.encoder.layers.2.linear1.bias\n",
            "transformer.encoder.layers.2.linear2.weight\n",
            "transformer.encoder.layers.2.linear2.bias\n",
            "transformer.encoder.layers.2.norm2.weight\n",
            "transformer.encoder.layers.2.norm2.bias\n",
            "transformer.encoder.layers.3.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.3.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.3.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.3.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.3.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.3.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.3.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.3.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.3.norm1.weight\n",
            "transformer.encoder.layers.3.norm1.bias\n",
            "transformer.encoder.layers.3.linear1.weight\n",
            "transformer.encoder.layers.3.linear1.bias\n",
            "transformer.encoder.layers.3.linear2.weight\n",
            "transformer.encoder.layers.3.linear2.bias\n",
            "transformer.encoder.layers.3.norm2.weight\n",
            "transformer.encoder.layers.3.norm2.bias\n",
            "transformer.encoder.layers.4.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.4.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.4.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.4.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.4.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.4.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.4.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.4.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.4.norm1.weight\n",
            "transformer.encoder.layers.4.norm1.bias\n",
            "transformer.encoder.layers.4.linear1.weight\n",
            "transformer.encoder.layers.4.linear1.bias\n",
            "transformer.encoder.layers.4.linear2.weight\n",
            "transformer.encoder.layers.4.linear2.bias\n",
            "transformer.encoder.layers.4.norm2.weight\n",
            "transformer.encoder.layers.4.norm2.bias\n",
            "transformer.encoder.layers.5.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.5.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.5.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.5.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.5.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.5.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.5.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.5.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.5.norm1.weight\n",
            "transformer.encoder.layers.5.norm1.bias\n",
            "transformer.encoder.layers.5.linear1.weight\n",
            "transformer.encoder.layers.5.linear1.bias\n",
            "transformer.encoder.layers.5.linear2.weight\n",
            "transformer.encoder.layers.5.linear2.bias\n",
            "transformer.encoder.layers.5.norm2.weight\n",
            "transformer.encoder.layers.5.norm2.bias\n",
            "transformer.decoder.layers.0.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.0.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.0.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.0.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.0.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.0.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.0.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.0.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.0.norm1.weight\n",
            "transformer.decoder.layers.0.norm1.bias\n",
            "transformer.decoder.layers.0.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.0.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.0.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.0.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.0.norm2.weight\n",
            "transformer.decoder.layers.0.norm2.bias\n",
            "transformer.decoder.layers.0.linear1.weight\n",
            "transformer.decoder.layers.0.linear1.bias\n",
            "transformer.decoder.layers.0.linear2.weight\n",
            "transformer.decoder.layers.0.linear2.bias\n",
            "transformer.decoder.layers.0.norm3.weight\n",
            "transformer.decoder.layers.0.norm3.bias\n",
            "transformer.decoder.layers.1.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.1.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.1.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.1.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.1.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.1.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.1.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.1.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.1.norm1.weight\n",
            "transformer.decoder.layers.1.norm1.bias\n",
            "transformer.decoder.layers.1.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.1.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.1.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.1.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.1.norm2.weight\n",
            "transformer.decoder.layers.1.norm2.bias\n",
            "transformer.decoder.layers.1.linear1.weight\n",
            "transformer.decoder.layers.1.linear1.bias\n",
            "transformer.decoder.layers.1.linear2.weight\n",
            "transformer.decoder.layers.1.linear2.bias\n",
            "transformer.decoder.layers.1.norm3.weight\n",
            "transformer.decoder.layers.1.norm3.bias\n",
            "transformer.decoder.layers.2.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.2.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.2.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.2.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.2.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.2.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.2.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.2.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.2.norm1.weight\n",
            "transformer.decoder.layers.2.norm1.bias\n",
            "transformer.decoder.layers.2.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.2.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.2.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.2.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.2.norm2.weight\n",
            "transformer.decoder.layers.2.norm2.bias\n",
            "transformer.decoder.layers.2.linear1.weight\n",
            "transformer.decoder.layers.2.linear1.bias\n",
            "transformer.decoder.layers.2.linear2.weight\n",
            "transformer.decoder.layers.2.linear2.bias\n",
            "transformer.decoder.layers.2.norm3.weight\n",
            "transformer.decoder.layers.2.norm3.bias\n",
            "transformer.decoder.layers.3.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.3.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.3.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.3.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.3.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.3.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.3.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.3.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.3.norm1.weight\n",
            "transformer.decoder.layers.3.norm1.bias\n",
            "transformer.decoder.layers.3.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.3.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.3.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.3.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.3.norm2.weight\n",
            "transformer.decoder.layers.3.norm2.bias\n",
            "transformer.decoder.layers.3.linear1.weight\n",
            "transformer.decoder.layers.3.linear1.bias\n",
            "transformer.decoder.layers.3.linear2.weight\n",
            "transformer.decoder.layers.3.linear2.bias\n",
            "transformer.decoder.layers.3.norm3.weight\n",
            "transformer.decoder.layers.3.norm3.bias\n",
            "transformer.decoder.layers.4.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.4.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.4.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.4.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.4.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.4.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.4.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.4.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.4.norm1.weight\n",
            "transformer.decoder.layers.4.norm1.bias\n",
            "transformer.decoder.layers.4.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.4.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.4.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.4.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.4.norm2.weight\n",
            "transformer.decoder.layers.4.norm2.bias\n",
            "transformer.decoder.layers.4.linear1.weight\n",
            "transformer.decoder.layers.4.linear1.bias\n",
            "transformer.decoder.layers.4.linear2.weight\n",
            "transformer.decoder.layers.4.linear2.bias\n",
            "transformer.decoder.layers.4.norm3.weight\n",
            "transformer.decoder.layers.4.norm3.bias\n",
            "transformer.decoder.layers.5.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.5.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.5.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.5.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.5.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.5.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.5.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.5.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.5.norm1.weight\n",
            "transformer.decoder.layers.5.norm1.bias\n",
            "transformer.decoder.layers.5.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.5.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.5.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.5.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.5.norm2.weight\n",
            "transformer.decoder.layers.5.norm2.bias\n",
            "transformer.decoder.layers.5.linear1.weight\n",
            "transformer.decoder.layers.5.linear1.bias\n",
            "transformer.decoder.layers.5.linear2.weight\n",
            "transformer.decoder.layers.5.linear2.bias\n",
            "transformer.decoder.layers.5.norm3.weight\n",
            "transformer.decoder.layers.5.norm3.bias\n",
            "transformer.decoder.bbox_embed.0.layers.0.weight\n",
            "transformer.decoder.bbox_embed.0.layers.0.bias\n",
            "transformer.decoder.bbox_embed.0.layers.1.weight\n",
            "transformer.decoder.bbox_embed.0.layers.1.bias\n",
            "transformer.decoder.bbox_embed.0.layers.2.weight\n",
            "transformer.decoder.bbox_embed.0.layers.2.bias\n",
            "transformer.decoder.bbox_embed.1.layers.0.weight\n",
            "transformer.decoder.bbox_embed.1.layers.0.bias\n",
            "transformer.decoder.bbox_embed.1.layers.1.weight\n",
            "transformer.decoder.bbox_embed.1.layers.1.bias\n",
            "transformer.decoder.bbox_embed.1.layers.2.weight\n",
            "transformer.decoder.bbox_embed.1.layers.2.bias\n",
            "transformer.decoder.bbox_embed.2.layers.0.weight\n",
            "transformer.decoder.bbox_embed.2.layers.0.bias\n",
            "transformer.decoder.bbox_embed.2.layers.1.weight\n",
            "transformer.decoder.bbox_embed.2.layers.1.bias\n",
            "transformer.decoder.bbox_embed.2.layers.2.weight\n",
            "transformer.decoder.bbox_embed.2.layers.2.bias\n",
            "transformer.decoder.bbox_embed.3.layers.0.weight\n",
            "transformer.decoder.bbox_embed.3.layers.0.bias\n",
            "transformer.decoder.bbox_embed.3.layers.1.weight\n",
            "transformer.decoder.bbox_embed.3.layers.1.bias\n",
            "transformer.decoder.bbox_embed.3.layers.2.weight\n",
            "transformer.decoder.bbox_embed.3.layers.2.bias\n",
            "transformer.decoder.bbox_embed.4.layers.0.weight\n",
            "transformer.decoder.bbox_embed.4.layers.0.bias\n",
            "transformer.decoder.bbox_embed.4.layers.1.weight\n",
            "transformer.decoder.bbox_embed.4.layers.1.bias\n",
            "transformer.decoder.bbox_embed.4.layers.2.weight\n",
            "transformer.decoder.bbox_embed.4.layers.2.bias\n",
            "transformer.decoder.bbox_embed.5.layers.0.weight\n",
            "transformer.decoder.bbox_embed.5.layers.0.bias\n",
            "transformer.decoder.bbox_embed.5.layers.1.weight\n",
            "transformer.decoder.bbox_embed.5.layers.1.bias\n",
            "transformer.decoder.bbox_embed.5.layers.2.weight\n",
            "transformer.decoder.bbox_embed.5.layers.2.bias\n",
            "transformer.reference_points.weight\n",
            "transformer.reference_points.bias\n",
            "class_embed.0.weight\n",
            "class_embed.0.bias\n",
            "class_embed.1.weight\n",
            "class_embed.1.bias\n",
            "class_embed.2.weight\n",
            "class_embed.2.bias\n",
            "class_embed.3.weight\n",
            "class_embed.3.bias\n",
            "class_embed.4.weight\n",
            "class_embed.4.bias\n",
            "class_embed.5.weight\n",
            "class_embed.5.bias\n",
            "query_embed.weight\n",
            "input_proj.0.0.weight\n",
            "input_proj.0.0.bias\n",
            "input_proj.0.1.weight\n",
            "input_proj.0.1.bias\n",
            "input_proj.1.0.weight\n",
            "input_proj.1.0.bias\n",
            "input_proj.1.1.weight\n",
            "input_proj.1.1.bias\n",
            "input_proj.2.0.weight\n",
            "input_proj.2.0.bias\n",
            "input_proj.2.1.weight\n",
            "input_proj.2.1.bias\n",
            "input_proj.3.0.weight\n",
            "input_proj.3.0.bias\n",
            "input_proj.3.1.weight\n",
            "input_proj.3.1.bias\n",
            "backbone.0.body.conv1.weight\n",
            "backbone.0.body.layer1.0.conv1.weight\n",
            "backbone.0.body.layer1.0.conv2.weight\n",
            "backbone.0.body.layer1.0.conv3.weight\n",
            "backbone.0.body.layer1.0.downsample.0.weight\n",
            "backbone.0.body.layer1.1.conv1.weight\n",
            "backbone.0.body.layer1.1.conv2.weight\n",
            "backbone.0.body.layer1.1.conv3.weight\n",
            "backbone.0.body.layer1.2.conv1.weight\n",
            "backbone.0.body.layer1.2.conv2.weight\n",
            "backbone.0.body.layer1.2.conv3.weight\n",
            "backbone.0.body.layer2.0.conv1.weight\n",
            "backbone.0.body.layer2.0.conv2.weight\n",
            "backbone.0.body.layer2.0.conv3.weight\n",
            "backbone.0.body.layer2.0.downsample.0.weight\n",
            "backbone.0.body.layer2.1.conv1.weight\n",
            "backbone.0.body.layer2.1.conv2.weight\n",
            "backbone.0.body.layer2.1.conv3.weight\n",
            "backbone.0.body.layer2.2.conv1.weight\n",
            "backbone.0.body.layer2.2.conv2.weight\n",
            "backbone.0.body.layer2.2.conv3.weight\n",
            "backbone.0.body.layer2.3.conv1.weight\n",
            "backbone.0.body.layer2.3.conv2.weight\n",
            "backbone.0.body.layer2.3.conv3.weight\n",
            "backbone.0.body.layer3.0.conv1.weight\n",
            "backbone.0.body.layer3.0.conv2.weight\n",
            "backbone.0.body.layer3.0.conv3.weight\n",
            "backbone.0.body.layer3.0.downsample.0.weight\n",
            "backbone.0.body.layer3.1.conv1.weight\n",
            "backbone.0.body.layer3.1.conv2.weight\n",
            "backbone.0.body.layer3.1.conv3.weight\n",
            "backbone.0.body.layer3.2.conv1.weight\n",
            "backbone.0.body.layer3.2.conv2.weight\n",
            "backbone.0.body.layer3.2.conv3.weight\n",
            "backbone.0.body.layer3.3.conv1.weight\n",
            "backbone.0.body.layer3.3.conv2.weight\n",
            "backbone.0.body.layer3.3.conv3.weight\n",
            "backbone.0.body.layer3.4.conv1.weight\n",
            "backbone.0.body.layer3.4.conv2.weight\n",
            "backbone.0.body.layer3.4.conv3.weight\n",
            "backbone.0.body.layer3.5.conv1.weight\n",
            "backbone.0.body.layer3.5.conv2.weight\n",
            "backbone.0.body.layer3.5.conv3.weight\n",
            "backbone.0.body.layer4.0.conv1.weight\n",
            "backbone.0.body.layer4.0.conv2.weight\n",
            "backbone.0.body.layer4.0.conv3.weight\n",
            "backbone.0.body.layer4.0.downsample.0.weight\n",
            "backbone.0.body.layer4.1.conv1.weight\n",
            "backbone.0.body.layer4.1.conv2.weight\n",
            "backbone.0.body.layer4.1.conv3.weight\n",
            "backbone.0.body.layer4.2.conv1.weight\n",
            "backbone.0.body.layer4.2.conv2.weight\n",
            "backbone.0.body.layer4.2.conv3.weight\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Test:  [  0/417]  eta: 0:27:41  class_error: 56.84  loss: 24.9253 (24.9253)  loss_ce: 2.2219 (2.2219)  loss_bbox: 0.7683 (0.7683)  loss_giou: 1.1336 (1.1336)  loss_ce_0: 2.2519 (2.2519)  loss_bbox_0: 0.7880 (0.7880)  loss_giou_0: 1.2630 (1.2630)  loss_ce_1: 2.2421 (2.2421)  loss_bbox_1: 0.7765 (0.7765)  loss_giou_1: 1.1299 (1.1299)  loss_ce_2: 2.2104 (2.2104)  loss_bbox_2: 0.7870 (0.7870)  loss_giou_2: 1.1363 (1.1363)  loss_ce_3: 2.2002 (2.2002)  loss_bbox_3: 0.7774 (0.7774)  loss_giou_3: 1.1254 (1.1254)  loss_ce_4: 2.2134 (2.2134)  loss_bbox_4: 0.7657 (0.7657)  loss_giou_4: 1.1343 (1.1343)  loss_ce_unscaled: 1.1109 (1.1109)  class_error_unscaled: 56.8421 (56.8421)  loss_bbox_unscaled: 0.1537 (0.1537)  loss_giou_unscaled: 0.5668 (0.5668)  cardinality_error_unscaled: 292.0000 (292.0000)  loss_ce_0_unscaled: 1.1259 (1.1259)  loss_bbox_0_unscaled: 0.1576 (0.1576)  loss_giou_0_unscaled: 0.6315 (0.6315)  cardinality_error_0_unscaled: 291.5000 (291.5000)  loss_ce_1_unscaled: 1.1211 (1.1211)  loss_bbox_1_unscaled: 0.1553 (0.1553)  loss_giou_1_unscaled: 0.5650 (0.5650)  cardinality_error_1_unscaled: 291.4167 (291.4167)  loss_ce_2_unscaled: 1.1052 (1.1052)  loss_bbox_2_unscaled: 0.1574 (0.1574)  loss_giou_2_unscaled: 0.5682 (0.5682)  cardinality_error_2_unscaled: 292.0833 (292.0833)  loss_ce_3_unscaled: 1.1001 (1.1001)  loss_bbox_3_unscaled: 0.1555 (0.1555)  loss_giou_3_unscaled: 0.5627 (0.5627)  cardinality_error_3_unscaled: 290.8333 (290.8333)  loss_ce_4_unscaled: 1.1067 (1.1067)  loss_bbox_4_unscaled: 0.1531 (0.1531)  loss_giou_4_unscaled: 0.5672 (0.5672)  cardinality_error_4_unscaled: 292.0000 (292.0000)  time: 3.9833  data: 0.9473  max mem: 6678\n",
            "Test:  [ 10/417]  eta: 0:19:52  class_error: 80.81  loss: 27.0537 (27.0249)  loss_ce: 2.7502 (2.7689)  loss_bbox: 0.6652 (0.6576)  loss_giou: 1.1018 (1.0705)  loss_ce_0: 2.7566 (2.7991)  loss_bbox_0: 0.6785 (0.6886)  loss_giou_0: 1.1545 (1.1323)  loss_ce_1: 2.7508 (2.7560)  loss_bbox_1: 0.6624 (0.6610)  loss_giou_1: 1.1196 (1.0732)  loss_ce_2: 2.7169 (2.7597)  loss_bbox_2: 0.6747 (0.6615)  loss_giou_2: 1.1154 (1.0766)  loss_ce_3: 2.6907 (2.7191)  loss_bbox_3: 0.6687 (0.6580)  loss_giou_3: 1.1012 (1.0698)  loss_ce_4: 2.7321 (2.7462)  loss_bbox_4: 0.6604 (0.6560)  loss_giou_4: 1.1047 (1.0707)  loss_ce_unscaled: 1.3751 (1.3845)  class_error_unscaled: 77.7778 (73.0584)  loss_bbox_unscaled: 0.1330 (0.1315)  loss_giou_unscaled: 0.5509 (0.5352)  cardinality_error_unscaled: 292.2500 (292.8485)  loss_ce_0_unscaled: 1.3783 (1.3995)  loss_bbox_0_unscaled: 0.1357 (0.1377)  loss_giou_0_unscaled: 0.5772 (0.5661)  cardinality_error_0_unscaled: 292.3333 (292.6667)  loss_ce_1_unscaled: 1.3754 (1.3780)  loss_bbox_1_unscaled: 0.1325 (0.1322)  loss_giou_1_unscaled: 0.5598 (0.5366)  cardinality_error_1_unscaled: 292.1667 (292.6894)  loss_ce_2_unscaled: 1.3584 (1.3799)  loss_bbox_2_unscaled: 0.1349 (0.1323)  loss_giou_2_unscaled: 0.5577 (0.5383)  cardinality_error_2_unscaled: 292.0833 (292.7803)  loss_ce_3_unscaled: 1.3453 (1.3596)  loss_bbox_3_unscaled: 0.1337 (0.1316)  loss_giou_3_unscaled: 0.5506 (0.5349)  cardinality_error_3_unscaled: 291.4167 (291.7652)  loss_ce_4_unscaled: 1.3661 (1.3731)  loss_bbox_4_unscaled: 0.1321 (0.1312)  loss_giou_4_unscaled: 0.5524 (0.5354)  cardinality_error_4_unscaled: 292.4167 (292.8485)  time: 2.9299  data: 0.1134  max mem: 7374\n",
            "Test:  [ 20/417]  eta: 0:19:05  class_error: 62.77  loss: 27.0537 (26.4778)  loss_ce: 2.7384 (2.7251)  loss_bbox: 0.5758 (0.6310)  loss_giou: 1.0349 (1.0372)  loss_ce_0: 2.7391 (2.7572)  loss_bbox_0: 0.6432 (0.6731)  loss_giou_0: 1.1084 (1.1183)  loss_ce_1: 2.7231 (2.7225)  loss_bbox_1: 0.5838 (0.6358)  loss_giou_1: 1.0565 (1.0497)  loss_ce_2: 2.7161 (2.7227)  loss_bbox_2: 0.5736 (0.6360)  loss_giou_2: 1.0240 (1.0448)  loss_ce_3: 2.6672 (2.6828)  loss_bbox_3: 0.5685 (0.6305)  loss_giou_3: 1.0274 (1.0377)  loss_ce_4: 2.7163 (2.7062)  loss_bbox_4: 0.5675 (0.6304)  loss_giou_4: 1.0252 (1.0368)  loss_ce_unscaled: 1.3692 (1.3625)  class_error_unscaled: 73.5849 (71.6017)  loss_bbox_unscaled: 0.1152 (0.1262)  loss_giou_unscaled: 0.5175 (0.5186)  cardinality_error_unscaled: 292.2500 (292.8254)  loss_ce_0_unscaled: 1.3695 (1.3786)  loss_bbox_0_unscaled: 0.1286 (0.1346)  loss_giou_0_unscaled: 0.5542 (0.5591)  cardinality_error_0_unscaled: 292.3333 (292.6587)  loss_ce_1_unscaled: 1.3616 (1.3613)  loss_bbox_1_unscaled: 0.1168 (0.1272)  loss_giou_1_unscaled: 0.5283 (0.5249)  cardinality_error_1_unscaled: 292.1667 (292.6270)  loss_ce_2_unscaled: 1.3581 (1.3613)  loss_bbox_2_unscaled: 0.1147 (0.1272)  loss_giou_2_unscaled: 0.5120 (0.5224)  cardinality_error_2_unscaled: 292.1667 (292.7619)  loss_ce_3_unscaled: 1.3336 (1.3414)  loss_bbox_3_unscaled: 0.1137 (0.1261)  loss_giou_3_unscaled: 0.5137 (0.5188)  cardinality_error_3_unscaled: 291.4167 (291.6230)  loss_ce_4_unscaled: 1.3581 (1.3531)  loss_bbox_4_unscaled: 0.1135 (0.1261)  loss_giou_4_unscaled: 0.5126 (0.5184)  cardinality_error_4_unscaled: 292.4167 (292.8413)  time: 2.8316  data: 0.0324  max mem: 7755\n",
            "Test:  [ 30/417]  eta: 0:19:02  class_error: 67.61  loss: 24.7281 (25.4804)  loss_ce: 2.4608 (2.5989)  loss_bbox: 0.5680 (0.5891)  loss_giou: 0.9914 (1.0379)  loss_ce_0: 2.4828 (2.6343)  loss_bbox_0: 0.5894 (0.6292)  loss_giou_0: 1.0954 (1.1184)  loss_ce_1: 2.4482 (2.6003)  loss_bbox_1: 0.5771 (0.5937)  loss_giou_1: 1.0266 (1.0512)  loss_ce_2: 2.4582 (2.5987)  loss_bbox_2: 0.5585 (0.5924)  loss_giou_2: 1.0056 (1.0455)  loss_ce_3: 2.4164 (2.5597)  loss_bbox_3: 0.5609 (0.5877)  loss_giou_3: 0.9965 (1.0376)  loss_ce_4: 2.4447 (2.5797)  loss_bbox_4: 0.5626 (0.5889)  loss_giou_4: 0.9882 (1.0372)  loss_ce_unscaled: 1.2304 (1.2995)  class_error_unscaled: 62.7660 (68.0501)  loss_bbox_unscaled: 0.1136 (0.1178)  loss_giou_unscaled: 0.4957 (0.5190)  cardinality_error_unscaled: 291.5000 (292.3414)  loss_ce_0_unscaled: 1.2414 (1.3172)  loss_bbox_0_unscaled: 0.1179 (0.1258)  loss_giou_0_unscaled: 0.5477 (0.5592)  cardinality_error_0_unscaled: 291.5833 (292.1882)  loss_ce_1_unscaled: 1.2241 (1.3002)  loss_bbox_1_unscaled: 0.1154 (0.1187)  loss_giou_1_unscaled: 0.5133 (0.5256)  cardinality_error_1_unscaled: 291.5833 (292.2258)  loss_ce_2_unscaled: 1.2291 (1.2993)  loss_bbox_2_unscaled: 0.1117 (0.1185)  loss_giou_2_unscaled: 0.5028 (0.5227)  cardinality_error_2_unscaled: 291.4167 (292.3199)  loss_ce_3_unscaled: 1.2082 (1.2799)  loss_bbox_3_unscaled: 0.1122 (0.1175)  loss_giou_3_unscaled: 0.4983 (0.5188)  cardinality_error_3_unscaled: 290.6667 (291.2097)  loss_ce_4_unscaled: 1.2223 (1.2899)  loss_bbox_4_unscaled: 0.1125 (0.1178)  loss_giou_4_unscaled: 0.4941 (0.5186)  cardinality_error_4_unscaled: 291.5000 (292.3280)  time: 2.9638  data: 0.0365  max mem: 8137\n",
            "Test:  [ 40/417]  eta: 0:18:52  class_error: 71.43  loss: 26.8264 (26.0432)  loss_ce: 2.6915 (2.6594)  loss_bbox: 0.5992 (0.6086)  loss_giou: 1.0317 (1.0482)  loss_ce_0: 2.7344 (2.6994)  loss_bbox_0: 0.6018 (0.6483)  loss_giou_0: 1.1349 (1.1251)  loss_ce_1: 2.7406 (2.6682)  loss_bbox_1: 0.5976 (0.6129)  loss_giou_1: 1.0408 (1.0618)  loss_ce_2: 2.7171 (2.6638)  loss_bbox_2: 0.5837 (0.6109)  loss_giou_2: 1.0332 (1.0581)  loss_ce_3: 2.6813 (2.6236)  loss_bbox_3: 0.5856 (0.6068)  loss_giou_3: 1.0261 (1.0493)  loss_ce_4: 2.6790 (2.6423)  loss_bbox_4: 0.5965 (0.6089)  loss_giou_4: 1.0279 (1.0475)  loss_ce_unscaled: 1.3458 (1.3297)  class_error_unscaled: 71.4286 (69.4887)  loss_bbox_unscaled: 0.1198 (0.1217)  loss_giou_unscaled: 0.5159 (0.5241)  cardinality_error_unscaled: 293.0000 (292.6240)  loss_ce_0_unscaled: 1.3672 (1.3497)  loss_bbox_0_unscaled: 0.1204 (0.1297)  loss_giou_0_unscaled: 0.5674 (0.5625)  cardinality_error_0_unscaled: 292.6667 (292.4939)  loss_ce_1_unscaled: 1.3703 (1.3341)  loss_bbox_1_unscaled: 0.1195 (0.1226)  loss_giou_1_unscaled: 0.5204 (0.5309)  cardinality_error_1_unscaled: 292.7500 (292.5102)  loss_ce_2_unscaled: 1.3585 (1.3319)  loss_bbox_2_unscaled: 0.1167 (0.1222)  loss_giou_2_unscaled: 0.5166 (0.5291)  cardinality_error_2_unscaled: 292.6667 (292.5508)  loss_ce_3_unscaled: 1.3406 (1.3118)  loss_bbox_3_unscaled: 0.1171 (0.1214)  loss_giou_3_unscaled: 0.5130 (0.5247)  cardinality_error_3_unscaled: 290.7500 (291.4309)  loss_ce_4_unscaled: 1.3395 (1.3212)  loss_bbox_4_unscaled: 0.1193 (0.1218)  loss_giou_4_unscaled: 0.5139 (0.5237)  cardinality_error_4_unscaled: 292.7500 (292.6077)  time: 3.1273  data: 0.0385  max mem: 8137\n",
            "Test:  [ 50/417]  eta: 0:18:27  class_error: 61.11  loss: 27.2822 (26.0732)  loss_ce: 2.8055 (2.6542)  loss_bbox: 0.6498 (0.6168)  loss_giou: 1.0723 (1.0531)  loss_ce_0: 2.8546 (2.6909)  loss_bbox_0: 0.6825 (0.6537)  loss_giou_0: 1.1382 (1.1292)  loss_ce_1: 2.8343 (2.6605)  loss_bbox_1: 0.6451 (0.6197)  loss_giou_1: 1.1044 (1.0665)  loss_ce_2: 2.8264 (2.6552)  loss_bbox_2: 0.6386 (0.6189)  loss_giou_2: 1.0827 (1.0625)  loss_ce_3: 2.7772 (2.6180)  loss_bbox_3: 0.6322 (0.6143)  loss_giou_3: 1.0668 (1.0539)  loss_ce_4: 2.7960 (2.6367)  loss_bbox_4: 0.6447 (0.6163)  loss_giou_4: 1.0638 (1.0528)  loss_ce_unscaled: 1.4028 (1.3271)  class_error_unscaled: 73.3333 (69.4095)  loss_bbox_unscaled: 0.1300 (0.1234)  loss_giou_unscaled: 0.5361 (0.5266)  cardinality_error_unscaled: 292.5000 (292.2974)  loss_ce_0_unscaled: 1.4273 (1.3455)  loss_bbox_0_unscaled: 0.1365 (0.1307)  loss_giou_0_unscaled: 0.5691 (0.5646)  cardinality_error_0_unscaled: 292.4167 (292.2075)  loss_ce_1_unscaled: 1.4172 (1.3303)  loss_bbox_1_unscaled: 0.1290 (0.1239)  loss_giou_1_unscaled: 0.5522 (0.5333)  cardinality_error_1_unscaled: 292.5000 (292.2157)  loss_ce_2_unscaled: 1.4132 (1.3276)  loss_bbox_2_unscaled: 0.1277 (0.1238)  loss_giou_2_unscaled: 0.5413 (0.5312)  cardinality_error_2_unscaled: 292.3333 (292.2647)  loss_ce_3_unscaled: 1.3886 (1.3090)  loss_bbox_3_unscaled: 0.1264 (0.1229)  loss_giou_3_unscaled: 0.5334 (0.5270)  cardinality_error_3_unscaled: 290.9167 (291.0964)  loss_ce_4_unscaled: 1.3980 (1.3184)  loss_bbox_4_unscaled: 0.1289 (0.1233)  loss_giou_4_unscaled: 0.5319 (0.5264)  cardinality_error_4_unscaled: 292.5000 (292.2663)  time: 3.1181  data: 0.0379  max mem: 8138\n",
            "Test:  [ 60/417]  eta: 0:18:00  class_error: 69.32  loss: 27.1924 (26.3881)  loss_ce: 2.7432 (2.6889)  loss_bbox: 0.6378 (0.6213)  loss_giou: 1.1203 (1.0664)  loss_ce_0: 2.8011 (2.7269)  loss_bbox_0: 0.6523 (0.6549)  loss_giou_0: 1.1626 (1.1393)  loss_ce_1: 2.7578 (2.6958)  loss_bbox_1: 0.6320 (0.6237)  loss_giou_1: 1.1245 (1.0799)  loss_ce_2: 2.7897 (2.6929)  loss_bbox_2: 0.6369 (0.6227)  loss_giou_2: 1.1232 (1.0761)  loss_ce_3: 2.7202 (2.6541)  loss_bbox_3: 0.6322 (0.6187)  loss_giou_3: 1.1201 (1.0674)  loss_ce_4: 2.7464 (2.6720)  loss_bbox_4: 0.6278 (0.6208)  loss_giou_4: 1.1167 (1.0662)  loss_ce_unscaled: 1.3716 (1.3445)  class_error_unscaled: 73.3333 (70.3823)  loss_bbox_unscaled: 0.1276 (0.1243)  loss_giou_unscaled: 0.5602 (0.5332)  cardinality_error_unscaled: 291.1667 (292.1421)  loss_ce_0_unscaled: 1.4006 (1.3635)  loss_bbox_0_unscaled: 0.1305 (0.1310)  loss_giou_0_unscaled: 0.5813 (0.5696)  cardinality_error_0_unscaled: 290.9167 (292.0506)  loss_ce_1_unscaled: 1.3789 (1.3479)  loss_bbox_1_unscaled: 0.1264 (0.1247)  loss_giou_1_unscaled: 0.5623 (0.5400)  cardinality_error_1_unscaled: 291.0833 (292.0478)  loss_ce_2_unscaled: 1.3948 (1.3465)  loss_bbox_2_unscaled: 0.1274 (0.1245)  loss_giou_2_unscaled: 0.5616 (0.5381)  cardinality_error_2_unscaled: 291.3333 (292.1407)  loss_ce_3_unscaled: 1.3601 (1.3271)  loss_bbox_3_unscaled: 0.1264 (0.1237)  loss_giou_3_unscaled: 0.5601 (0.5337)  cardinality_error_3_unscaled: 290.4167 (291.0205)  loss_ce_4_unscaled: 1.3732 (1.3360)  loss_bbox_4_unscaled: 0.1256 (0.1242)  loss_giou_4_unscaled: 0.5584 (0.5331)  cardinality_error_4_unscaled: 291.0833 (292.1394)  time: 3.0773  data: 0.0379  max mem: 8138\n",
            "Test:  [ 70/417]  eta: 0:17:31  class_error: 62.37  loss: 27.3798 (26.3248)  loss_ce: 2.7134 (2.6809)  loss_bbox: 0.6264 (0.6200)  loss_giou: 1.1203 (1.0650)  loss_ce_0: 2.7873 (2.7188)  loss_bbox_0: 0.6523 (0.6532)  loss_giou_0: 1.1602 (1.1365)  loss_ce_1: 2.7578 (2.6889)  loss_bbox_1: 0.6297 (0.6212)  loss_giou_1: 1.1245 (1.0784)  loss_ce_2: 2.7612 (2.6871)  loss_bbox_2: 0.6345 (0.6203)  loss_giou_2: 1.1232 (1.0741)  loss_ce_3: 2.7171 (2.6473)  loss_bbox_3: 0.6280 (0.6174)  loss_giou_3: 1.1201 (1.0663)  loss_ce_4: 2.6990 (2.6653)  loss_bbox_4: 0.6260 (0.6194)  loss_giou_4: 1.1167 (1.0646)  loss_ce_unscaled: 1.3567 (1.3404)  class_error_unscaled: 75.3086 (70.1563)  loss_bbox_unscaled: 0.1253 (0.1240)  loss_giou_unscaled: 0.5602 (0.5325)  cardinality_error_unscaled: 291.1667 (292.1948)  loss_ce_0_unscaled: 1.3936 (1.3594)  loss_bbox_0_unscaled: 0.1305 (0.1306)  loss_giou_0_unscaled: 0.5801 (0.5683)  cardinality_error_0_unscaled: 290.8333 (292.0810)  loss_ce_1_unscaled: 1.3789 (1.3444)  loss_bbox_1_unscaled: 0.1259 (0.1242)  loss_giou_1_unscaled: 0.5623 (0.5392)  cardinality_error_1_unscaled: 291.0833 (292.0951)  loss_ce_2_unscaled: 1.3806 (1.3436)  loss_bbox_2_unscaled: 0.1269 (0.1241)  loss_giou_2_unscaled: 0.5616 (0.5371)  cardinality_error_2_unscaled: 291.3333 (292.1608)  loss_ce_3_unscaled: 1.3586 (1.3236)  loss_bbox_3_unscaled: 0.1256 (0.1235)  loss_giou_3_unscaled: 0.5601 (0.5332)  cardinality_error_3_unscaled: 290.1667 (290.9695)  loss_ce_4_unscaled: 1.3495 (1.3327)  loss_bbox_4_unscaled: 0.1252 (0.1239)  loss_giou_4_unscaled: 0.5584 (0.5323)  cardinality_error_4_unscaled: 291.0833 (292.1573)  time: 3.0686  data: 0.0387  max mem: 8138\n",
            "Test:  [ 80/417]  eta: 0:17:06  class_error: 71.88  loss: 27.0573 (26.2579)  loss_ce: 2.6826 (2.6772)  loss_bbox: 0.6382 (0.6202)  loss_giou: 1.0598 (1.0552)  loss_ce_0: 2.6952 (2.7163)  loss_bbox_0: 0.6729 (0.6551)  loss_giou_0: 1.1118 (1.1272)  loss_ce_1: 2.6827 (2.6888)  loss_bbox_1: 0.6423 (0.6216)  loss_giou_1: 1.0676 (1.0689)  loss_ce_2: 2.6833 (2.6861)  loss_bbox_2: 0.6413 (0.6210)  loss_giou_2: 1.0541 (1.0643)  loss_ce_3: 2.6220 (2.6455)  loss_bbox_3: 0.6328 (0.6170)  loss_giou_3: 1.0538 (1.0566)  loss_ce_4: 2.6428 (2.6630)  loss_bbox_4: 0.6263 (0.6193)  loss_giou_4: 1.0505 (1.0547)  loss_ce_unscaled: 1.3413 (1.3386)  class_error_unscaled: 70.0000 (70.0516)  loss_bbox_unscaled: 0.1276 (0.1240)  loss_giou_unscaled: 0.5299 (0.5276)  cardinality_error_unscaled: 292.5833 (292.3138)  loss_ce_0_unscaled: 1.3476 (1.3582)  loss_bbox_0_unscaled: 0.1346 (0.1310)  loss_giou_0_unscaled: 0.5559 (0.5636)  cardinality_error_0_unscaled: 292.1667 (292.1739)  loss_ce_1_unscaled: 1.3414 (1.3444)  loss_bbox_1_unscaled: 0.1285 (0.1243)  loss_giou_1_unscaled: 0.5338 (0.5344)  cardinality_error_1_unscaled: 292.7500 (292.2191)  loss_ce_2_unscaled: 1.3417 (1.3431)  loss_bbox_2_unscaled: 0.1283 (0.1242)  loss_giou_2_unscaled: 0.5271 (0.5321)  cardinality_error_2_unscaled: 292.7500 (292.2747)  loss_ce_3_unscaled: 1.3110 (1.3227)  loss_bbox_3_unscaled: 0.1266 (0.1234)  loss_giou_3_unscaled: 0.5269 (0.5283)  cardinality_error_3_unscaled: 290.9167 (291.0412)  loss_ce_4_unscaled: 1.3214 (1.3315)  loss_bbox_4_unscaled: 0.1253 (0.1239)  loss_giou_4_unscaled: 0.5253 (0.5273)  cardinality_error_4_unscaled: 292.5000 (292.2582)  time: 3.0960  data: 0.0374  max mem: 8138\n",
            "Test:  [ 90/417]  eta: 0:16:36  class_error: 67.86  loss: 27.1562 (26.3414)  loss_ce: 2.7427 (2.6821)  loss_bbox: 0.5772 (0.6208)  loss_giou: 1.0858 (1.0647)  loss_ce_0: 2.8045 (2.7214)  loss_bbox_0: 0.6550 (0.6553)  loss_giou_0: 1.1366 (1.1339)  loss_ce_1: 2.7511 (2.6944)  loss_bbox_1: 0.5918 (0.6217)  loss_giou_1: 1.1022 (1.0777)  loss_ce_2: 2.7864 (2.6913)  loss_bbox_2: 0.5851 (0.6215)  loss_giou_2: 1.1037 (1.0734)  loss_ce_3: 2.7277 (2.6492)  loss_bbox_3: 0.5828 (0.6176)  loss_giou_3: 1.0959 (1.0664)  loss_ce_4: 2.7296 (2.6665)  loss_bbox_4: 0.5700 (0.6194)  loss_giou_4: 1.0898 (1.0642)  loss_ce_unscaled: 1.3714 (1.3410)  class_error_unscaled: 71.6216 (70.2903)  loss_bbox_unscaled: 0.1154 (0.1242)  loss_giou_unscaled: 0.5429 (0.5323)  cardinality_error_unscaled: 292.4167 (292.2042)  loss_ce_0_unscaled: 1.4022 (1.3607)  loss_bbox_0_unscaled: 0.1310 (0.1311)  loss_giou_0_unscaled: 0.5683 (0.5669)  cardinality_error_0_unscaled: 292.0833 (292.0696)  loss_ce_1_unscaled: 1.3755 (1.3472)  loss_bbox_1_unscaled: 0.1184 (0.1243)  loss_giou_1_unscaled: 0.5511 (0.5389)  cardinality_error_1_unscaled: 292.2500 (292.1236)  loss_ce_2_unscaled: 1.3932 (1.3457)  loss_bbox_2_unscaled: 0.1170 (0.1243)  loss_giou_2_unscaled: 0.5519 (0.5367)  cardinality_error_2_unscaled: 292.0000 (292.1484)  loss_ce_3_unscaled: 1.3638 (1.3246)  loss_bbox_3_unscaled: 0.1166 (0.1235)  loss_giou_3_unscaled: 0.5480 (0.5332)  cardinality_error_3_unscaled: 290.9167 (290.8892)  loss_ce_4_unscaled: 1.3648 (1.3332)  loss_bbox_4_unscaled: 0.1140 (0.1239)  loss_giou_4_unscaled: 0.5449 (0.5321)  cardinality_error_4_unscaled: 292.0833 (292.1584)  time: 3.0998  data: 0.0371  max mem: 8138\n",
            "Test:  [100/417]  eta: 0:16:03  class_error: 71.83  loss: 27.4582 (26.5163)  loss_ce: 2.7888 (2.7062)  loss_bbox: 0.6224 (0.6225)  loss_giou: 1.1251 (1.0681)  loss_ce_0: 2.8354 (2.7456)  loss_bbox_0: 0.6534 (0.6557)  loss_giou_0: 1.1748 (1.1358)  loss_ce_1: 2.7916 (2.7190)  loss_bbox_1: 0.6326 (0.6235)  loss_giou_1: 1.1515 (1.0808)  loss_ce_2: 2.7904 (2.7166)  loss_bbox_2: 0.6347 (0.6235)  loss_giou_2: 1.1520 (1.0770)  loss_ce_3: 2.7480 (2.6742)  loss_bbox_3: 0.6227 (0.6194)  loss_giou_3: 1.1304 (1.0697)  loss_ce_4: 2.7676 (2.6901)  loss_bbox_4: 0.6217 (0.6205)  loss_giou_4: 1.1486 (1.0681)  loss_ce_unscaled: 1.3944 (1.3531)  class_error_unscaled: 71.9512 (70.9068)  loss_bbox_unscaled: 0.1245 (0.1245)  loss_giou_unscaled: 0.5626 (0.5341)  cardinality_error_unscaled: 291.4167 (292.1485)  loss_ce_0_unscaled: 1.4177 (1.3728)  loss_bbox_0_unscaled: 0.1307 (0.1311)  loss_giou_0_unscaled: 0.5874 (0.5679)  cardinality_error_0_unscaled: 291.3333 (292.0297)  loss_ce_1_unscaled: 1.3958 (1.3595)  loss_bbox_1_unscaled: 0.1265 (0.1247)  loss_giou_1_unscaled: 0.5757 (0.5404)  cardinality_error_1_unscaled: 291.4167 (292.0635)  loss_ce_2_unscaled: 1.3952 (1.3583)  loss_bbox_2_unscaled: 0.1269 (0.1247)  loss_giou_2_unscaled: 0.5760 (0.5385)  cardinality_error_2_unscaled: 291.5000 (292.0982)  loss_ce_3_unscaled: 1.3740 (1.3371)  loss_bbox_3_unscaled: 0.1245 (0.1239)  loss_giou_3_unscaled: 0.5652 (0.5349)  cardinality_error_3_unscaled: 289.4167 (290.7954)  loss_ce_4_unscaled: 1.3838 (1.3450)  loss_bbox_4_unscaled: 0.1243 (0.1241)  loss_giou_4_unscaled: 0.5743 (0.5341)  cardinality_error_4_unscaled: 291.5000 (292.1163)  time: 3.0253  data: 0.0368  max mem: 8138\n",
            "Test:  [110/417]  eta: 0:15:32  class_error: 74.29  loss: 27.8465 (26.7059)  loss_ce: 2.8580 (2.7216)  loss_bbox: 0.6777 (0.6326)  loss_giou: 1.1237 (1.0754)  loss_ce_0: 2.8774 (2.7603)  loss_bbox_0: 0.7175 (0.6647)  loss_giou_0: 1.1699 (1.1427)  loss_ce_1: 2.8405 (2.7330)  loss_bbox_1: 0.6623 (0.6334)  loss_giou_1: 1.1515 (1.0875)  loss_ce_2: 2.8656 (2.7313)  loss_bbox_2: 0.6671 (0.6336)  loss_giou_2: 1.1440 (1.0834)  loss_ce_3: 2.8155 (2.6896)  loss_bbox_3: 0.6677 (0.6294)  loss_giou_3: 1.1304 (1.0769)  loss_ce_4: 2.8587 (2.7051)  loss_bbox_4: 0.6679 (0.6305)  loss_giou_4: 1.1155 (1.0752)  loss_ce_unscaled: 1.4290 (1.3608)  class_error_unscaled: 74.2857 (71.3895)  loss_bbox_unscaled: 0.1355 (0.1265)  loss_giou_unscaled: 0.5618 (0.5377)  cardinality_error_unscaled: 291.6667 (292.1502)  loss_ce_0_unscaled: 1.4387 (1.3801)  loss_bbox_0_unscaled: 0.1435 (0.1329)  loss_giou_0_unscaled: 0.5849 (0.5713)  cardinality_error_0_unscaled: 291.4167 (292.0308)  loss_ce_1_unscaled: 1.4203 (1.3665)  loss_bbox_1_unscaled: 0.1325 (0.1267)  loss_giou_1_unscaled: 0.5757 (0.5437)  cardinality_error_1_unscaled: 291.4167 (292.0743)  loss_ce_2_unscaled: 1.4328 (1.3656)  loss_bbox_2_unscaled: 0.1334 (0.1267)  loss_giou_2_unscaled: 0.5720 (0.5417)  cardinality_error_2_unscaled: 291.5000 (292.1051)  loss_ce_3_unscaled: 1.4077 (1.3448)  loss_bbox_3_unscaled: 0.1335 (0.1259)  loss_giou_3_unscaled: 0.5652 (0.5384)  cardinality_error_3_unscaled: 289.4167 (290.7951)  loss_ce_4_unscaled: 1.4294 (1.3526)  loss_bbox_4_unscaled: 0.1336 (0.1261)  loss_giou_4_unscaled: 0.5578 (0.5376)  cardinality_error_4_unscaled: 291.5000 (292.1126)  time: 2.9905  data: 0.0353  max mem: 8138\n",
            "Test:  [120/417]  eta: 0:14:57  class_error: 70.91  loss: 28.8873 (26.7967)  loss_ce: 2.8702 (2.7311)  loss_bbox: 0.6630 (0.6322)  loss_giou: 1.1879 (1.0813)  loss_ce_0: 2.9128 (2.7709)  loss_bbox_0: 0.6897 (0.6646)  loss_giou_0: 1.2421 (1.1467)  loss_ce_1: 2.8943 (2.7434)  loss_bbox_1: 0.6604 (0.6329)  loss_giou_1: 1.1844 (1.0925)  loss_ce_2: 2.8766 (2.7417)  loss_bbox_2: 0.6671 (0.6329)  loss_giou_2: 1.1823 (1.0890)  loss_ce_3: 2.8388 (2.6994)  loss_bbox_3: 0.6451 (0.6289)  loss_giou_3: 1.1807 (1.0827)  loss_ce_4: 2.8592 (2.7154)  loss_bbox_4: 0.6453 (0.6299)  loss_giou_4: 1.1899 (1.0812)  loss_ce_unscaled: 1.4351 (1.3655)  class_error_unscaled: 76.0870 (71.6827)  loss_bbox_unscaled: 0.1326 (0.1264)  loss_giou_unscaled: 0.5939 (0.5407)  cardinality_error_unscaled: 291.2500 (292.1019)  loss_ce_0_unscaled: 1.4564 (1.3855)  loss_bbox_0_unscaled: 0.1379 (0.1329)  loss_giou_0_unscaled: 0.6211 (0.5733)  cardinality_error_0_unscaled: 291.1667 (291.9532)  loss_ce_1_unscaled: 1.4472 (1.3717)  loss_bbox_1_unscaled: 0.1321 (0.1266)  loss_giou_1_unscaled: 0.5922 (0.5462)  cardinality_error_1_unscaled: 291.2500 (291.9993)  loss_ce_2_unscaled: 1.4383 (1.3709)  loss_bbox_2_unscaled: 0.1334 (0.1266)  loss_giou_2_unscaled: 0.5911 (0.5445)  cardinality_error_2_unscaled: 291.2500 (292.0448)  loss_ce_3_unscaled: 1.4194 (1.3497)  loss_bbox_3_unscaled: 0.1290 (0.1258)  loss_giou_3_unscaled: 0.5903 (0.5414)  cardinality_error_3_unscaled: 289.4167 (290.7204)  loss_ce_4_unscaled: 1.4296 (1.3577)  loss_bbox_4_unscaled: 0.1291 (0.1260)  loss_giou_4_unscaled: 0.5949 (0.5406)  cardinality_error_4_unscaled: 291.1667 (292.0351)  time: 2.9302  data: 0.0339  max mem: 8138\n",
            "Test:  [130/417]  eta: 0:14:25  class_error: 59.02  loss: 24.8350 (26.6472)  loss_ce: 2.5031 (2.7130)  loss_bbox: 0.6303 (0.6307)  loss_giou: 1.0807 (1.0762)  loss_ce_0: 2.6047 (2.7519)  loss_bbox_0: 0.6631 (0.6632)  loss_giou_0: 1.1460 (1.1417)  loss_ce_1: 2.5546 (2.7246)  loss_bbox_1: 0.6405 (0.6311)  loss_giou_1: 1.0907 (1.0873)  loss_ce_2: 2.5108 (2.7240)  loss_bbox_2: 0.6161 (0.6309)  loss_giou_2: 1.0885 (1.0838)  loss_ce_3: 2.4846 (2.6824)  loss_bbox_3: 0.6220 (0.6271)  loss_giou_3: 1.0843 (1.0780)  loss_ce_4: 2.4576 (2.6970)  loss_bbox_4: 0.6122 (0.6281)  loss_giou_4: 1.0851 (1.0764)  loss_ce_unscaled: 1.2515 (1.3565)  class_error_unscaled: 67.0886 (71.3327)  loss_bbox_unscaled: 0.1261 (0.1261)  loss_giou_unscaled: 0.5404 (0.5381)  cardinality_error_unscaled: 291.2500 (292.0719)  loss_ce_0_unscaled: 1.3023 (1.3760)  loss_bbox_0_unscaled: 0.1326 (0.1326)  loss_giou_0_unscaled: 0.5730 (0.5708)  cardinality_error_0_unscaled: 290.5833 (291.9141)  loss_ce_1_unscaled: 1.2773 (1.3623)  loss_bbox_1_unscaled: 0.1281 (0.1262)  loss_giou_1_unscaled: 0.5453 (0.5437)  cardinality_error_1_unscaled: 291.2500 (291.9606)  loss_ce_2_unscaled: 1.2554 (1.3620)  loss_bbox_2_unscaled: 0.1232 (0.1262)  loss_giou_2_unscaled: 0.5442 (0.5419)  cardinality_error_2_unscaled: 291.0833 (292.0216)  loss_ce_3_unscaled: 1.2423 (1.3412)  loss_bbox_3_unscaled: 0.1244 (0.1254)  loss_giou_3_unscaled: 0.5422 (0.5390)  cardinality_error_3_unscaled: 289.2500 (290.6915)  loss_ce_4_unscaled: 1.2288 (1.3485)  loss_bbox_4_unscaled: 0.1224 (0.1256)  loss_giou_4_unscaled: 0.5426 (0.5382)  cardinality_error_4_unscaled: 291.0000 (291.9956)  time: 2.9031  data: 0.0334  max mem: 8138\n",
            "Test:  [140/417]  eta: 0:13:56  class_error: 77.00  loss: 25.6447 (26.7099)  loss_ce: 2.5404 (2.7198)  loss_bbox: 0.6371 (0.6319)  loss_giou: 1.0807 (1.0792)  loss_ce_0: 2.5943 (2.7582)  loss_bbox_0: 0.6647 (0.6636)  loss_giou_0: 1.1312 (1.1438)  loss_ce_1: 2.5546 (2.7305)  loss_bbox_1: 0.6405 (0.6320)  loss_giou_1: 1.0907 (1.0907)  loss_ce_2: 2.5516 (2.7306)  loss_bbox_2: 0.6336 (0.6324)  loss_giou_2: 1.0885 (1.0868)  loss_ce_3: 2.5106 (2.6886)  loss_bbox_3: 0.6246 (0.6286)  loss_giou_3: 1.0843 (1.0810)  loss_ce_4: 2.5501 (2.7034)  loss_bbox_4: 0.6194 (0.6293)  loss_giou_4: 1.0851 (1.0795)  loss_ce_unscaled: 1.2702 (1.3599)  class_error_unscaled: 67.0886 (71.4843)  loss_bbox_unscaled: 0.1274 (0.1264)  loss_giou_unscaled: 0.5404 (0.5396)  cardinality_error_unscaled: 291.2500 (292.0219)  loss_ce_0_unscaled: 1.2971 (1.3791)  loss_bbox_0_unscaled: 0.1329 (0.1327)  loss_giou_0_unscaled: 0.5656 (0.5719)  cardinality_error_0_unscaled: 291.0000 (291.8659)  loss_ce_1_unscaled: 1.2773 (1.3652)  loss_bbox_1_unscaled: 0.1281 (0.1264)  loss_giou_1_unscaled: 0.5453 (0.5454)  cardinality_error_1_unscaled: 291.2500 (291.9054)  loss_ce_2_unscaled: 1.2758 (1.3653)  loss_bbox_2_unscaled: 0.1267 (0.1265)  loss_giou_2_unscaled: 0.5442 (0.5434)  cardinality_error_2_unscaled: 291.2500 (291.9669)  loss_ce_3_unscaled: 1.2553 (1.3443)  loss_bbox_3_unscaled: 0.1249 (0.1257)  loss_giou_3_unscaled: 0.5422 (0.5405)  cardinality_error_3_unscaled: 290.0833 (290.6419)  loss_ce_4_unscaled: 1.2750 (1.3517)  loss_bbox_4_unscaled: 0.1239 (0.1259)  loss_giou_4_unscaled: 0.5426 (0.5398)  cardinality_error_4_unscaled: 291.2500 (291.9521)  time: 3.0084  data: 0.0352  max mem: 8138\n",
            "Test:  [150/417]  eta: 0:13:25  class_error: 60.00  loss: 27.5283 (26.7386)  loss_ce: 2.8597 (2.7240)  loss_bbox: 0.6345 (0.6330)  loss_giou: 1.0926 (1.0783)  loss_ce_0: 2.9393 (2.7632)  loss_bbox_0: 0.6647 (0.6657)  loss_giou_0: 1.1066 (1.1428)  loss_ce_1: 2.9491 (2.7349)  loss_bbox_1: 0.6457 (0.6335)  loss_giou_1: 1.0919 (1.0898)  loss_ce_2: 2.9054 (2.7348)  loss_bbox_2: 0.6336 (0.6334)  loss_giou_2: 1.0891 (1.0863)  loss_ce_3: 2.8759 (2.6923)  loss_bbox_3: 0.6317 (0.6297)  loss_giou_3: 1.1043 (1.0803)  loss_ce_4: 2.8563 (2.7074)  loss_bbox_4: 0.6349 (0.6305)  loss_giou_4: 1.0937 (1.0787)  loss_ce_unscaled: 1.4298 (1.3620)  class_error_unscaled: 77.0000 (71.5802)  loss_bbox_unscaled: 0.1269 (0.1266)  loss_giou_unscaled: 0.5463 (0.5391)  cardinality_error_unscaled: 291.4167 (292.0133)  loss_ce_0_unscaled: 1.4696 (1.3816)  loss_bbox_0_unscaled: 0.1329 (0.1331)  loss_giou_0_unscaled: 0.5533 (0.5714)  cardinality_error_0_unscaled: 291.2500 (291.8516)  loss_ce_1_unscaled: 1.4745 (1.3674)  loss_bbox_1_unscaled: 0.1291 (0.1267)  loss_giou_1_unscaled: 0.5459 (0.5449)  cardinality_error_1_unscaled: 291.4167 (291.9029)  loss_ce_2_unscaled: 1.4527 (1.3674)  loss_bbox_2_unscaled: 0.1267 (0.1267)  loss_giou_2_unscaled: 0.5445 (0.5432)  cardinality_error_2_unscaled: 291.3333 (291.9686)  loss_ce_3_unscaled: 1.4380 (1.3462)  loss_bbox_3_unscaled: 0.1263 (0.1259)  loss_giou_3_unscaled: 0.5522 (0.5401)  cardinality_error_3_unscaled: 290.0833 (290.6214)  loss_ce_4_unscaled: 1.4282 (1.3537)  loss_bbox_4_unscaled: 0.1270 (0.1261)  loss_giou_4_unscaled: 0.5468 (0.5393)  cardinality_error_4_unscaled: 291.4167 (291.9542)  time: 3.0294  data: 0.0353  max mem: 8138\n",
            "Test:  [160/417]  eta: 0:12:54  class_error: 74.00  loss: 26.8990 (26.7551)  loss_ce: 2.7950 (2.7266)  loss_bbox: 0.6244 (0.6319)  loss_giou: 1.0658 (1.0791)  loss_ce_0: 2.8191 (2.7653)  loss_bbox_0: 0.6704 (0.6647)  loss_giou_0: 1.1080 (1.1436)  loss_ce_1: 2.8075 (2.7383)  loss_bbox_1: 0.6271 (0.6321)  loss_giou_1: 1.0919 (1.0912)  loss_ce_2: 2.8420 (2.7385)  loss_bbox_2: 0.6243 (0.6321)  loss_giou_2: 1.0766 (1.0874)  loss_ce_3: 2.7850 (2.6953)  loss_bbox_3: 0.6317 (0.6285)  loss_giou_3: 1.0673 (1.0813)  loss_ce_4: 2.8064 (2.7104)  loss_bbox_4: 0.6292 (0.6291)  loss_giou_4: 1.0666 (1.0798)  loss_ce_unscaled: 1.3975 (1.3633)  class_error_unscaled: 74.0385 (71.6809)  loss_bbox_unscaled: 0.1249 (0.1264)  loss_giou_unscaled: 0.5329 (0.5395)  cardinality_error_unscaled: 291.4167 (291.9757)  loss_ce_0_unscaled: 1.4095 (1.3827)  loss_bbox_0_unscaled: 0.1341 (0.1329)  loss_giou_0_unscaled: 0.5540 (0.5718)  cardinality_error_0_unscaled: 291.4167 (291.8209)  loss_ce_1_unscaled: 1.4038 (1.3691)  loss_bbox_1_unscaled: 0.1254 (0.1264)  loss_giou_1_unscaled: 0.5459 (0.5456)  cardinality_error_1_unscaled: 291.6667 (291.8763)  loss_ce_2_unscaled: 1.4210 (1.3692)  loss_bbox_2_unscaled: 0.1249 (0.1264)  loss_giou_2_unscaled: 0.5383 (0.5437)  cardinality_error_2_unscaled: 291.4167 (291.9358)  loss_ce_3_unscaled: 1.3925 (1.3476)  loss_bbox_3_unscaled: 0.1263 (0.1257)  loss_giou_3_unscaled: 0.5336 (0.5406)  cardinality_error_3_unscaled: 290.0000 (290.5435)  loss_ce_4_unscaled: 1.4032 (1.3552)  loss_bbox_4_unscaled: 0.1258 (0.1258)  loss_giou_4_unscaled: 0.5333 (0.5399)  cardinality_error_4_unscaled: 291.4167 (291.9208)  time: 2.9795  data: 0.0349  max mem: 8138\n",
            "Test:  [170/417]  eta: 0:12:22  class_error: 71.91  loss: 26.9772 (26.7457)  loss_ce: 2.7776 (2.7251)  loss_bbox: 0.5746 (0.6318)  loss_giou: 1.1296 (1.0786)  loss_ce_0: 2.8079 (2.7643)  loss_bbox_0: 0.6141 (0.6650)  loss_giou_0: 1.1697 (1.1439)  loss_ce_1: 2.7846 (2.7366)  loss_bbox_1: 0.5917 (0.6316)  loss_giou_1: 1.1381 (1.0911)  loss_ce_2: 2.8010 (2.7383)  loss_bbox_2: 0.5704 (0.6314)  loss_giou_2: 1.1315 (1.0873)  loss_ce_3: 2.7783 (2.6947)  loss_bbox_3: 0.5721 (0.6281)  loss_giou_3: 1.1332 (1.0807)  loss_ce_4: 2.7756 (2.7093)  loss_bbox_4: 0.5732 (0.6288)  loss_giou_4: 1.1299 (1.0792)  loss_ce_unscaled: 1.3888 (1.3626)  class_error_unscaled: 73.0159 (71.6065)  loss_bbox_unscaled: 0.1149 (0.1264)  loss_giou_unscaled: 0.5648 (0.5393)  cardinality_error_unscaled: 291.2500 (291.8908)  loss_ce_0_unscaled: 1.4039 (1.3822)  loss_bbox_0_unscaled: 0.1228 (0.1330)  loss_giou_0_unscaled: 0.5849 (0.5720)  cardinality_error_0_unscaled: 291.4167 (291.7291)  loss_ce_1_unscaled: 1.3923 (1.3683)  loss_bbox_1_unscaled: 0.1183 (0.1263)  loss_giou_1_unscaled: 0.5691 (0.5456)  cardinality_error_1_unscaled: 291.5000 (291.7997)  loss_ce_2_unscaled: 1.4005 (1.3691)  loss_bbox_2_unscaled: 0.1141 (0.1263)  loss_giou_2_unscaled: 0.5658 (0.5436)  cardinality_error_2_unscaled: 291.5000 (291.8558)  loss_ce_3_unscaled: 1.3891 (1.3474)  loss_bbox_3_unscaled: 0.1144 (0.1256)  loss_giou_3_unscaled: 0.5666 (0.5404)  cardinality_error_3_unscaled: 289.9167 (290.4449)  loss_ce_4_unscaled: 1.3878 (1.3546)  loss_bbox_4_unscaled: 0.1146 (0.1258)  loss_giou_4_unscaled: 0.5649 (0.5396)  cardinality_error_4_unscaled: 291.1667 (291.8212)  time: 2.9063  data: 0.0344  max mem: 8138\n",
            "Test:  [180/417]  eta: 0:11:54  class_error: 56.45  loss: 26.5801 (26.5751)  loss_ce: 2.6462 (2.7077)  loss_bbox: 0.5644 (0.6285)  loss_giou: 0.9667 (1.0707)  loss_ce_0: 2.7069 (2.7464)  loss_bbox_0: 0.6039 (0.6628)  loss_giou_0: 1.0256 (1.1374)  loss_ce_1: 2.6466 (2.7196)  loss_bbox_1: 0.5595 (0.6285)  loss_giou_1: 0.9544 (1.0833)  loss_ce_2: 2.6900 (2.7204)  loss_bbox_2: 0.5607 (0.6281)  loss_giou_2: 0.9750 (1.0792)  loss_ce_3: 2.6316 (2.6768)  loss_bbox_3: 0.5626 (0.6248)  loss_giou_3: 0.9654 (1.0726)  loss_ce_4: 2.6590 (2.6911)  loss_bbox_4: 0.5627 (0.6257)  loss_giou_4: 0.9672 (1.0714)  loss_ce_unscaled: 1.3231 (1.3539)  class_error_unscaled: 69.0266 (71.1111)  loss_bbox_unscaled: 0.1129 (0.1257)  loss_giou_unscaled: 0.4834 (0.5353)  cardinality_error_unscaled: 291.5833 (291.8670)  loss_ce_0_unscaled: 1.3534 (1.3732)  loss_bbox_0_unscaled: 0.1208 (0.1326)  loss_giou_0_unscaled: 0.5128 (0.5687)  cardinality_error_0_unscaled: 291.4167 (291.6823)  loss_ce_1_unscaled: 1.3233 (1.3598)  loss_bbox_1_unscaled: 0.1119 (0.1257)  loss_giou_1_unscaled: 0.4772 (0.5416)  cardinality_error_1_unscaled: 291.5833 (291.7740)  loss_ce_2_unscaled: 1.3450 (1.3602)  loss_bbox_2_unscaled: 0.1121 (0.1256)  loss_giou_2_unscaled: 0.4875 (0.5396)  cardinality_error_2_unscaled: 291.6667 (291.8329)  loss_ce_3_unscaled: 1.3158 (1.3384)  loss_bbox_3_unscaled: 0.1125 (0.1250)  loss_giou_3_unscaled: 0.4827 (0.5363)  cardinality_error_3_unscaled: 289.7500 (290.3379)  loss_ce_4_unscaled: 1.3295 (1.3456)  loss_bbox_4_unscaled: 0.1125 (0.1251)  loss_giou_4_unscaled: 0.4836 (0.5357)  cardinality_error_4_unscaled: 291.5000 (291.7740)  time: 2.9921  data: 0.0333  max mem: 8138\n",
            "Test:  [190/417]  eta: 0:11:23  class_error: 85.00  loss: 26.8202 (26.7001)  loss_ce: 2.7445 (2.7243)  loss_bbox: 0.6021 (0.6314)  loss_giou: 0.9672 (1.0729)  loss_ce_0: 2.8118 (2.7626)  loss_bbox_0: 0.6176 (0.6653)  loss_giou_0: 1.0613 (1.1388)  loss_ce_1: 2.7996 (2.7348)  loss_bbox_1: 0.5723 (0.6310)  loss_giou_1: 0.9954 (1.0856)  loss_ce_2: 2.7547 (2.7363)  loss_bbox_2: 0.5790 (0.6312)  loss_giou_2: 0.9769 (1.0812)  loss_ce_3: 2.7071 (2.6923)  loss_bbox_3: 0.5900 (0.6280)  loss_giou_3: 0.9666 (1.0747)  loss_ce_4: 2.7293 (2.7075)  loss_bbox_4: 0.6020 (0.6287)  loss_giou_4: 0.9696 (1.0735)  loss_ce_unscaled: 1.3723 (1.3621)  class_error_unscaled: 71.8750 (71.5689)  loss_bbox_unscaled: 0.1204 (0.1263)  loss_giou_unscaled: 0.4836 (0.5365)  cardinality_error_unscaled: 292.6667 (291.9080)  loss_ce_0_unscaled: 1.4059 (1.3813)  loss_bbox_0_unscaled: 0.1235 (0.1331)  loss_giou_0_unscaled: 0.5307 (0.5694)  cardinality_error_0_unscaled: 292.6667 (291.7339)  loss_ce_1_unscaled: 1.3998 (1.3674)  loss_bbox_1_unscaled: 0.1145 (0.1262)  loss_giou_1_unscaled: 0.4977 (0.5428)  cardinality_error_1_unscaled: 292.9167 (291.8141)  loss_ce_2_unscaled: 1.3774 (1.3682)  loss_bbox_2_unscaled: 0.1158 (0.1262)  loss_giou_2_unscaled: 0.4884 (0.5406)  cardinality_error_2_unscaled: 293.0000 (291.8700)  loss_ce_3_unscaled: 1.3535 (1.3461)  loss_bbox_3_unscaled: 0.1180 (0.1256)  loss_giou_3_unscaled: 0.4833 (0.5374)  cardinality_error_3_unscaled: 290.9167 (290.4049)  loss_ce_4_unscaled: 1.3647 (1.3538)  loss_bbox_4_unscaled: 0.1204 (0.1257)  loss_giou_4_unscaled: 0.4848 (0.5367)  cardinality_error_4_unscaled: 292.5000 (291.8137)  time: 3.0641  data: 0.0340  max mem: 8138\n",
            "Test:  [200/417]  eta: 0:10:52  class_error: 76.36  loss: 28.5221 (26.7596)  loss_ce: 2.9322 (2.7297)  loss_bbox: 0.6509 (0.6338)  loss_giou: 1.1262 (1.0745)  loss_ce_0: 3.0001 (2.7685)  loss_bbox_0: 0.7000 (0.6680)  loss_giou_0: 1.1526 (1.1406)  loss_ce_1: 2.9692 (2.7404)  loss_bbox_1: 0.6432 (0.6340)  loss_giou_1: 1.1185 (1.0872)  loss_ce_2: 2.9816 (2.7413)  loss_bbox_2: 0.6540 (0.6339)  loss_giou_2: 1.1240 (1.0829)  loss_ce_3: 2.9477 (2.6983)  loss_bbox_3: 0.6494 (0.6305)  loss_giou_3: 1.1302 (1.0764)  loss_ce_4: 2.9721 (2.7135)  loss_bbox_4: 0.6477 (0.6312)  loss_giou_4: 1.1249 (1.0751)  loss_ce_unscaled: 1.4661 (1.3648)  class_error_unscaled: 76.9231 (71.7520)  loss_bbox_unscaled: 0.1302 (0.1268)  loss_giou_unscaled: 0.5631 (0.5372)  cardinality_error_unscaled: 292.6667 (291.9345)  loss_ce_0_unscaled: 1.5001 (1.3842)  loss_bbox_0_unscaled: 0.1400 (0.1336)  loss_giou_0_unscaled: 0.5763 (0.5703)  cardinality_error_0_unscaled: 291.5833 (291.7542)  loss_ce_1_unscaled: 1.4846 (1.3702)  loss_bbox_1_unscaled: 0.1286 (0.1268)  loss_giou_1_unscaled: 0.5592 (0.5436)  cardinality_error_1_unscaled: 292.5000 (291.8387)  loss_ce_2_unscaled: 1.4908 (1.3707)  loss_bbox_2_unscaled: 0.1308 (0.1268)  loss_giou_2_unscaled: 0.5620 (0.5414)  cardinality_error_2_unscaled: 292.5833 (291.8976)  loss_ce_3_unscaled: 1.4738 (1.3491)  loss_bbox_3_unscaled: 0.1299 (0.1261)  loss_giou_3_unscaled: 0.5651 (0.5382)  cardinality_error_3_unscaled: 290.9167 (290.4121)  loss_ce_4_unscaled: 1.4861 (1.3568)  loss_bbox_4_unscaled: 0.1295 (0.1262)  loss_giou_4_unscaled: 0.5625 (0.5375)  cardinality_error_4_unscaled: 292.1667 (291.8309)  time: 2.9464  data: 0.0336  max mem: 8138\n",
            "Test:  [210/417]  eta: 0:10:21  class_error: 70.27  loss: 27.0917 (26.7678)  loss_ce: 2.8374 (2.7314)  loss_bbox: 0.6433 (0.6316)  loss_giou: 1.1005 (1.0764)  loss_ce_0: 2.8370 (2.7698)  loss_bbox_0: 0.6802 (0.6659)  loss_giou_0: 1.1742 (1.1425)  loss_ce_1: 2.8322 (2.7418)  loss_bbox_1: 0.6432 (0.6316)  loss_giou_1: 1.1001 (1.0889)  loss_ce_2: 2.8257 (2.7433)  loss_bbox_2: 0.6540 (0.6317)  loss_giou_2: 1.1136 (1.0848)  loss_ce_3: 2.7979 (2.7003)  loss_bbox_3: 0.6373 (0.6285)  loss_giou_3: 1.1003 (1.0781)  loss_ce_4: 2.8141 (2.7153)  loss_bbox_4: 0.6427 (0.6290)  loss_giou_4: 1.0992 (1.0768)  loss_ce_unscaled: 1.4187 (1.3657)  class_error_unscaled: 72.5490 (71.7395)  loss_bbox_unscaled: 0.1287 (0.1263)  loss_giou_unscaled: 0.5502 (0.5382)  cardinality_error_unscaled: 291.4167 (291.9139)  loss_ce_0_unscaled: 1.4185 (1.3849)  loss_bbox_0_unscaled: 0.1360 (0.1332)  loss_giou_0_unscaled: 0.5871 (0.5713)  cardinality_error_0_unscaled: 291.4167 (291.7386)  loss_ce_1_unscaled: 1.4161 (1.3709)  loss_bbox_1_unscaled: 0.1286 (0.1263)  loss_giou_1_unscaled: 0.5500 (0.5445)  cardinality_error_1_unscaled: 291.3333 (291.8227)  loss_ce_2_unscaled: 1.4129 (1.3717)  loss_bbox_2_unscaled: 0.1308 (0.1263)  loss_giou_2_unscaled: 0.5568 (0.5424)  cardinality_error_2_unscaled: 291.5833 (291.8768)  loss_ce_3_unscaled: 1.3990 (1.3502)  loss_bbox_3_unscaled: 0.1275 (0.1257)  loss_giou_3_unscaled: 0.5501 (0.5390)  cardinality_error_3_unscaled: 290.1667 (290.3954)  loss_ce_4_unscaled: 1.4070 (1.3577)  loss_bbox_4_unscaled: 0.1285 (0.1258)  loss_giou_4_unscaled: 0.5496 (0.5384)  cardinality_error_4_unscaled: 291.4167 (291.8132)  time: 2.9244  data: 0.0331  max mem: 8138\n",
            "Test:  [220/417]  eta: 0:09:51  class_error: 75.70  loss: 27.0917 (26.7662)  loss_ce: 2.7322 (2.7293)  loss_bbox: 0.5935 (0.6300)  loss_giou: 1.1186 (1.0804)  loss_ce_0: 2.8058 (2.7670)  loss_bbox_0: 0.6020 (0.6637)  loss_giou_0: 1.1870 (1.1453)  loss_ce_1: 2.7545 (2.7392)  loss_bbox_1: 0.5829 (0.6300)  loss_giou_1: 1.1509 (1.0926)  loss_ce_2: 2.7658 (2.7412)  loss_bbox_2: 0.5830 (0.6300)  loss_giou_2: 1.1363 (1.0887)  loss_ce_3: 2.7305 (2.6983)  loss_bbox_3: 0.5819 (0.6270)  loss_giou_3: 1.1379 (1.0820)  loss_ce_4: 2.7202 (2.7130)  loss_bbox_4: 0.5893 (0.6275)  loss_giou_4: 1.1279 (1.0808)  loss_ce_unscaled: 1.3661 (1.3647)  class_error_unscaled: 72.5490 (71.7338)  loss_bbox_unscaled: 0.1187 (0.1260)  loss_giou_unscaled: 0.5593 (0.5402)  cardinality_error_unscaled: 291.3333 (291.8688)  loss_ce_0_unscaled: 1.4029 (1.3835)  loss_bbox_0_unscaled: 0.1204 (0.1327)  loss_giou_0_unscaled: 0.5935 (0.5727)  cardinality_error_0_unscaled: 291.3333 (291.7029)  loss_ce_1_unscaled: 1.3772 (1.3696)  loss_bbox_1_unscaled: 0.1166 (0.1260)  loss_giou_1_unscaled: 0.5754 (0.5463)  cardinality_error_1_unscaled: 291.3333 (291.7813)  loss_ce_2_unscaled: 1.3829 (1.3706)  loss_bbox_2_unscaled: 0.1166 (0.1260)  loss_giou_2_unscaled: 0.5682 (0.5444)  cardinality_error_2_unscaled: 291.0833 (291.8337)  loss_ce_3_unscaled: 1.3653 (1.3491)  loss_bbox_3_unscaled: 0.1164 (0.1254)  loss_giou_3_unscaled: 0.5690 (0.5410)  cardinality_error_3_unscaled: 290.1667 (290.3552)  loss_ce_4_unscaled: 1.3601 (1.3565)  loss_bbox_4_unscaled: 0.1179 (0.1255)  loss_giou_4_unscaled: 0.5639 (0.5404)  cardinality_error_4_unscaled: 291.1667 (291.7643)  time: 2.9868  data: 0.0339  max mem: 8138\n",
            "Test:  [230/417]  eta: 0:09:20  class_error: 76.25  loss: 27.4291 (26.8096)  loss_ce: 2.7613 (2.7365)  loss_bbox: 0.5783 (0.6280)  loss_giou: 1.1174 (1.0821)  loss_ce_0: 2.8372 (2.7744)  loss_bbox_0: 0.6020 (0.6623)  loss_giou_0: 1.1669 (1.1469)  loss_ce_1: 2.8183 (2.7469)  loss_bbox_1: 0.5829 (0.6287)  loss_giou_1: 1.1419 (1.0945)  loss_ce_2: 2.8151 (2.7484)  loss_bbox_2: 0.5830 (0.6281)  loss_giou_2: 1.1353 (1.0908)  loss_ce_3: 2.7641 (2.7052)  loss_bbox_3: 0.5819 (0.6251)  loss_giou_3: 1.1237 (1.0838)  loss_ce_4: 2.7372 (2.7197)  loss_bbox_4: 0.5893 (0.6257)  loss_giou_4: 1.1241 (1.0824)  loss_ce_unscaled: 1.3807 (1.3682)  class_error_unscaled: 74.4681 (71.9049)  loss_bbox_unscaled: 0.1157 (0.1256)  loss_giou_unscaled: 0.5587 (0.5410)  cardinality_error_unscaled: 291.4167 (291.8124)  loss_ce_0_unscaled: 1.4186 (1.3872)  loss_bbox_0_unscaled: 0.1204 (0.1325)  loss_giou_0_unscaled: 0.5835 (0.5735)  cardinality_error_0_unscaled: 290.6667 (291.6194)  loss_ce_1_unscaled: 1.4091 (1.3735)  loss_bbox_1_unscaled: 0.1166 (0.1257)  loss_giou_1_unscaled: 0.5709 (0.5472)  cardinality_error_1_unscaled: 291.0833 (291.7028)  loss_ce_2_unscaled: 1.4076 (1.3742)  loss_bbox_2_unscaled: 0.1166 (0.1256)  loss_giou_2_unscaled: 0.5677 (0.5454)  cardinality_error_2_unscaled: 291.4167 (291.7634)  loss_ce_3_unscaled: 1.3821 (1.3526)  loss_bbox_3_unscaled: 0.1164 (0.1250)  loss_giou_3_unscaled: 0.5618 (0.5419)  cardinality_error_3_unscaled: 290.7500 (290.3034)  loss_ce_4_unscaled: 1.3686 (1.3599)  loss_bbox_4_unscaled: 0.1179 (0.1251)  loss_giou_4_unscaled: 0.5621 (0.5412)  cardinality_error_4_unscaled: 291.1667 (291.7136)  time: 2.9482  data: 0.0333  max mem: 8138\n",
            "Test:  [240/417]  eta: 0:08:50  class_error: 80.58  loss: 25.7241 (26.7497)  loss_ce: 2.7613 (2.7280)  loss_bbox: 0.5783 (0.6280)  loss_giou: 1.0339 (1.0804)  loss_ce_0: 2.8034 (2.7658)  loss_bbox_0: 0.6029 (0.6623)  loss_giou_0: 1.1407 (1.1455)  loss_ce_1: 2.8053 (2.7387)  loss_bbox_1: 0.5998 (0.6288)  loss_giou_1: 1.0460 (1.0931)  loss_ce_2: 2.7816 (2.7399)  loss_bbox_2: 0.5982 (0.6282)  loss_giou_2: 1.0370 (1.0892)  loss_ce_3: 2.7341 (2.6967)  loss_bbox_3: 0.5891 (0.6251)  loss_giou_3: 1.0299 (1.0820)  loss_ce_4: 2.7372 (2.7116)  loss_bbox_4: 0.5903 (0.6256)  loss_giou_4: 1.0341 (1.0808)  loss_ce_unscaled: 1.3807 (1.3640)  class_error_unscaled: 71.5909 (71.7056)  loss_bbox_unscaled: 0.1157 (0.1256)  loss_giou_unscaled: 0.5170 (0.5402)  cardinality_error_unscaled: 291.9167 (291.8112)  loss_ce_0_unscaled: 1.4017 (1.3829)  loss_bbox_0_unscaled: 0.1206 (0.1325)  loss_giou_0_unscaled: 0.5703 (0.5728)  cardinality_error_0_unscaled: 290.6667 (291.6124)  loss_ce_1_unscaled: 1.4026 (1.3694)  loss_bbox_1_unscaled: 0.1200 (0.1258)  loss_giou_1_unscaled: 0.5230 (0.5465)  cardinality_error_1_unscaled: 291.3333 (291.7061)  loss_ce_2_unscaled: 1.3908 (1.3699)  loss_bbox_2_unscaled: 0.1196 (0.1256)  loss_giou_2_unscaled: 0.5185 (0.5446)  cardinality_error_2_unscaled: 291.4167 (291.7618)  loss_ce_3_unscaled: 1.3671 (1.3484)  loss_bbox_3_unscaled: 0.1178 (0.1250)  loss_giou_3_unscaled: 0.5150 (0.5410)  cardinality_error_3_unscaled: 291.1667 (290.3192)  loss_ce_4_unscaled: 1.3686 (1.3558)  loss_bbox_4_unscaled: 0.1181 (0.1251)  loss_giou_4_unscaled: 0.5171 (0.5404)  cardinality_error_4_unscaled: 291.5000 (291.7123)  time: 2.9401  data: 0.0331  max mem: 8138\n",
            "Test:  [250/417]  eta: 0:08:21  class_error: 60.38  loss: 25.2501 (26.7072)  loss_ce: 2.7002 (2.7270)  loss_bbox: 0.5702 (0.6249)  loss_giou: 0.9929 (1.0769)  loss_ce_0: 2.7119 (2.7650)  loss_bbox_0: 0.5996 (0.6596)  loss_giou_0: 1.0964 (1.1439)  loss_ce_1: 2.7060 (2.7379)  loss_bbox_1: 0.5543 (0.6257)  loss_giou_1: 1.0035 (1.0902)  loss_ce_2: 2.7269 (2.7389)  loss_bbox_2: 0.5687 (0.6250)  loss_giou_2: 0.9895 (1.0860)  loss_ce_3: 2.6815 (2.6954)  loss_bbox_3: 0.5730 (0.6221)  loss_giou_3: 0.9854 (1.0786)  loss_ce_4: 2.6794 (2.7102)  loss_bbox_4: 0.5666 (0.6224)  loss_giou_4: 0.9847 (1.0774)  loss_ce_unscaled: 1.3501 (1.3635)  class_error_unscaled: 71.0145 (71.6537)  loss_bbox_unscaled: 0.1140 (0.1250)  loss_giou_unscaled: 0.4965 (0.5385)  cardinality_error_unscaled: 291.9167 (291.8274)  loss_ce_0_unscaled: 1.3559 (1.3825)  loss_bbox_0_unscaled: 0.1199 (0.1319)  loss_giou_0_unscaled: 0.5482 (0.5720)  cardinality_error_0_unscaled: 291.4167 (291.6375)  loss_ce_1_unscaled: 1.3530 (1.3689)  loss_bbox_1_unscaled: 0.1109 (0.1251)  loss_giou_1_unscaled: 0.5018 (0.5451)  cardinality_error_1_unscaled: 291.7500 (291.7168)  loss_ce_2_unscaled: 1.3634 (1.3695)  loss_bbox_2_unscaled: 0.1137 (0.1250)  loss_giou_2_unscaled: 0.4947 (0.5430)  cardinality_error_2_unscaled: 291.4167 (291.7889)  loss_ce_3_unscaled: 1.3408 (1.3477)  loss_bbox_3_unscaled: 0.1146 (0.1244)  loss_giou_3_unscaled: 0.4927 (0.5393)  cardinality_error_3_unscaled: 290.2500 (290.3327)  loss_ce_4_unscaled: 1.3397 (1.3551)  loss_bbox_4_unscaled: 0.1133 (0.1245)  loss_giou_4_unscaled: 0.4923 (0.5387)  cardinality_error_4_unscaled: 291.5000 (291.7401)  time: 3.0367  data: 0.0339  max mem: 8138\n",
            "Test:  [260/417]  eta: 0:07:50  class_error: 85.00  loss: 25.2140 (26.6784)  loss_ce: 2.7002 (2.7247)  loss_bbox: 0.5863 (0.6241)  loss_giou: 1.0075 (1.0747)  loss_ce_0: 2.7119 (2.7627)  loss_bbox_0: 0.6219 (0.6597)  loss_giou_0: 1.0964 (1.1418)  loss_ce_1: 2.7060 (2.7360)  loss_bbox_1: 0.5921 (0.6251)  loss_giou_1: 1.0278 (1.0879)  loss_ce_2: 2.7269 (2.7374)  loss_bbox_2: 0.5840 (0.6244)  loss_giou_2: 1.0183 (1.0836)  loss_ce_3: 2.6698 (2.6936)  loss_bbox_3: 0.5844 (0.6216)  loss_giou_3: 1.0101 (1.0760)  loss_ce_4: 2.6794 (2.7081)  loss_bbox_4: 0.5721 (0.6216)  loss_giou_4: 1.0097 (1.0752)  loss_ce_unscaled: 1.3501 (1.3624)  class_error_unscaled: 69.8113 (71.5907)  loss_bbox_unscaled: 0.1173 (0.1248)  loss_giou_unscaled: 0.5037 (0.5374)  cardinality_error_unscaled: 292.6667 (291.8484)  loss_ce_0_unscaled: 1.3559 (1.3814)  loss_bbox_0_unscaled: 0.1244 (0.1319)  loss_giou_0_unscaled: 0.5482 (0.5709)  cardinality_error_0_unscaled: 292.0833 (291.6533)  loss_ce_1_unscaled: 1.3530 (1.3680)  loss_bbox_1_unscaled: 0.1184 (0.1250)  loss_giou_1_unscaled: 0.5139 (0.5440)  cardinality_error_1_unscaled: 292.0833 (291.7331)  loss_ce_2_unscaled: 1.3634 (1.3687)  loss_bbox_2_unscaled: 0.1168 (0.1249)  loss_giou_2_unscaled: 0.5091 (0.5418)  cardinality_error_2_unscaled: 292.1667 (291.8021)  loss_ce_3_unscaled: 1.3349 (1.3468)  loss_bbox_3_unscaled: 0.1169 (0.1243)  loss_giou_3_unscaled: 0.5050 (0.5380)  cardinality_error_3_unscaled: 291.5833 (290.3302)  loss_ce_4_unscaled: 1.3397 (1.3541)  loss_bbox_4_unscaled: 0.1144 (0.1243)  loss_giou_4_unscaled: 0.5049 (0.5376)  cardinality_error_4_unscaled: 292.0833 (291.7577)  time: 3.0074  data: 0.0337  max mem: 8138\n",
            "Test:  [270/417]  eta: 0:07:21  class_error: 69.23  loss: 25.1142 (26.6011)  loss_ce: 2.6748 (2.7178)  loss_bbox: 0.6036 (0.6222)  loss_giou: 0.9461 (1.0705)  loss_ce_0: 2.6805 (2.7555)  loss_bbox_0: 0.6339 (0.6581)  loss_giou_0: 1.0463 (1.1386)  loss_ce_1: 2.6449 (2.7286)  loss_bbox_1: 0.5983 (0.6233)  loss_giou_1: 0.9683 (1.0841)  loss_ce_2: 2.6552 (2.7305)  loss_bbox_2: 0.6020 (0.6226)  loss_giou_2: 0.9475 (1.0793)  loss_ce_3: 2.6191 (2.6865)  loss_bbox_3: 0.6104 (0.6200)  loss_giou_3: 0.9452 (1.0718)  loss_ce_4: 2.6270 (2.7010)  loss_bbox_4: 0.5983 (0.6199)  loss_giou_4: 0.9516 (1.0709)  loss_ce_unscaled: 1.3374 (1.3589)  class_error_unscaled: 66.3158 (71.4067)  loss_bbox_unscaled: 0.1207 (0.1244)  loss_giou_unscaled: 0.4731 (0.5353)  cardinality_error_unscaled: 292.0833 (291.7915)  loss_ce_0_unscaled: 1.3403 (1.3777)  loss_bbox_0_unscaled: 0.1268 (0.1316)  loss_giou_0_unscaled: 0.5231 (0.5693)  cardinality_error_0_unscaled: 291.2500 (291.5999)  loss_ce_1_unscaled: 1.3224 (1.3643)  loss_bbox_1_unscaled: 0.1197 (0.1247)  loss_giou_1_unscaled: 0.4841 (0.5420)  cardinality_error_1_unscaled: 292.0833 (291.6762)  loss_ce_2_unscaled: 1.3276 (1.3652)  loss_bbox_2_unscaled: 0.1204 (0.1245)  loss_giou_2_unscaled: 0.4737 (0.5397)  cardinality_error_2_unscaled: 292.0833 (291.7414)  loss_ce_3_unscaled: 1.3095 (1.3432)  loss_bbox_3_unscaled: 0.1221 (0.1240)  loss_giou_3_unscaled: 0.4726 (0.5359)  cardinality_error_3_unscaled: 291.0833 (290.2494)  loss_ce_4_unscaled: 1.3135 (1.3505)  loss_bbox_4_unscaled: 0.1197 (0.1240)  loss_giou_4_unscaled: 0.4758 (0.5355)  cardinality_error_4_unscaled: 292.0833 (291.6947)  time: 3.0002  data: 0.0337  max mem: 8138\n",
            "Test:  [280/417]  eta: 0:06:51  class_error: 73.13  loss: 24.9037 (26.5714)  loss_ce: 2.5143 (2.7123)  loss_bbox: 0.5935 (0.6213)  loss_giou: 1.0437 (1.0724)  loss_ce_0: 2.5671 (2.7494)  loss_bbox_0: 0.6283 (0.6568)  loss_giou_0: 1.1222 (1.1400)  loss_ce_1: 2.5617 (2.7228)  loss_bbox_1: 0.5908 (0.6226)  loss_giou_1: 1.0704 (1.0858)  loss_ce_2: 2.5902 (2.7243)  loss_bbox_2: 0.5813 (0.6218)  loss_giou_2: 1.0517 (1.0811)  loss_ce_3: 2.5086 (2.6807)  loss_bbox_3: 0.5771 (0.6193)  loss_giou_3: 1.0483 (1.0736)  loss_ce_4: 2.4904 (2.6953)  loss_bbox_4: 0.5866 (0.6192)  loss_giou_4: 1.0484 (1.0727)  loss_ce_unscaled: 1.2571 (1.3561)  class_error_unscaled: 63.7931 (71.2272)  loss_bbox_unscaled: 0.1187 (0.1243)  loss_giou_unscaled: 0.5219 (0.5362)  cardinality_error_unscaled: 290.8333 (291.7408)  loss_ce_0_unscaled: 1.2836 (1.3747)  loss_bbox_0_unscaled: 0.1257 (0.1314)  loss_giou_0_unscaled: 0.5611 (0.5700)  cardinality_error_0_unscaled: 290.5833 (291.5454)  loss_ce_1_unscaled: 1.2808 (1.3614)  loss_bbox_1_unscaled: 0.1182 (0.1245)  loss_giou_1_unscaled: 0.5352 (0.5429)  cardinality_error_1_unscaled: 290.6667 (291.6228)  loss_ce_2_unscaled: 1.2951 (1.3622)  loss_bbox_2_unscaled: 0.1163 (0.1244)  loss_giou_2_unscaled: 0.5259 (0.5406)  cardinality_error_2_unscaled: 290.8333 (291.6889)  loss_ce_3_unscaled: 1.2543 (1.3403)  loss_bbox_3_unscaled: 0.1154 (0.1239)  loss_giou_3_unscaled: 0.5242 (0.5368)  cardinality_error_3_unscaled: 288.5833 (290.1957)  loss_ce_4_unscaled: 1.2452 (1.3477)  loss_bbox_4_unscaled: 0.1173 (0.1238)  loss_giou_4_unscaled: 0.5242 (0.5363)  cardinality_error_4_unscaled: 290.6667 (291.6397)  time: 3.0369  data: 0.0327  max mem: 8138\n",
            "Test:  [290/417]  eta: 0:06:21  class_error: 75.76  loss: 27.4330 (26.6023)  loss_ce: 2.7882 (2.7150)  loss_bbox: 0.5965 (0.6236)  loss_giou: 1.0807 (1.0723)  loss_ce_0: 2.8485 (2.7522)  loss_bbox_0: 0.6473 (0.6594)  loss_giou_0: 1.1234 (1.1401)  loss_ce_1: 2.8124 (2.7256)  loss_bbox_1: 0.6232 (0.6254)  loss_giou_1: 1.0743 (1.0858)  loss_ce_2: 2.8195 (2.7269)  loss_bbox_2: 0.6001 (0.6244)  loss_giou_2: 1.0929 (1.0809)  loss_ce_3: 2.7871 (2.6833)  loss_bbox_3: 0.6010 (0.6217)  loss_giou_3: 1.0781 (1.0734)  loss_ce_4: 2.7708 (2.6983)  loss_bbox_4: 0.6010 (0.6215)  loss_giou_4: 1.0778 (1.0725)  loss_ce_unscaled: 1.3941 (1.3575)  class_error_unscaled: 70.5357 (71.2907)  loss_bbox_unscaled: 0.1193 (0.1247)  loss_giou_unscaled: 0.5403 (0.5361)  cardinality_error_unscaled: 291.5833 (291.7924)  loss_ce_0_unscaled: 1.4242 (1.3761)  loss_bbox_0_unscaled: 0.1295 (0.1319)  loss_giou_0_unscaled: 0.5617 (0.5700)  cardinality_error_0_unscaled: 291.3333 (291.6000)  loss_ce_1_unscaled: 1.4062 (1.3628)  loss_bbox_1_unscaled: 0.1246 (0.1251)  loss_giou_1_unscaled: 0.5371 (0.5429)  cardinality_error_1_unscaled: 291.5833 (291.6753)  loss_ce_2_unscaled: 1.4098 (1.3634)  loss_bbox_2_unscaled: 0.1200 (0.1249)  loss_giou_2_unscaled: 0.5465 (0.5405)  cardinality_error_2_unscaled: 291.5000 (291.7437)  loss_ce_3_unscaled: 1.3936 (1.3416)  loss_bbox_3_unscaled: 0.1202 (0.1243)  loss_giou_3_unscaled: 0.5391 (0.5367)  cardinality_error_3_unscaled: 290.5833 (290.2469)  loss_ce_4_unscaled: 1.3854 (1.3492)  loss_bbox_4_unscaled: 0.1202 (0.1243)  loss_giou_4_unscaled: 0.5389 (0.5363)  cardinality_error_4_unscaled: 291.4167 (291.6930)  time: 2.9911  data: 0.0327  max mem: 8138\n",
            "Test:  [300/417]  eta: 0:05:50  class_error: 59.62  loss: 27.7320 (26.6143)  loss_ce: 2.8595 (2.7152)  loss_bbox: 0.6179 (0.6238)  loss_giou: 1.0598 (1.0743)  loss_ce_0: 2.8976 (2.7523)  loss_bbox_0: 0.6585 (0.6593)  loss_giou_0: 1.1380 (1.1418)  loss_ce_1: 2.8418 (2.7253)  loss_bbox_1: 0.6379 (0.6255)  loss_giou_1: 1.0681 (1.0879)  loss_ce_2: 2.8751 (2.7264)  loss_bbox_2: 0.6353 (0.6246)  loss_giou_2: 1.0481 (1.0831)  loss_ce_3: 2.8222 (2.6831)  loss_bbox_3: 0.6193 (0.6218)  loss_giou_3: 1.0644 (1.0755)  loss_ce_4: 2.8598 (2.6985)  loss_bbox_4: 0.6168 (0.6215)  loss_giou_4: 1.0567 (1.0747)  loss_ce_unscaled: 1.4298 (1.3576)  class_error_unscaled: 74.3590 (71.2721)  loss_bbox_unscaled: 0.1236 (0.1248)  loss_giou_unscaled: 0.5299 (0.5372)  cardinality_error_unscaled: 292.4167 (291.7960)  loss_ce_0_unscaled: 1.4488 (1.3761)  loss_bbox_0_unscaled: 0.1317 (0.1319)  loss_giou_0_unscaled: 0.5690 (0.5709)  cardinality_error_0_unscaled: 292.0833 (291.6036)  loss_ce_1_unscaled: 1.4209 (1.3626)  loss_bbox_1_unscaled: 0.1276 (0.1251)  loss_giou_1_unscaled: 0.5340 (0.5439)  cardinality_error_1_unscaled: 292.2500 (291.6836)  loss_ce_2_unscaled: 1.4375 (1.3632)  loss_bbox_2_unscaled: 0.1271 (0.1249)  loss_giou_2_unscaled: 0.5241 (0.5415)  cardinality_error_2_unscaled: 292.2500 (291.7478)  loss_ce_3_unscaled: 1.4111 (1.3415)  loss_bbox_3_unscaled: 0.1239 (0.1244)  loss_giou_3_unscaled: 0.5322 (0.5377)  cardinality_error_3_unscaled: 291.3333 (290.2580)  loss_ce_4_unscaled: 1.4299 (1.3493)  loss_bbox_4_unscaled: 0.1234 (0.1243)  loss_giou_4_unscaled: 0.5284 (0.5373)  cardinality_error_4_unscaled: 292.2500 (291.6960)  time: 2.9382  data: 0.0331  max mem: 8138\n",
            "Test:  [310/417]  eta: 0:05:20  class_error: 88.04  loss: 25.5160 (26.6032)  loss_ce: 2.6315 (2.7162)  loss_bbox: 0.6092 (0.6227)  loss_giou: 1.0242 (1.0727)  loss_ce_0: 2.6728 (2.7531)  loss_bbox_0: 0.6310 (0.6583)  loss_giou_0: 1.0937 (1.1399)  loss_ce_1: 2.6108 (2.7259)  loss_bbox_1: 0.6283 (0.6243)  loss_giou_1: 1.0449 (1.0863)  loss_ce_2: 2.6246 (2.7274)  loss_bbox_2: 0.6353 (0.6237)  loss_giou_2: 1.0384 (1.0814)  loss_ce_3: 2.5742 (2.6838)  loss_bbox_3: 0.6193 (0.6208)  loss_giou_3: 1.0398 (1.0739)  loss_ce_4: 2.6208 (2.6995)  loss_bbox_4: 0.6094 (0.6203)  loss_giou_4: 1.0335 (1.0731)  loss_ce_unscaled: 1.3157 (1.3581)  class_error_unscaled: 68.5185 (71.2662)  loss_bbox_unscaled: 0.1218 (0.1245)  loss_giou_unscaled: 0.5121 (0.5363)  cardinality_error_unscaled: 292.2500 (291.8283)  loss_ce_0_unscaled: 1.3364 (1.3766)  loss_bbox_0_unscaled: 0.1262 (0.1317)  loss_giou_0_unscaled: 0.5468 (0.5700)  cardinality_error_0_unscaled: 292.0833 (291.6407)  loss_ce_1_unscaled: 1.3054 (1.3629)  loss_bbox_1_unscaled: 0.1257 (0.1249)  loss_giou_1_unscaled: 0.5225 (0.5431)  cardinality_error_1_unscaled: 292.0833 (291.7208)  loss_ce_2_unscaled: 1.3123 (1.3637)  loss_bbox_2_unscaled: 0.1271 (0.1247)  loss_giou_2_unscaled: 0.5192 (0.5407)  cardinality_error_2_unscaled: 292.2500 (291.7792)  loss_ce_3_unscaled: 1.2871 (1.3419)  loss_bbox_3_unscaled: 0.1239 (0.1242)  loss_giou_3_unscaled: 0.5199 (0.5369)  cardinality_error_3_unscaled: 291.4167 (290.3044)  loss_ce_4_unscaled: 1.3104 (1.3497)  loss_bbox_4_unscaled: 0.1219 (0.1241)  loss_giou_4_unscaled: 0.5167 (0.5365)  cardinality_error_4_unscaled: 292.1667 (291.7339)  time: 2.9807  data: 0.0327  max mem: 8138\n",
            "Test:  [320/417]  eta: 0:04:50  class_error: 77.61  loss: 25.5826 (26.5960)  loss_ce: 2.5790 (2.7135)  loss_bbox: 0.6092 (0.6230)  loss_giou: 1.0565 (1.0738)  loss_ce_0: 2.6426 (2.7507)  loss_bbox_0: 0.6252 (0.6581)  loss_giou_0: 1.1324 (1.1411)  loss_ce_1: 2.6323 (2.7235)  loss_bbox_1: 0.6144 (0.6246)  loss_giou_1: 1.0647 (1.0873)  loss_ce_2: 2.6069 (2.7247)  loss_bbox_2: 0.6334 (0.6242)  loss_giou_2: 1.0652 (1.0823)  loss_ce_3: 2.5548 (2.6812)  loss_bbox_3: 0.6163 (0.6212)  loss_giou_3: 1.0535 (1.0749)  loss_ce_4: 2.5761 (2.6971)  loss_bbox_4: 0.6094 (0.6208)  loss_giou_4: 1.0589 (1.0741)  loss_ce_unscaled: 1.2895 (1.3568)  class_error_unscaled: 68.5185 (71.2223)  loss_bbox_unscaled: 0.1218 (0.1246)  loss_giou_unscaled: 0.5283 (0.5369)  cardinality_error_unscaled: 292.2500 (291.8232)  loss_ce_0_unscaled: 1.3213 (1.3754)  loss_bbox_0_unscaled: 0.1250 (0.1316)  loss_giou_0_unscaled: 0.5662 (0.5705)  cardinality_error_0_unscaled: 292.1667 (291.6392)  loss_ce_1_unscaled: 1.3162 (1.3617)  loss_bbox_1_unscaled: 0.1229 (0.1249)  loss_giou_1_unscaled: 0.5324 (0.5436)  cardinality_error_1_unscaled: 292.0833 (291.7183)  loss_ce_2_unscaled: 1.3035 (1.3624)  loss_bbox_2_unscaled: 0.1267 (0.1248)  loss_giou_2_unscaled: 0.5326 (0.5412)  cardinality_error_2_unscaled: 292.2500 (291.7783)  loss_ce_3_unscaled: 1.2774 (1.3406)  loss_bbox_3_unscaled: 0.1233 (0.1242)  loss_giou_3_unscaled: 0.5268 (0.5374)  cardinality_error_3_unscaled: 291.5000 (290.3235)  loss_ce_4_unscaled: 1.2880 (1.3486)  loss_bbox_4_unscaled: 0.1219 (0.1242)  loss_giou_4_unscaled: 0.5294 (0.5370)  cardinality_error_4_unscaled: 292.1667 (291.7321)  time: 2.9607  data: 0.0337  max mem: 8138\n",
            "Test:  [330/417]  eta: 0:04:20  class_error: 87.65  loss: 26.0246 (26.5717)  loss_ce: 2.5790 (2.7097)  loss_bbox: 0.6455 (0.6247)  loss_giou: 1.1095 (1.0714)  loss_ce_0: 2.6295 (2.7475)  loss_bbox_0: 0.6587 (0.6595)  loss_giou_0: 1.1714 (1.1389)  loss_ce_1: 2.6147 (2.7205)  loss_bbox_1: 0.6264 (0.6262)  loss_giou_1: 1.1069 (1.0848)  loss_ce_2: 2.6061 (2.7216)  loss_bbox_2: 0.6476 (0.6259)  loss_giou_2: 1.0992 (1.0798)  loss_ce_3: 2.5493 (2.6778)  loss_bbox_3: 0.6444 (0.6230)  loss_giou_3: 1.0986 (1.0725)  loss_ce_4: 2.5662 (2.6936)  loss_bbox_4: 0.6411 (0.6225)  loss_giou_4: 1.0954 (1.0717)  loss_ce_unscaled: 1.2895 (1.3549)  class_error_unscaled: 66.1017 (71.1246)  loss_bbox_unscaled: 0.1291 (0.1249)  loss_giou_unscaled: 0.5547 (0.5357)  cardinality_error_unscaled: 291.6667 (291.8550)  loss_ce_0_unscaled: 1.3147 (1.3738)  loss_bbox_0_unscaled: 0.1317 (0.1319)  loss_giou_0_unscaled: 0.5857 (0.5695)  cardinality_error_0_unscaled: 291.6667 (291.6730)  loss_ce_1_unscaled: 1.3073 (1.3603)  loss_bbox_1_unscaled: 0.1253 (0.1252)  loss_giou_1_unscaled: 0.5535 (0.5424)  cardinality_error_1_unscaled: 291.7500 (291.7525)  loss_ce_2_unscaled: 1.3031 (1.3608)  loss_bbox_2_unscaled: 0.1295 (0.1252)  loss_giou_2_unscaled: 0.5496 (0.5399)  cardinality_error_2_unscaled: 291.6667 (291.8046)  loss_ce_3_unscaled: 1.2747 (1.3389)  loss_bbox_3_unscaled: 0.1289 (0.1246)  loss_giou_3_unscaled: 0.5493 (0.5363)  cardinality_error_3_unscaled: 291.2500 (290.3472)  loss_ce_4_unscaled: 1.2831 (1.3468)  loss_bbox_4_unscaled: 0.1282 (0.1245)  loss_giou_4_unscaled: 0.5477 (0.5359)  cardinality_error_4_unscaled: 291.6667 (291.7596)  time: 2.8368  data: 0.0321  max mem: 8138\n",
            "Test:  [340/417]  eta: 0:03:49  class_error: 42.20  loss: 23.7386 (26.5029)  loss_ce: 2.4520 (2.7007)  loss_bbox: 0.6229 (0.6242)  loss_giou: 1.0070 (1.0697)  loss_ce_0: 2.4937 (2.7381)  loss_bbox_0: 0.6436 (0.6589)  loss_giou_0: 1.1380 (1.1376)  loss_ce_1: 2.4593 (2.7111)  loss_bbox_1: 0.6213 (0.6256)  loss_giou_1: 1.0287 (1.0831)  loss_ce_2: 2.4799 (2.7124)  loss_bbox_2: 0.6227 (0.6254)  loss_giou_2: 1.0194 (1.0778)  loss_ce_3: 2.4223 (2.6686)  loss_bbox_3: 0.6260 (0.6226)  loss_giou_3: 1.0123 (1.0708)  loss_ce_4: 2.4402 (2.6845)  loss_bbox_4: 0.6186 (0.6220)  loss_giou_4: 1.0174 (1.0700)  loss_ce_unscaled: 1.2260 (1.3504)  class_error_unscaled: 64.0351 (70.8944)  loss_bbox_unscaled: 0.1246 (0.1248)  loss_giou_unscaled: 0.5035 (0.5348)  cardinality_error_unscaled: 291.2500 (291.8292)  loss_ce_0_unscaled: 1.2469 (1.3690)  loss_bbox_0_unscaled: 0.1287 (0.1318)  loss_giou_0_unscaled: 0.5690 (0.5688)  cardinality_error_0_unscaled: 291.3333 (291.6520)  loss_ce_1_unscaled: 1.2296 (1.3556)  loss_bbox_1_unscaled: 0.1243 (0.1251)  loss_giou_1_unscaled: 0.5143 (0.5415)  cardinality_error_1_unscaled: 291.2500 (291.7265)  loss_ce_2_unscaled: 1.2399 (1.3562)  loss_bbox_2_unscaled: 0.1245 (0.1251)  loss_giou_2_unscaled: 0.5097 (0.5389)  cardinality_error_2_unscaled: 291.2500 (291.7754)  loss_ce_3_unscaled: 1.2111 (1.3343)  loss_bbox_3_unscaled: 0.1252 (0.1245)  loss_giou_3_unscaled: 0.5062 (0.5354)  cardinality_error_3_unscaled: 290.2500 (290.3243)  loss_ce_4_unscaled: 1.2201 (1.3423)  loss_bbox_4_unscaled: 0.1237 (0.1244)  loss_giou_4_unscaled: 0.5087 (0.5350)  cardinality_error_4_unscaled: 291.1667 (291.7307)  time: 2.8121  data: 0.0302  max mem: 8138\n",
            "Test:  [350/417]  eta: 0:03:19  class_error: 63.16  loss: 23.3714 (26.4561)  loss_ce: 2.3423 (2.6944)  loss_bbox: 0.6041 (0.6235)  loss_giou: 0.9442 (1.0687)  loss_ce_0: 2.4093 (2.7313)  loss_bbox_0: 0.6326 (0.6586)  loss_giou_0: 1.0991 (1.1372)  loss_ce_1: 2.3628 (2.7047)  loss_bbox_1: 0.6072 (0.6251)  loss_giou_1: 0.9840 (1.0823)  loss_ce_2: 2.3260 (2.7058)  loss_bbox_2: 0.5926 (0.6249)  loss_giou_2: 0.9600 (1.0769)  loss_ce_3: 2.3506 (2.6625)  loss_bbox_3: 0.6155 (0.6219)  loss_giou_3: 0.9376 (1.0698)  loss_ce_4: 2.3564 (2.6783)  loss_bbox_4: 0.6042 (0.6213)  loss_giou_4: 0.9449 (1.0690)  loss_ce_unscaled: 1.1712 (1.3472)  class_error_unscaled: 63.1579 (70.6968)  loss_bbox_unscaled: 0.1208 (0.1247)  loss_giou_unscaled: 0.4721 (0.5344)  cardinality_error_unscaled: 291.2500 (291.8428)  loss_ce_0_unscaled: 1.2047 (1.3657)  loss_bbox_0_unscaled: 0.1265 (0.1317)  loss_giou_0_unscaled: 0.5496 (0.5686)  cardinality_error_0_unscaled: 291.0833 (291.6674)  loss_ce_1_unscaled: 1.1814 (1.3524)  loss_bbox_1_unscaled: 0.1214 (0.1250)  loss_giou_1_unscaled: 0.4920 (0.5411)  cardinality_error_1_unscaled: 291.0000 (291.7386)  loss_ce_2_unscaled: 1.1630 (1.3529)  loss_bbox_2_unscaled: 0.1185 (0.1250)  loss_giou_2_unscaled: 0.4800 (0.5385)  cardinality_error_2_unscaled: 290.8333 (291.7849)  loss_ce_3_unscaled: 1.1753 (1.3312)  loss_bbox_3_unscaled: 0.1231 (0.1244)  loss_giou_3_unscaled: 0.4688 (0.5349)  cardinality_error_3_unscaled: 290.2500 (290.3419)  loss_ce_4_unscaled: 1.1782 (1.3391)  loss_bbox_4_unscaled: 0.1208 (0.1243)  loss_giou_4_unscaled: 0.4725 (0.5345)  cardinality_error_4_unscaled: 291.0833 (291.7419)  time: 2.8974  data: 0.0315  max mem: 8138\n",
            "Test:  [360/417]  eta: 0:02:50  class_error: 72.92  loss: 25.0261 (26.4312)  loss_ce: 2.5173 (2.6923)  loss_bbox: 0.6161 (0.6225)  loss_giou: 0.9535 (1.0675)  loss_ce_0: 2.5770 (2.7292)  loss_bbox_0: 0.6326 (0.6577)  loss_giou_0: 1.0991 (1.1363)  loss_ce_1: 2.5416 (2.7024)  loss_bbox_1: 0.6257 (0.6242)  loss_giou_1: 0.9851 (1.0814)  loss_ce_2: 2.5596 (2.7040)  loss_bbox_2: 0.6309 (0.6240)  loss_giou_2: 0.9664 (1.0757)  loss_ce_3: 2.5410 (2.6604)  loss_bbox_3: 0.6155 (0.6209)  loss_giou_3: 0.9483 (1.0685)  loss_ce_4: 2.5184 (2.6762)  loss_bbox_4: 0.6145 (0.6203)  loss_giou_4: 0.9571 (1.0677)  loss_ce_unscaled: 1.2587 (1.3461)  class_error_unscaled: 66.6667 (70.6666)  loss_bbox_unscaled: 0.1232 (0.1245)  loss_giou_unscaled: 0.4768 (0.5338)  cardinality_error_unscaled: 291.5833 (291.8174)  loss_ce_0_unscaled: 1.2885 (1.3646)  loss_bbox_0_unscaled: 0.1265 (0.1315)  loss_giou_0_unscaled: 0.5496 (0.5682)  cardinality_error_0_unscaled: 291.5000 (291.6512)  loss_ce_1_unscaled: 1.2708 (1.3512)  loss_bbox_1_unscaled: 0.1251 (0.1248)  loss_giou_1_unscaled: 0.4926 (0.5407)  cardinality_error_1_unscaled: 291.3333 (291.7200)  loss_ce_2_unscaled: 1.2798 (1.3520)  loss_bbox_2_unscaled: 0.1262 (0.1248)  loss_giou_2_unscaled: 0.4832 (0.5378)  cardinality_error_2_unscaled: 291.2500 (291.7648)  loss_ce_3_unscaled: 1.2705 (1.3302)  loss_bbox_3_unscaled: 0.1231 (0.1242)  loss_giou_3_unscaled: 0.4741 (0.5342)  cardinality_error_3_unscaled: 289.8333 (290.2936)  loss_ce_4_unscaled: 1.2592 (1.3381)  loss_bbox_4_unscaled: 0.1229 (0.1241)  loss_giou_4_unscaled: 0.4785 (0.5338)  cardinality_error_4_unscaled: 292.1667 (291.7221)  time: 3.0086  data: 0.0330  max mem: 8138\n",
            "Test:  [370/417]  eta: 0:02:20  class_error: 73.61  loss: 27.8076 (26.5237)  loss_ce: 2.8292 (2.7026)  loss_bbox: 0.6453 (0.6248)  loss_giou: 1.1451 (1.0701)  loss_ce_0: 2.8731 (2.7401)  loss_bbox_0: 0.6902 (0.6597)  loss_giou_0: 1.1784 (1.1384)  loss_ce_1: 2.8255 (2.7137)  loss_bbox_1: 0.6517 (0.6267)  loss_giou_1: 1.1464 (1.0838)  loss_ce_2: 2.8889 (2.7144)  loss_bbox_2: 0.6432 (0.6263)  loss_giou_2: 1.1422 (1.0784)  loss_ce_3: 2.8194 (2.6707)  loss_bbox_3: 0.6508 (0.6233)  loss_giou_3: 1.1431 (1.0710)  loss_ce_4: 2.8181 (2.6869)  loss_bbox_4: 0.6390 (0.6226)  loss_giou_4: 1.1402 (1.0703)  loss_ce_unscaled: 1.4146 (1.3513)  class_error_unscaled: 74.0741 (70.9175)  loss_bbox_unscaled: 0.1291 (0.1250)  loss_giou_unscaled: 0.5725 (0.5350)  cardinality_error_unscaled: 291.9167 (291.8277)  loss_ce_0_unscaled: 1.4366 (1.3701)  loss_bbox_0_unscaled: 0.1380 (0.1319)  loss_giou_0_unscaled: 0.5892 (0.5692)  cardinality_error_0_unscaled: 291.7500 (291.6413)  loss_ce_1_unscaled: 1.4128 (1.3568)  loss_bbox_1_unscaled: 0.1303 (0.1253)  loss_giou_1_unscaled: 0.5732 (0.5419)  cardinality_error_1_unscaled: 292.1667 (291.7289)  loss_ce_2_unscaled: 1.4444 (1.3572)  loss_bbox_2_unscaled: 0.1286 (0.1253)  loss_giou_2_unscaled: 0.5711 (0.5392)  cardinality_error_2_unscaled: 292.2500 (291.7783)  loss_ce_3_unscaled: 1.4097 (1.3353)  loss_bbox_3_unscaled: 0.1302 (0.1247)  loss_giou_3_unscaled: 0.5716 (0.5355)  cardinality_error_3_unscaled: 290.4167 (290.2967)  loss_ce_4_unscaled: 1.4090 (1.3434)  loss_bbox_4_unscaled: 0.1278 (0.1245)  loss_giou_4_unscaled: 0.5701 (0.5351)  cardinality_error_4_unscaled: 292.1667 (291.7237)  time: 2.9965  data: 0.0330  max mem: 8138\n",
            "Test:  [380/417]  eta: 0:01:50  class_error: 39.46  loss: 28.5692 (26.5156)  loss_ce: 2.9716 (2.7009)  loss_bbox: 0.6423 (0.6242)  loss_giou: 1.1451 (1.0708)  loss_ce_0: 3.0255 (2.7388)  loss_bbox_0: 0.6851 (0.6589)  loss_giou_0: 1.1784 (1.1388)  loss_ce_1: 3.0301 (2.7127)  loss_bbox_1: 0.6370 (0.6260)  loss_giou_1: 1.1367 (1.0845)  loss_ce_2: 2.9753 (2.7130)  loss_bbox_2: 0.6360 (0.6256)  loss_giou_2: 1.1422 (1.0792)  loss_ce_3: 2.9425 (2.6692)  loss_bbox_3: 0.6348 (0.6227)  loss_giou_3: 1.1431 (1.0718)  loss_ce_4: 2.9491 (2.6855)  loss_bbox_4: 0.6343 (0.6219)  loss_giou_4: 1.1402 (1.0711)  loss_ce_unscaled: 1.4858 (1.3504)  class_error_unscaled: 77.4648 (70.8698)  loss_bbox_unscaled: 0.1285 (0.1248)  loss_giou_unscaled: 0.5725 (0.5354)  cardinality_error_unscaled: 292.7500 (291.8360)  loss_ce_0_unscaled: 1.5127 (1.3694)  loss_bbox_0_unscaled: 0.1370 (0.1318)  loss_giou_0_unscaled: 0.5892 (0.5694)  cardinality_error_0_unscaled: 292.3333 (291.6496)  loss_ce_1_unscaled: 1.5150 (1.3563)  loss_bbox_1_unscaled: 0.1274 (0.1252)  loss_giou_1_unscaled: 0.5683 (0.5422)  cardinality_error_1_unscaled: 292.8333 (291.7360)  loss_ce_2_unscaled: 1.4876 (1.3565)  loss_bbox_2_unscaled: 0.1272 (0.1251)  loss_giou_2_unscaled: 0.5711 (0.5396)  cardinality_error_2_unscaled: 292.6667 (291.7808)  loss_ce_3_unscaled: 1.4713 (1.3346)  loss_bbox_3_unscaled: 0.1270 (0.1245)  loss_giou_3_unscaled: 0.5716 (0.5359)  cardinality_error_3_unscaled: 291.1667 (290.2983)  loss_ce_4_unscaled: 1.4746 (1.3427)  loss_bbox_4_unscaled: 0.1269 (0.1244)  loss_giou_4_unscaled: 0.5701 (0.5355)  cardinality_error_4_unscaled: 292.8333 (291.7338)  time: 2.9048  data: 0.0327  max mem: 8138\n",
            "Test:  [390/417]  eta: 0:01:20  class_error: 70.11  loss: 26.1591 (26.4902)  loss_ce: 2.7195 (2.6994)  loss_bbox: 0.5855 (0.6228)  loss_giou: 1.0300 (1.0692)  loss_ce_0: 2.8012 (2.7377)  loss_bbox_0: 0.6133 (0.6575)  loss_giou_0: 1.1198 (1.1379)  loss_ce_1: 2.7605 (2.7114)  loss_bbox_1: 0.5937 (0.6246)  loss_giou_1: 1.0644 (1.0832)  loss_ce_2: 2.7680 (2.7119)  loss_bbox_2: 0.5824 (0.6241)  loss_giou_2: 1.0373 (1.0776)  loss_ce_3: 2.6789 (2.6678)  loss_bbox_3: 0.5852 (0.6212)  loss_giou_3: 1.0326 (1.0702)  loss_ce_4: 2.7109 (2.6836)  loss_bbox_4: 0.5819 (0.6204)  loss_giou_4: 1.0303 (1.0695)  loss_ce_unscaled: 1.3598 (1.3497)  class_error_unscaled: 74.1071 (70.8622)  loss_bbox_unscaled: 0.1171 (0.1246)  loss_giou_unscaled: 0.5150 (0.5346)  cardinality_error_unscaled: 292.6667 (291.8289)  loss_ce_0_unscaled: 1.4006 (1.3689)  loss_bbox_0_unscaled: 0.1227 (0.1315)  loss_giou_0_unscaled: 0.5599 (0.5689)  cardinality_error_0_unscaled: 292.2500 (291.6353)  loss_ce_1_unscaled: 1.3802 (1.3557)  loss_bbox_1_unscaled: 0.1187 (0.1249)  loss_giou_1_unscaled: 0.5322 (0.5416)  cardinality_error_1_unscaled: 292.5000 (291.7266)  loss_ce_2_unscaled: 1.3840 (1.3559)  loss_bbox_2_unscaled: 0.1165 (0.1248)  loss_giou_2_unscaled: 0.5186 (0.5388)  cardinality_error_2_unscaled: 292.5000 (291.7781)  loss_ce_3_unscaled: 1.3394 (1.3339)  loss_bbox_3_unscaled: 0.1170 (0.1242)  loss_giou_3_unscaled: 0.5163 (0.5351)  cardinality_error_3_unscaled: 290.5000 (290.2816)  loss_ce_4_unscaled: 1.3555 (1.3418)  loss_bbox_4_unscaled: 0.1164 (0.1241)  loss_giou_4_unscaled: 0.5151 (0.5348)  cardinality_error_4_unscaled: 292.5000 (291.7266)  time: 2.8627  data: 0.0318  max mem: 8138\n",
            "Test:  [400/417]  eta: 0:00:50  class_error: 55.21  loss: 25.6012 (26.4764)  loss_ce: 2.6473 (2.6980)  loss_bbox: 0.5545 (0.6228)  loss_giou: 1.0622 (1.0685)  loss_ce_0: 2.6926 (2.7360)  loss_bbox_0: 0.6170 (0.6575)  loss_giou_0: 1.1247 (1.1371)  loss_ce_1: 2.6565 (2.7098)  loss_bbox_1: 0.5570 (0.6247)  loss_giou_1: 1.0789 (1.0826)  loss_ce_2: 2.6568 (2.7103)  loss_bbox_2: 0.5615 (0.6241)  loss_giou_2: 1.0373 (1.0769)  loss_ce_3: 2.5968 (2.6664)  loss_bbox_3: 0.5536 (0.6211)  loss_giou_3: 1.0326 (1.0694)  loss_ce_4: 2.6196 (2.6820)  loss_bbox_4: 0.5506 (0.6204)  loss_giou_4: 1.0563 (1.0688)  loss_ce_unscaled: 1.3237 (1.3490)  class_error_unscaled: 71.4286 (70.8199)  loss_bbox_unscaled: 0.1109 (0.1246)  loss_giou_unscaled: 0.5311 (0.5342)  cardinality_error_unscaled: 291.9167 (291.8358)  loss_ce_0_unscaled: 1.3463 (1.3680)  loss_bbox_0_unscaled: 0.1234 (0.1315)  loss_giou_0_unscaled: 0.5624 (0.5686)  cardinality_error_0_unscaled: 291.9167 (291.6419)  loss_ce_1_unscaled: 1.3283 (1.3549)  loss_bbox_1_unscaled: 0.1114 (0.1249)  loss_giou_1_unscaled: 0.5394 (0.5413)  cardinality_error_1_unscaled: 291.9167 (291.7319)  loss_ce_2_unscaled: 1.3284 (1.3551)  loss_bbox_2_unscaled: 0.1123 (0.1248)  loss_giou_2_unscaled: 0.5186 (0.5384)  cardinality_error_2_unscaled: 292.2500 (291.7855)  loss_ce_3_unscaled: 1.2984 (1.3332)  loss_bbox_3_unscaled: 0.1107 (0.1242)  loss_giou_3_unscaled: 0.5163 (0.5347)  cardinality_error_3_unscaled: 290.7500 (290.2914)  loss_ce_4_unscaled: 1.3098 (1.3410)  loss_bbox_4_unscaled: 0.1101 (0.1241)  loss_giou_4_unscaled: 0.5281 (0.5344)  cardinality_error_4_unscaled: 291.9167 (291.7340)  time: 2.9455  data: 0.0313  max mem: 8138\n",
            "Test:  [410/417]  eta: 0:00:20  class_error: 69.17  loss: 27.2634 (26.5068)  loss_ce: 2.6786 (2.6996)  loss_bbox: 0.5914 (0.6235)  loss_giou: 1.1098 (1.0714)  loss_ce_0: 2.7857 (2.7374)  loss_bbox_0: 0.6170 (0.6578)  loss_giou_0: 1.1411 (1.1395)  loss_ce_1: 2.7718 (2.7116)  loss_bbox_1: 0.5970 (0.6253)  loss_giou_1: 1.1370 (1.0855)  loss_ce_2: 2.7257 (2.7120)  loss_bbox_2: 0.6010 (0.6249)  loss_giou_2: 1.1202 (1.0799)  loss_ce_3: 2.6619 (2.6679)  loss_bbox_3: 0.5973 (0.6218)  loss_giou_3: 1.1081 (1.0724)  loss_ce_4: 2.6780 (2.6836)  loss_bbox_4: 0.5955 (0.6211)  loss_giou_4: 1.1086 (1.0717)  loss_ce_unscaled: 1.3393 (1.3498)  class_error_unscaled: 71.2644 (70.8971)  loss_bbox_unscaled: 0.1183 (0.1247)  loss_giou_unscaled: 0.5549 (0.5357)  cardinality_error_unscaled: 292.1667 (291.8384)  loss_ce_0_unscaled: 1.3929 (1.3687)  loss_bbox_0_unscaled: 0.1234 (0.1316)  loss_giou_0_unscaled: 0.5706 (0.5697)  cardinality_error_0_unscaled: 291.9167 (291.6413)  loss_ce_1_unscaled: 1.3859 (1.3558)  loss_bbox_1_unscaled: 0.1194 (0.1251)  loss_giou_1_unscaled: 0.5685 (0.5428)  cardinality_error_1_unscaled: 292.0833 (291.7344)  loss_ce_2_unscaled: 1.3629 (1.3560)  loss_bbox_2_unscaled: 0.1202 (0.1250)  loss_giou_2_unscaled: 0.5601 (0.5399)  cardinality_error_2_unscaled: 292.2500 (291.7885)  loss_ce_3_unscaled: 1.3310 (1.3339)  loss_bbox_3_unscaled: 0.1195 (0.1244)  loss_giou_3_unscaled: 0.5541 (0.5362)  cardinality_error_3_unscaled: 290.9167 (290.2936)  loss_ce_4_unscaled: 1.3390 (1.3418)  loss_bbox_4_unscaled: 0.1191 (0.1242)  loss_giou_4_unscaled: 0.5543 (0.5359)  cardinality_error_4_unscaled: 291.9167 (291.7389)  time: 2.9464  data: 0.0312  max mem: 8138\n",
            "Test:  [416/417]  eta: 0:00:02  class_error: 53.66  loss: 25.3530 (26.4784)  loss_ce: 2.4914 (2.6964)  loss_bbox: 0.6032 (0.6227)  loss_giou: 1.0567 (1.0702)  loss_ce_0: 2.5725 (2.7343)  loss_bbox_0: 0.6079 (0.6573)  loss_giou_0: 1.1288 (1.1387)  loss_ce_1: 2.5928 (2.7088)  loss_bbox_1: 0.5970 (0.6246)  loss_giou_1: 1.0627 (1.0845)  loss_ce_2: 2.5624 (2.7091)  loss_bbox_2: 0.6010 (0.6241)  loss_giou_2: 1.0555 (1.0787)  loss_ce_3: 2.4763 (2.6652)  loss_bbox_3: 0.6019 (0.6211)  loss_giou_3: 1.0669 (1.0712)  loss_ce_4: 2.4487 (2.6806)  loss_bbox_4: 0.6010 (0.6203)  loss_giou_4: 1.0598 (1.0705)  loss_ce_unscaled: 1.2457 (1.3482)  class_error_unscaled: 68.4931 (70.8294)  loss_bbox_unscaled: 0.1206 (0.1245)  loss_giou_unscaled: 0.5283 (0.5351)  cardinality_error_unscaled: 292.2500 (291.8633)  loss_ce_0_unscaled: 1.2862 (1.3672)  loss_bbox_0_unscaled: 0.1216 (0.1315)  loss_giou_0_unscaled: 0.5644 (0.5694)  cardinality_error_0_unscaled: 292.1667 (291.6626)  loss_ce_1_unscaled: 1.2964 (1.3544)  loss_bbox_1_unscaled: 0.1194 (0.1249)  loss_giou_1_unscaled: 0.5314 (0.5422)  cardinality_error_1_unscaled: 292.2500 (291.7553)  loss_ce_2_unscaled: 1.2812 (1.3546)  loss_bbox_2_unscaled: 0.1202 (0.1248)  loss_giou_2_unscaled: 0.5277 (0.5393)  cardinality_error_2_unscaled: 292.3333 (291.8121)  loss_ce_3_unscaled: 1.2381 (1.3326)  loss_bbox_3_unscaled: 0.1204 (0.1242)  loss_giou_3_unscaled: 0.5334 (0.5356)  cardinality_error_3_unscaled: 290.9167 (290.3117)  loss_ce_4_unscaled: 1.2243 (1.3403)  loss_bbox_4_unscaled: 0.1202 (0.1241)  loss_giou_4_unscaled: 0.5299 (0.5352)  cardinality_error_4_unscaled: 292.5833 (291.7616)  time: 2.8486  data: 0.0304  max mem: 8138\n",
            "Test: Total time: 0:20:40 (2.9750 s / it)\n",
            "Averaged stats: class_error: 53.66  loss: 25.3530 (26.4784)  loss_ce: 2.4914 (2.6964)  loss_bbox: 0.6032 (0.6227)  loss_giou: 1.0567 (1.0702)  loss_ce_0: 2.5725 (2.7343)  loss_bbox_0: 0.6079 (0.6573)  loss_giou_0: 1.1288 (1.1387)  loss_ce_1: 2.5928 (2.7088)  loss_bbox_1: 0.5970 (0.6246)  loss_giou_1: 1.0627 (1.0845)  loss_ce_2: 2.5624 (2.7091)  loss_bbox_2: 0.6010 (0.6241)  loss_giou_2: 1.0555 (1.0787)  loss_ce_3: 2.4763 (2.6652)  loss_bbox_3: 0.6019 (0.6211)  loss_giou_3: 1.0669 (1.0712)  loss_ce_4: 2.4487 (2.6806)  loss_bbox_4: 0.6010 (0.6203)  loss_giou_4: 1.0598 (1.0705)  loss_ce_unscaled: 1.2457 (1.3482)  class_error_unscaled: 68.4931 (70.8294)  loss_bbox_unscaled: 0.1206 (0.1245)  loss_giou_unscaled: 0.5283 (0.5351)  cardinality_error_unscaled: 292.2500 (291.8633)  loss_ce_0_unscaled: 1.2862 (1.3672)  loss_bbox_0_unscaled: 0.1216 (0.1315)  loss_giou_0_unscaled: 0.5644 (0.5694)  cardinality_error_0_unscaled: 292.1667 (291.6626)  loss_ce_1_unscaled: 1.2964 (1.3544)  loss_bbox_1_unscaled: 0.1194 (0.1249)  loss_giou_1_unscaled: 0.5314 (0.5422)  cardinality_error_1_unscaled: 292.2500 (291.7553)  loss_ce_2_unscaled: 1.2812 (1.3546)  loss_bbox_2_unscaled: 0.1202 (0.1248)  loss_giou_2_unscaled: 0.5277 (0.5393)  cardinality_error_2_unscaled: 292.3333 (291.8121)  loss_ce_3_unscaled: 1.2381 (1.3326)  loss_bbox_3_unscaled: 0.1204 (0.1242)  loss_giou_3_unscaled: 0.5334 (0.5356)  cardinality_error_3_unscaled: 290.9167 (290.3117)  loss_ce_4_unscaled: 1.2243 (1.3403)  loss_bbox_4_unscaled: 0.1202 (0.1241)  loss_giou_4_unscaled: 0.5299 (0.5352)  cardinality_error_4_unscaled: 292.5833 (291.7616)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=12.02s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.165\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.246\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.178\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.080\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.173\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.238\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.159\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.260\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.282\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.148\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.294\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py \\\n",
        "    --output_dir exps/deform \\\n",
        "    --coco_path ../COCODIR \\\n",
        "    --batch_size 12 \\\n",
        "    --resume ./pth/upperbound.pth \\\n",
        "    --with_box_refine \\\n",
        "    --eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfGTaoOQdDh-",
        "outputId": "c185ec98-d15e-443a-b91e-45f405f0f63f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "git:\n",
            "  sha: 11169a60c33333af00a4849f1808023eba96a931, status: has uncommited changes, branch: main\n",
            "\n",
            "Namespace(lr=0.0002, lr_backbone_names=['backbone.0'], lr_backbone=2e-05, lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, batch_size=12, weight_decay=0.0001, epochs=50, lr_drop=40, lr_drop_epochs=None, clip_max_norm=0.1, sgd=False, with_box_refine=True, two_stage=False, frozen_weights=None, backbone='resnet50', dilation=False, position_embedding='sine', position_embedding_scale=6.283185307179586, num_feature_levels=4, enc_layers=6, dec_layers=6, dim_feedforward=1024, hidden_dim=256, dropout=0.1, nheads=8, num_queries=300, dec_n_points=4, enc_n_points=4, masks=False, aux_loss=True, set_cost_class=2, set_cost_bbox=5, set_cost_giou=2, mask_loss_coef=1, dice_loss_coef=1, cls_loss_coef=2, bbox_loss_coef=5, giou_loss_coef=2, focal_alpha=0.25, dataset_file='coco', coco_path='../COCODIR', coco_panoptic_path=None, remove_difficult=False, output_dir='exps/deform', device='cuda', seed=42, resume='./pth/upperbound.pth', start_epoch=0, eval=True, num_workers=2, cache_mode=False, distributed=False)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100% 97.8M/97.8M [00:00<00:00, 200MB/s]\n",
            "number of params: 40627260\n",
            "loading annotations into memory...\n",
            "Done (t=22.19s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=2.74s)\n",
            "creating index...\n",
            "index created!\n",
            "transformer.level_embed\n",
            "transformer.encoder.layers.0.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.0.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.0.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.0.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.0.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.0.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.0.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.0.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.0.norm1.weight\n",
            "transformer.encoder.layers.0.norm1.bias\n",
            "transformer.encoder.layers.0.linear1.weight\n",
            "transformer.encoder.layers.0.linear1.bias\n",
            "transformer.encoder.layers.0.linear2.weight\n",
            "transformer.encoder.layers.0.linear2.bias\n",
            "transformer.encoder.layers.0.norm2.weight\n",
            "transformer.encoder.layers.0.norm2.bias\n",
            "transformer.encoder.layers.1.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.1.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.1.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.1.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.1.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.1.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.1.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.1.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.1.norm1.weight\n",
            "transformer.encoder.layers.1.norm1.bias\n",
            "transformer.encoder.layers.1.linear1.weight\n",
            "transformer.encoder.layers.1.linear1.bias\n",
            "transformer.encoder.layers.1.linear2.weight\n",
            "transformer.encoder.layers.1.linear2.bias\n",
            "transformer.encoder.layers.1.norm2.weight\n",
            "transformer.encoder.layers.1.norm2.bias\n",
            "transformer.encoder.layers.2.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.2.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.2.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.2.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.2.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.2.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.2.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.2.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.2.norm1.weight\n",
            "transformer.encoder.layers.2.norm1.bias\n",
            "transformer.encoder.layers.2.linear1.weight\n",
            "transformer.encoder.layers.2.linear1.bias\n",
            "transformer.encoder.layers.2.linear2.weight\n",
            "transformer.encoder.layers.2.linear2.bias\n",
            "transformer.encoder.layers.2.norm2.weight\n",
            "transformer.encoder.layers.2.norm2.bias\n",
            "transformer.encoder.layers.3.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.3.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.3.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.3.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.3.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.3.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.3.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.3.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.3.norm1.weight\n",
            "transformer.encoder.layers.3.norm1.bias\n",
            "transformer.encoder.layers.3.linear1.weight\n",
            "transformer.encoder.layers.3.linear1.bias\n",
            "transformer.encoder.layers.3.linear2.weight\n",
            "transformer.encoder.layers.3.linear2.bias\n",
            "transformer.encoder.layers.3.norm2.weight\n",
            "transformer.encoder.layers.3.norm2.bias\n",
            "transformer.encoder.layers.4.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.4.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.4.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.4.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.4.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.4.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.4.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.4.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.4.norm1.weight\n",
            "transformer.encoder.layers.4.norm1.bias\n",
            "transformer.encoder.layers.4.linear1.weight\n",
            "transformer.encoder.layers.4.linear1.bias\n",
            "transformer.encoder.layers.4.linear2.weight\n",
            "transformer.encoder.layers.4.linear2.bias\n",
            "transformer.encoder.layers.4.norm2.weight\n",
            "transformer.encoder.layers.4.norm2.bias\n",
            "transformer.encoder.layers.5.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.5.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.5.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.5.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.5.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.5.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.5.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.5.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.5.norm1.weight\n",
            "transformer.encoder.layers.5.norm1.bias\n",
            "transformer.encoder.layers.5.linear1.weight\n",
            "transformer.encoder.layers.5.linear1.bias\n",
            "transformer.encoder.layers.5.linear2.weight\n",
            "transformer.encoder.layers.5.linear2.bias\n",
            "transformer.encoder.layers.5.norm2.weight\n",
            "transformer.encoder.layers.5.norm2.bias\n",
            "transformer.decoder.layers.0.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.0.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.0.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.0.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.0.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.0.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.0.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.0.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.0.norm1.weight\n",
            "transformer.decoder.layers.0.norm1.bias\n",
            "transformer.decoder.layers.0.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.0.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.0.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.0.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.0.norm2.weight\n",
            "transformer.decoder.layers.0.norm2.bias\n",
            "transformer.decoder.layers.0.linear1.weight\n",
            "transformer.decoder.layers.0.linear1.bias\n",
            "transformer.decoder.layers.0.linear2.weight\n",
            "transformer.decoder.layers.0.linear2.bias\n",
            "transformer.decoder.layers.0.norm3.weight\n",
            "transformer.decoder.layers.0.norm3.bias\n",
            "transformer.decoder.layers.1.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.1.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.1.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.1.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.1.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.1.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.1.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.1.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.1.norm1.weight\n",
            "transformer.decoder.layers.1.norm1.bias\n",
            "transformer.decoder.layers.1.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.1.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.1.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.1.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.1.norm2.weight\n",
            "transformer.decoder.layers.1.norm2.bias\n",
            "transformer.decoder.layers.1.linear1.weight\n",
            "transformer.decoder.layers.1.linear1.bias\n",
            "transformer.decoder.layers.1.linear2.weight\n",
            "transformer.decoder.layers.1.linear2.bias\n",
            "transformer.decoder.layers.1.norm3.weight\n",
            "transformer.decoder.layers.1.norm3.bias\n",
            "transformer.decoder.layers.2.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.2.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.2.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.2.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.2.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.2.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.2.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.2.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.2.norm1.weight\n",
            "transformer.decoder.layers.2.norm1.bias\n",
            "transformer.decoder.layers.2.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.2.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.2.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.2.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.2.norm2.weight\n",
            "transformer.decoder.layers.2.norm2.bias\n",
            "transformer.decoder.layers.2.linear1.weight\n",
            "transformer.decoder.layers.2.linear1.bias\n",
            "transformer.decoder.layers.2.linear2.weight\n",
            "transformer.decoder.layers.2.linear2.bias\n",
            "transformer.decoder.layers.2.norm3.weight\n",
            "transformer.decoder.layers.2.norm3.bias\n",
            "transformer.decoder.layers.3.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.3.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.3.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.3.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.3.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.3.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.3.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.3.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.3.norm1.weight\n",
            "transformer.decoder.layers.3.norm1.bias\n",
            "transformer.decoder.layers.3.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.3.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.3.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.3.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.3.norm2.weight\n",
            "transformer.decoder.layers.3.norm2.bias\n",
            "transformer.decoder.layers.3.linear1.weight\n",
            "transformer.decoder.layers.3.linear1.bias\n",
            "transformer.decoder.layers.3.linear2.weight\n",
            "transformer.decoder.layers.3.linear2.bias\n",
            "transformer.decoder.layers.3.norm3.weight\n",
            "transformer.decoder.layers.3.norm3.bias\n",
            "transformer.decoder.layers.4.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.4.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.4.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.4.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.4.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.4.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.4.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.4.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.4.norm1.weight\n",
            "transformer.decoder.layers.4.norm1.bias\n",
            "transformer.decoder.layers.4.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.4.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.4.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.4.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.4.norm2.weight\n",
            "transformer.decoder.layers.4.norm2.bias\n",
            "transformer.decoder.layers.4.linear1.weight\n",
            "transformer.decoder.layers.4.linear1.bias\n",
            "transformer.decoder.layers.4.linear2.weight\n",
            "transformer.decoder.layers.4.linear2.bias\n",
            "transformer.decoder.layers.4.norm3.weight\n",
            "transformer.decoder.layers.4.norm3.bias\n",
            "transformer.decoder.layers.5.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.5.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.5.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.5.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.5.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.5.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.5.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.5.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.5.norm1.weight\n",
            "transformer.decoder.layers.5.norm1.bias\n",
            "transformer.decoder.layers.5.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.5.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.5.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.5.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.5.norm2.weight\n",
            "transformer.decoder.layers.5.norm2.bias\n",
            "transformer.decoder.layers.5.linear1.weight\n",
            "transformer.decoder.layers.5.linear1.bias\n",
            "transformer.decoder.layers.5.linear2.weight\n",
            "transformer.decoder.layers.5.linear2.bias\n",
            "transformer.decoder.layers.5.norm3.weight\n",
            "transformer.decoder.layers.5.norm3.bias\n",
            "transformer.decoder.bbox_embed.0.layers.0.weight\n",
            "transformer.decoder.bbox_embed.0.layers.0.bias\n",
            "transformer.decoder.bbox_embed.0.layers.1.weight\n",
            "transformer.decoder.bbox_embed.0.layers.1.bias\n",
            "transformer.decoder.bbox_embed.0.layers.2.weight\n",
            "transformer.decoder.bbox_embed.0.layers.2.bias\n",
            "transformer.decoder.bbox_embed.1.layers.0.weight\n",
            "transformer.decoder.bbox_embed.1.layers.0.bias\n",
            "transformer.decoder.bbox_embed.1.layers.1.weight\n",
            "transformer.decoder.bbox_embed.1.layers.1.bias\n",
            "transformer.decoder.bbox_embed.1.layers.2.weight\n",
            "transformer.decoder.bbox_embed.1.layers.2.bias\n",
            "transformer.decoder.bbox_embed.2.layers.0.weight\n",
            "transformer.decoder.bbox_embed.2.layers.0.bias\n",
            "transformer.decoder.bbox_embed.2.layers.1.weight\n",
            "transformer.decoder.bbox_embed.2.layers.1.bias\n",
            "transformer.decoder.bbox_embed.2.layers.2.weight\n",
            "transformer.decoder.bbox_embed.2.layers.2.bias\n",
            "transformer.decoder.bbox_embed.3.layers.0.weight\n",
            "transformer.decoder.bbox_embed.3.layers.0.bias\n",
            "transformer.decoder.bbox_embed.3.layers.1.weight\n",
            "transformer.decoder.bbox_embed.3.layers.1.bias\n",
            "transformer.decoder.bbox_embed.3.layers.2.weight\n",
            "transformer.decoder.bbox_embed.3.layers.2.bias\n",
            "transformer.decoder.bbox_embed.4.layers.0.weight\n",
            "transformer.decoder.bbox_embed.4.layers.0.bias\n",
            "transformer.decoder.bbox_embed.4.layers.1.weight\n",
            "transformer.decoder.bbox_embed.4.layers.1.bias\n",
            "transformer.decoder.bbox_embed.4.layers.2.weight\n",
            "transformer.decoder.bbox_embed.4.layers.2.bias\n",
            "transformer.decoder.bbox_embed.5.layers.0.weight\n",
            "transformer.decoder.bbox_embed.5.layers.0.bias\n",
            "transformer.decoder.bbox_embed.5.layers.1.weight\n",
            "transformer.decoder.bbox_embed.5.layers.1.bias\n",
            "transformer.decoder.bbox_embed.5.layers.2.weight\n",
            "transformer.decoder.bbox_embed.5.layers.2.bias\n",
            "transformer.reference_points.weight\n",
            "transformer.reference_points.bias\n",
            "class_embed.0.weight\n",
            "class_embed.0.bias\n",
            "class_embed.1.weight\n",
            "class_embed.1.bias\n",
            "class_embed.2.weight\n",
            "class_embed.2.bias\n",
            "class_embed.3.weight\n",
            "class_embed.3.bias\n",
            "class_embed.4.weight\n",
            "class_embed.4.bias\n",
            "class_embed.5.weight\n",
            "class_embed.5.bias\n",
            "query_embed.weight\n",
            "input_proj.0.0.weight\n",
            "input_proj.0.0.bias\n",
            "input_proj.0.1.weight\n",
            "input_proj.0.1.bias\n",
            "input_proj.1.0.weight\n",
            "input_proj.1.0.bias\n",
            "input_proj.1.1.weight\n",
            "input_proj.1.1.bias\n",
            "input_proj.2.0.weight\n",
            "input_proj.2.0.bias\n",
            "input_proj.2.1.weight\n",
            "input_proj.2.1.bias\n",
            "input_proj.3.0.weight\n",
            "input_proj.3.0.bias\n",
            "input_proj.3.1.weight\n",
            "input_proj.3.1.bias\n",
            "backbone.0.body.conv1.weight\n",
            "backbone.0.body.layer1.0.conv1.weight\n",
            "backbone.0.body.layer1.0.conv2.weight\n",
            "backbone.0.body.layer1.0.conv3.weight\n",
            "backbone.0.body.layer1.0.downsample.0.weight\n",
            "backbone.0.body.layer1.1.conv1.weight\n",
            "backbone.0.body.layer1.1.conv2.weight\n",
            "backbone.0.body.layer1.1.conv3.weight\n",
            "backbone.0.body.layer1.2.conv1.weight\n",
            "backbone.0.body.layer1.2.conv2.weight\n",
            "backbone.0.body.layer1.2.conv3.weight\n",
            "backbone.0.body.layer2.0.conv1.weight\n",
            "backbone.0.body.layer2.0.conv2.weight\n",
            "backbone.0.body.layer2.0.conv3.weight\n",
            "backbone.0.body.layer2.0.downsample.0.weight\n",
            "backbone.0.body.layer2.1.conv1.weight\n",
            "backbone.0.body.layer2.1.conv2.weight\n",
            "backbone.0.body.layer2.1.conv3.weight\n",
            "backbone.0.body.layer2.2.conv1.weight\n",
            "backbone.0.body.layer2.2.conv2.weight\n",
            "backbone.0.body.layer2.2.conv3.weight\n",
            "backbone.0.body.layer2.3.conv1.weight\n",
            "backbone.0.body.layer2.3.conv2.weight\n",
            "backbone.0.body.layer2.3.conv3.weight\n",
            "backbone.0.body.layer3.0.conv1.weight\n",
            "backbone.0.body.layer3.0.conv2.weight\n",
            "backbone.0.body.layer3.0.conv3.weight\n",
            "backbone.0.body.layer3.0.downsample.0.weight\n",
            "backbone.0.body.layer3.1.conv1.weight\n",
            "backbone.0.body.layer3.1.conv2.weight\n",
            "backbone.0.body.layer3.1.conv3.weight\n",
            "backbone.0.body.layer3.2.conv1.weight\n",
            "backbone.0.body.layer3.2.conv2.weight\n",
            "backbone.0.body.layer3.2.conv3.weight\n",
            "backbone.0.body.layer3.3.conv1.weight\n",
            "backbone.0.body.layer3.3.conv2.weight\n",
            "backbone.0.body.layer3.3.conv3.weight\n",
            "backbone.0.body.layer3.4.conv1.weight\n",
            "backbone.0.body.layer3.4.conv2.weight\n",
            "backbone.0.body.layer3.4.conv3.weight\n",
            "backbone.0.body.layer3.5.conv1.weight\n",
            "backbone.0.body.layer3.5.conv2.weight\n",
            "backbone.0.body.layer3.5.conv3.weight\n",
            "backbone.0.body.layer4.0.conv1.weight\n",
            "backbone.0.body.layer4.0.conv2.weight\n",
            "backbone.0.body.layer4.0.conv3.weight\n",
            "backbone.0.body.layer4.0.downsample.0.weight\n",
            "backbone.0.body.layer4.1.conv1.weight\n",
            "backbone.0.body.layer4.1.conv2.weight\n",
            "backbone.0.body.layer4.1.conv3.weight\n",
            "backbone.0.body.layer4.2.conv1.weight\n",
            "backbone.0.body.layer4.2.conv2.weight\n",
            "backbone.0.body.layer4.2.conv3.weight\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Test:  [  0/417]  eta: 4:50:54  class_error: 5.26  loss: 7.3530 (7.3530)  loss_ce: 0.4069 (0.4069)  loss_bbox: 0.1992 (0.1992)  loss_giou: 0.5511 (0.5511)  loss_ce_0: 0.4306 (0.4306)  loss_bbox_0: 0.2840 (0.2840)  loss_giou_0: 0.7329 (0.7329)  loss_ce_1: 0.4452 (0.4452)  loss_bbox_1: 0.2106 (0.2106)  loss_giou_1: 0.5763 (0.5763)  loss_ce_2: 0.4367 (0.4367)  loss_bbox_2: 0.2018 (0.2018)  loss_giou_2: 0.5546 (0.5546)  loss_ce_3: 0.4117 (0.4117)  loss_bbox_3: 0.2043 (0.2043)  loss_giou_3: 0.5444 (0.5444)  loss_ce_4: 0.4156 (0.4156)  loss_bbox_4: 0.1984 (0.1984)  loss_giou_4: 0.5487 (0.5487)  loss_ce_unscaled: 0.2035 (0.2035)  class_error_unscaled: 5.2632 (5.2632)  loss_bbox_unscaled: 0.0398 (0.0398)  loss_giou_unscaled: 0.2756 (0.2756)  cardinality_error_unscaled: 292.0833 (292.0833)  loss_ce_0_unscaled: 0.2153 (0.2153)  loss_bbox_0_unscaled: 0.0568 (0.0568)  loss_giou_0_unscaled: 0.3664 (0.3664)  cardinality_error_0_unscaled: 292.0833 (292.0833)  loss_ce_1_unscaled: 0.2226 (0.2226)  loss_bbox_1_unscaled: 0.0421 (0.0421)  loss_giou_1_unscaled: 0.2881 (0.2881)  cardinality_error_1_unscaled: 292.0833 (292.0833)  loss_ce_2_unscaled: 0.2183 (0.2183)  loss_bbox_2_unscaled: 0.0404 (0.0404)  loss_giou_2_unscaled: 0.2773 (0.2773)  cardinality_error_2_unscaled: 292.0000 (292.0000)  loss_ce_3_unscaled: 0.2059 (0.2059)  loss_bbox_3_unscaled: 0.0409 (0.0409)  loss_giou_3_unscaled: 0.2722 (0.2722)  cardinality_error_3_unscaled: 292.0833 (292.0833)  loss_ce_4_unscaled: 0.2078 (0.2078)  loss_bbox_4_unscaled: 0.0397 (0.0397)  loss_giou_4_unscaled: 0.2743 (0.2743)  cardinality_error_4_unscaled: 292.0833 (292.0833)  time: 41.8566  data: 36.5714  max mem: 6678\n",
            "Test:  [ 10/417]  eta: 0:43:11  class_error: 7.07  loss: 6.8184 (7.0529)  loss_ce: 0.4069 (0.4137)  loss_bbox: 0.1992 (0.2141)  loss_giou: 0.5089 (0.4942)  loss_ce_0: 0.4795 (0.4764)  loss_bbox_0: 0.2784 (0.2821)  loss_giou_0: 0.6137 (0.6241)  loss_ce_1: 0.4468 (0.4432)  loss_bbox_1: 0.2110 (0.2208)  loss_giou_1: 0.5161 (0.5076)  loss_ce_2: 0.4183 (0.4230)  loss_bbox_2: 0.2024 (0.2145)  loss_giou_2: 0.5106 (0.4968)  loss_ce_3: 0.4117 (0.4151)  loss_bbox_3: 0.2043 (0.2146)  loss_giou_3: 0.5099 (0.4939)  loss_ce_4: 0.4156 (0.4141)  loss_bbox_4: 0.1986 (0.2120)  loss_giou_4: 0.5054 (0.4928)  loss_ce_unscaled: 0.2035 (0.2069)  class_error_unscaled: 8.7912 (8.4277)  loss_bbox_unscaled: 0.0398 (0.0428)  loss_giou_unscaled: 0.2545 (0.2471)  cardinality_error_unscaled: 292.0833 (292.7197)  loss_ce_0_unscaled: 0.2397 (0.2382)  loss_bbox_0_unscaled: 0.0557 (0.0564)  loss_giou_0_unscaled: 0.3069 (0.3121)  cardinality_error_0_unscaled: 292.1667 (292.5227)  loss_ce_1_unscaled: 0.2234 (0.2216)  loss_bbox_1_unscaled: 0.0422 (0.0442)  loss_giou_1_unscaled: 0.2581 (0.2538)  cardinality_error_1_unscaled: 292.3333 (292.5606)  loss_ce_2_unscaled: 0.2092 (0.2115)  loss_bbox_2_unscaled: 0.0405 (0.0429)  loss_giou_2_unscaled: 0.2553 (0.2484)  cardinality_error_2_unscaled: 291.9167 (292.2955)  loss_ce_3_unscaled: 0.2059 (0.2075)  loss_bbox_3_unscaled: 0.0409 (0.0429)  loss_giou_3_unscaled: 0.2549 (0.2469)  cardinality_error_3_unscaled: 292.0833 (292.3864)  loss_ce_4_unscaled: 0.2078 (0.2070)  loss_bbox_4_unscaled: 0.0397 (0.0424)  loss_giou_4_unscaled: 0.2527 (0.2464)  cardinality_error_4_unscaled: 292.0833 (292.6136)  time: 6.3680  data: 3.3504  max mem: 7374\n",
            "Test:  [ 20/417]  eta: 0:30:55  class_error: 8.51  loss: 7.1145 (7.1810)  loss_ce: 0.4163 (0.4218)  loss_bbox: 0.2212 (0.2209)  loss_giou: 0.5089 (0.5004)  loss_ce_0: 0.4817 (0.4877)  loss_bbox_0: 0.2784 (0.2841)  loss_giou_0: 0.6137 (0.6270)  loss_ce_1: 0.4582 (0.4542)  loss_bbox_1: 0.2218 (0.2286)  loss_giou_1: 0.5102 (0.5164)  loss_ce_2: 0.4183 (0.4365)  loss_bbox_2: 0.2182 (0.2189)  loss_giou_2: 0.5106 (0.5000)  loss_ce_3: 0.4073 (0.4238)  loss_bbox_3: 0.2222 (0.2199)  loss_giou_3: 0.5074 (0.5001)  loss_ce_4: 0.4146 (0.4217)  loss_bbox_4: 0.2211 (0.2193)  loss_giou_4: 0.5051 (0.4997)  loss_ce_unscaled: 0.2082 (0.2109)  class_error_unscaled: 9.5238 (9.5937)  loss_bbox_unscaled: 0.0442 (0.0442)  loss_giou_unscaled: 0.2545 (0.2502)  cardinality_error_unscaled: 292.1667 (292.8452)  loss_ce_0_unscaled: 0.2409 (0.2439)  loss_bbox_0_unscaled: 0.0557 (0.0568)  loss_giou_0_unscaled: 0.3069 (0.3135)  cardinality_error_0_unscaled: 292.1667 (292.6429)  loss_ce_1_unscaled: 0.2291 (0.2271)  loss_bbox_1_unscaled: 0.0444 (0.0457)  loss_giou_1_unscaled: 0.2551 (0.2582)  cardinality_error_1_unscaled: 292.3333 (292.7341)  loss_ce_2_unscaled: 0.2092 (0.2183)  loss_bbox_2_unscaled: 0.0436 (0.0438)  loss_giou_2_unscaled: 0.2553 (0.2500)  cardinality_error_2_unscaled: 291.9167 (292.3968)  loss_ce_3_unscaled: 0.2036 (0.2119)  loss_bbox_3_unscaled: 0.0444 (0.0440)  loss_giou_3_unscaled: 0.2537 (0.2501)  cardinality_error_3_unscaled: 292.0833 (292.5159)  loss_ce_4_unscaled: 0.2073 (0.2109)  loss_bbox_4_unscaled: 0.0442 (0.0439)  loss_giou_4_unscaled: 0.2525 (0.2498)  cardinality_error_4_unscaled: 292.1667 (292.7698)  time: 2.8150  data: 0.0280  max mem: 7755\n",
            "Test:  [ 30/417]  eta: 0:26:47  class_error: 4.23  loss: 7.3548 (7.3347)  loss_ce: 0.4123 (0.4242)  loss_bbox: 0.2212 (0.2170)  loss_giou: 0.5482 (0.5254)  loss_ce_0: 0.4817 (0.4858)  loss_bbox_0: 0.2658 (0.2808)  loss_giou_0: 0.6634 (0.6608)  loss_ce_1: 0.4534 (0.4573)  loss_bbox_1: 0.2139 (0.2258)  loss_giou_1: 0.5635 (0.5425)  loss_ce_2: 0.4250 (0.4390)  loss_bbox_2: 0.2106 (0.2157)  loss_giou_2: 0.5436 (0.5259)  loss_ce_3: 0.4072 (0.4267)  loss_bbox_3: 0.2214 (0.2171)  loss_giou_3: 0.5436 (0.5259)  loss_ce_4: 0.4146 (0.4236)  loss_bbox_4: 0.2201 (0.2160)  loss_giou_4: 0.5486 (0.5253)  loss_ce_unscaled: 0.2061 (0.2121)  class_error_unscaled: 9.2199 (9.2797)  loss_bbox_unscaled: 0.0442 (0.0434)  loss_giou_unscaled: 0.2741 (0.2627)  cardinality_error_unscaled: 291.5833 (292.3952)  loss_ce_0_unscaled: 0.2409 (0.2429)  loss_bbox_0_unscaled: 0.0532 (0.0562)  loss_giou_0_unscaled: 0.3317 (0.3304)  cardinality_error_0_unscaled: 291.3333 (292.2635)  loss_ce_1_unscaled: 0.2267 (0.2286)  loss_bbox_1_unscaled: 0.0428 (0.0452)  loss_giou_1_unscaled: 0.2817 (0.2712)  cardinality_error_1_unscaled: 291.5000 (292.3118)  loss_ce_2_unscaled: 0.2125 (0.2195)  loss_bbox_2_unscaled: 0.0421 (0.0431)  loss_giou_2_unscaled: 0.2718 (0.2630)  cardinality_error_2_unscaled: 290.7500 (292.0135)  loss_ce_3_unscaled: 0.2036 (0.2133)  loss_bbox_3_unscaled: 0.0443 (0.0434)  loss_giou_3_unscaled: 0.2718 (0.2630)  cardinality_error_3_unscaled: 291.5833 (291.9812)  loss_ce_4_unscaled: 0.2073 (0.2118)  loss_bbox_4_unscaled: 0.0440 (0.0432)  loss_giou_4_unscaled: 0.2743 (0.2626)  cardinality_error_4_unscaled: 291.5833 (292.3253)  time: 2.9348  data: 0.0287  max mem: 8137\n",
            "Test:  [ 40/417]  eta: 0:24:45  class_error: 12.61  loss: 7.1631 (7.3126)  loss_ce: 0.3974 (0.4182)  loss_bbox: 0.2107 (0.2192)  loss_giou: 0.5290 (0.5264)  loss_ce_0: 0.4643 (0.4807)  loss_bbox_0: 0.2631 (0.2808)  loss_giou_0: 0.6642 (0.6566)  loss_ce_1: 0.4250 (0.4509)  loss_bbox_1: 0.2185 (0.2277)  loss_giou_1: 0.5508 (0.5438)  loss_ce_2: 0.4115 (0.4326)  loss_bbox_2: 0.2096 (0.2171)  loss_giou_2: 0.5233 (0.5277)  loss_ce_3: 0.4034 (0.4210)  loss_bbox_3: 0.2103 (0.2196)  loss_giou_3: 0.5244 (0.5269)  loss_ce_4: 0.3970 (0.4194)  loss_bbox_4: 0.2109 (0.2178)  loss_giou_4: 0.5286 (0.5260)  loss_ce_unscaled: 0.1987 (0.2091)  class_error_unscaled: 5.4795 (8.5606)  loss_bbox_unscaled: 0.0421 (0.0438)  loss_giou_unscaled: 0.2645 (0.2632)  cardinality_error_unscaled: 293.3333 (292.7358)  loss_ce_0_unscaled: 0.2322 (0.2404)  loss_bbox_0_unscaled: 0.0526 (0.0562)  loss_giou_0_unscaled: 0.3321 (0.3283)  cardinality_error_0_unscaled: 293.2500 (292.6321)  loss_ce_1_unscaled: 0.2125 (0.2255)  loss_bbox_1_unscaled: 0.0437 (0.0455)  loss_giou_1_unscaled: 0.2754 (0.2719)  cardinality_error_1_unscaled: 293.2500 (292.6626)  loss_ce_2_unscaled: 0.2057 (0.2163)  loss_bbox_2_unscaled: 0.0419 (0.0434)  loss_giou_2_unscaled: 0.2616 (0.2639)  cardinality_error_2_unscaled: 293.0000 (292.4045)  loss_ce_3_unscaled: 0.2017 (0.2105)  loss_bbox_3_unscaled: 0.0421 (0.0439)  loss_giou_3_unscaled: 0.2622 (0.2634)  cardinality_error_3_unscaled: 292.9167 (292.3882)  loss_ce_4_unscaled: 0.1985 (0.2097)  loss_bbox_4_unscaled: 0.0422 (0.0436)  loss_giou_4_unscaled: 0.2643 (0.2630)  cardinality_error_4_unscaled: 293.4167 (292.6850)  time: 3.1699  data: 0.0312  max mem: 8137\n",
            "Test:  [ 50/417]  eta: 0:23:08  class_error: 2.22  loss: 7.3042 (7.4070)  loss_ce: 0.4129 (0.4214)  loss_bbox: 0.2218 (0.2257)  loss_giou: 0.5327 (0.5335)  loss_ce_0: 0.4881 (0.4836)  loss_bbox_0: 0.2744 (0.2864)  loss_giou_0: 0.6229 (0.6607)  loss_ce_1: 0.4556 (0.4541)  loss_bbox_1: 0.2351 (0.2334)  loss_giou_1: 0.5366 (0.5497)  loss_ce_2: 0.4247 (0.4360)  loss_bbox_2: 0.2225 (0.2238)  loss_giou_2: 0.5182 (0.5342)  loss_ce_3: 0.4351 (0.4244)  loss_bbox_3: 0.2218 (0.2257)  loss_giou_3: 0.5245 (0.5339)  loss_ce_4: 0.4388 (0.4233)  loss_bbox_4: 0.2219 (0.2243)  loss_giou_4: 0.5247 (0.5329)  loss_ce_unscaled: 0.2064 (0.2107)  class_error_unscaled: 8.3333 (8.9399)  loss_bbox_unscaled: 0.0444 (0.0451)  loss_giou_unscaled: 0.2663 (0.2667)  cardinality_error_unscaled: 292.8333 (292.5016)  loss_ce_0_unscaled: 0.2440 (0.2418)  loss_bbox_0_unscaled: 0.0549 (0.0573)  loss_giou_0_unscaled: 0.3115 (0.3304)  cardinality_error_0_unscaled: 292.8333 (292.3775)  loss_ce_1_unscaled: 0.2278 (0.2271)  loss_bbox_1_unscaled: 0.0470 (0.0467)  loss_giou_1_unscaled: 0.2683 (0.2748)  cardinality_error_1_unscaled: 292.8333 (292.4134)  loss_ce_2_unscaled: 0.2124 (0.2180)  loss_bbox_2_unscaled: 0.0445 (0.0448)  loss_giou_2_unscaled: 0.2591 (0.2671)  cardinality_error_2_unscaled: 292.6667 (292.1373)  loss_ce_3_unscaled: 0.2175 (0.2122)  loss_bbox_3_unscaled: 0.0444 (0.0451)  loss_giou_3_unscaled: 0.2623 (0.2670)  cardinality_error_3_unscaled: 292.5000 (292.1128)  loss_ce_4_unscaled: 0.2194 (0.2116)  loss_bbox_4_unscaled: 0.0444 (0.0449)  loss_giou_4_unscaled: 0.2623 (0.2665)  cardinality_error_4_unscaled: 292.7500 (292.4477)  time: 3.2089  data: 0.0318  max mem: 8138\n",
            "Test:  [ 60/417]  eta: 0:21:52  class_error: 17.05  loss: 7.5569 (7.4048)  loss_ce: 0.4307 (0.4232)  loss_bbox: 0.2175 (0.2249)  loss_giou: 0.5327 (0.5326)  loss_ce_0: 0.4881 (0.4837)  loss_bbox_0: 0.2798 (0.2860)  loss_giou_0: 0.6414 (0.6605)  loss_ce_1: 0.4556 (0.4545)  loss_bbox_1: 0.2267 (0.2331)  loss_giou_1: 0.5530 (0.5491)  loss_ce_2: 0.4358 (0.4364)  loss_bbox_2: 0.2186 (0.2232)  loss_giou_2: 0.5246 (0.5335)  loss_ce_3: 0.4390 (0.4258)  loss_bbox_3: 0.2197 (0.2247)  loss_giou_3: 0.5321 (0.5332)  loss_ce_4: 0.4388 (0.4254)  loss_bbox_4: 0.2163 (0.2234)  loss_giou_4: 0.5295 (0.5315)  loss_ce_unscaled: 0.2154 (0.2116)  class_error_unscaled: 10.2041 (9.1942)  loss_bbox_unscaled: 0.0435 (0.0450)  loss_giou_unscaled: 0.2663 (0.2663)  cardinality_error_unscaled: 291.0833 (292.3812)  loss_ce_0_unscaled: 0.2440 (0.2418)  loss_bbox_0_unscaled: 0.0560 (0.0572)  loss_giou_0_unscaled: 0.3207 (0.3303)  cardinality_error_0_unscaled: 290.8333 (292.2869)  loss_ce_1_unscaled: 0.2278 (0.2272)  loss_bbox_1_unscaled: 0.0453 (0.0466)  loss_giou_1_unscaled: 0.2765 (0.2746)  cardinality_error_1_unscaled: 290.9167 (292.3115)  loss_ce_2_unscaled: 0.2179 (0.2182)  loss_bbox_2_unscaled: 0.0437 (0.0446)  loss_giou_2_unscaled: 0.2623 (0.2668)  cardinality_error_2_unscaled: 290.5833 (292.0205)  loss_ce_3_unscaled: 0.2195 (0.2129)  loss_bbox_3_unscaled: 0.0439 (0.0449)  loss_giou_3_unscaled: 0.2661 (0.2666)  cardinality_error_3_unscaled: 290.6667 (292.0205)  loss_ce_4_unscaled: 0.2194 (0.2127)  loss_bbox_4_unscaled: 0.0433 (0.0447)  loss_giou_4_unscaled: 0.2647 (0.2658)  cardinality_error_4_unscaled: 291.0833 (292.3060)  time: 3.1388  data: 0.0310  max mem: 8138\n",
            "Test:  [ 70/417]  eta: 0:20:49  class_error: 15.05  loss: 6.9459 (7.3451)  loss_ce: 0.4194 (0.4220)  loss_bbox: 0.2108 (0.2224)  loss_giou: 0.4863 (0.5262)  loss_ce_0: 0.4742 (0.4820)  loss_bbox_0: 0.2787 (0.2855)  loss_giou_0: 0.6414 (0.6559)  loss_ce_1: 0.4425 (0.4523)  loss_bbox_1: 0.2211 (0.2309)  loss_giou_1: 0.5201 (0.5433)  loss_ce_2: 0.4166 (0.4340)  loss_bbox_2: 0.2081 (0.2210)  loss_giou_2: 0.4968 (0.5270)  loss_ce_3: 0.4083 (0.4229)  loss_bbox_3: 0.2098 (0.2227)  loss_giou_3: 0.4923 (0.5269)  loss_ce_4: 0.4083 (0.4238)  loss_bbox_4: 0.2038 (0.2210)  loss_giou_4: 0.4920 (0.5252)  loss_ce_unscaled: 0.2097 (0.2110)  class_error_unscaled: 9.1743 (9.0685)  loss_bbox_unscaled: 0.0422 (0.0445)  loss_giou_unscaled: 0.2431 (0.2631)  cardinality_error_unscaled: 291.0000 (292.4096)  loss_ce_0_unscaled: 0.2371 (0.2410)  loss_bbox_0_unscaled: 0.0557 (0.0571)  loss_giou_0_unscaled: 0.3207 (0.3279)  cardinality_error_0_unscaled: 290.9167 (292.2840)  loss_ce_1_unscaled: 0.2213 (0.2261)  loss_bbox_1_unscaled: 0.0442 (0.0462)  loss_giou_1_unscaled: 0.2600 (0.2716)  cardinality_error_1_unscaled: 291.0000 (292.2958)  loss_ce_2_unscaled: 0.2083 (0.2170)  loss_bbox_2_unscaled: 0.0416 (0.0442)  loss_giou_2_unscaled: 0.2484 (0.2635)  cardinality_error_2_unscaled: 290.8333 (292.0000)  loss_ce_3_unscaled: 0.2042 (0.2115)  loss_bbox_3_unscaled: 0.0420 (0.0445)  loss_giou_3_unscaled: 0.2462 (0.2635)  cardinality_error_3_unscaled: 291.0833 (291.9836)  loss_ce_4_unscaled: 0.2041 (0.2119)  loss_bbox_4_unscaled: 0.0408 (0.0442)  loss_giou_4_unscaled: 0.2460 (0.2626)  cardinality_error_4_unscaled: 291.0833 (292.3416)  time: 3.1323  data: 0.0312  max mem: 8138\n",
            "Test:  [ 80/417]  eta: 0:19:55  class_error: 12.50  loss: 6.8671 (7.2891)  loss_ce: 0.4285 (0.4241)  loss_bbox: 0.2083 (0.2197)  loss_giou: 0.4646 (0.5170)  loss_ce_0: 0.4818 (0.4829)  loss_bbox_0: 0.2768 (0.2851)  loss_giou_0: 0.5945 (0.6481)  loss_ce_1: 0.4563 (0.4532)  loss_bbox_1: 0.2181 (0.2289)  loss_giou_1: 0.4799 (0.5342)  loss_ce_2: 0.4198 (0.4363)  loss_bbox_2: 0.2033 (0.2184)  loss_giou_2: 0.4652 (0.5178)  loss_ce_3: 0.4237 (0.4250)  loss_bbox_3: 0.2056 (0.2200)  loss_giou_3: 0.4733 (0.5179)  loss_ce_4: 0.4247 (0.4259)  loss_bbox_4: 0.2039 (0.2185)  loss_giou_4: 0.4662 (0.5162)  loss_ce_unscaled: 0.2142 (0.2120)  class_error_unscaled: 9.1743 (9.1538)  loss_bbox_unscaled: 0.0417 (0.0439)  loss_giou_unscaled: 0.2323 (0.2585)  cardinality_error_unscaled: 293.2500 (292.5370)  loss_ce_0_unscaled: 0.2409 (0.2415)  loss_bbox_0_unscaled: 0.0554 (0.0570)  loss_giou_0_unscaled: 0.2972 (0.3241)  cardinality_error_0_unscaled: 293.0833 (292.4074)  loss_ce_1_unscaled: 0.2282 (0.2266)  loss_bbox_1_unscaled: 0.0436 (0.0458)  loss_giou_1_unscaled: 0.2399 (0.2671)  cardinality_error_1_unscaled: 293.0833 (292.3796)  loss_ce_2_unscaled: 0.2099 (0.2181)  loss_bbox_2_unscaled: 0.0407 (0.0437)  loss_giou_2_unscaled: 0.2326 (0.2589)  cardinality_error_2_unscaled: 292.9167 (292.0689)  loss_ce_3_unscaled: 0.2118 (0.2125)  loss_bbox_3_unscaled: 0.0411 (0.0440)  loss_giou_3_unscaled: 0.2366 (0.2589)  cardinality_error_3_unscaled: 292.2500 (292.0226)  loss_ce_4_unscaled: 0.2123 (0.2129)  loss_bbox_4_unscaled: 0.0408 (0.0437)  loss_giou_4_unscaled: 0.2331 (0.2581)  cardinality_error_4_unscaled: 293.0000 (292.4249)  time: 3.1554  data: 0.0300  max mem: 8138\n",
            "Test:  [ 90/417]  eta: 0:19:04  class_error: 9.82  loss: 7.2061 (7.3243)  loss_ce: 0.4428 (0.4261)  loss_bbox: 0.2137 (0.2210)  loss_giou: 0.4802 (0.5197)  loss_ce_0: 0.4916 (0.4860)  loss_bbox_0: 0.2660 (0.2846)  loss_giou_0: 0.6387 (0.6504)  loss_ce_1: 0.4563 (0.4561)  loss_bbox_1: 0.2135 (0.2288)  loss_giou_1: 0.5097 (0.5367)  loss_ce_2: 0.4457 (0.4387)  loss_bbox_2: 0.2086 (0.2195)  loss_giou_2: 0.4815 (0.5208)  loss_ce_3: 0.4433 (0.4272)  loss_bbox_3: 0.2149 (0.2213)  loss_giou_3: 0.4799 (0.5207)  loss_ce_4: 0.4412 (0.4278)  loss_bbox_4: 0.2099 (0.2200)  loss_giou_4: 0.4799 (0.5189)  loss_ce_unscaled: 0.2214 (0.2131)  class_error_unscaled: 10.3896 (9.3547)  loss_bbox_unscaled: 0.0427 (0.0442)  loss_giou_unscaled: 0.2401 (0.2598)  cardinality_error_unscaled: 293.1667 (292.4835)  loss_ce_0_unscaled: 0.2458 (0.2430)  loss_bbox_0_unscaled: 0.0532 (0.0569)  loss_giou_0_unscaled: 0.3194 (0.3252)  cardinality_error_0_unscaled: 292.8333 (292.3517)  loss_ce_1_unscaled: 0.2282 (0.2281)  loss_bbox_1_unscaled: 0.0427 (0.0458)  loss_giou_1_unscaled: 0.2548 (0.2684)  cardinality_error_1_unscaled: 292.8333 (292.3370)  loss_ce_2_unscaled: 0.2228 (0.2194)  loss_bbox_2_unscaled: 0.0417 (0.0439)  loss_giou_2_unscaled: 0.2408 (0.2604)  cardinality_error_2_unscaled: 292.8333 (292.0540)  loss_ce_3_unscaled: 0.2217 (0.2136)  loss_bbox_3_unscaled: 0.0430 (0.0443)  loss_giou_3_unscaled: 0.2400 (0.2603)  cardinality_error_3_unscaled: 292.4167 (292.0101)  loss_ce_4_unscaled: 0.2206 (0.2139)  loss_bbox_4_unscaled: 0.0420 (0.0440)  loss_giou_4_unscaled: 0.2400 (0.2594)  cardinality_error_4_unscaled: 292.8333 (292.3773)  time: 3.1514  data: 0.0292  max mem: 8138\n",
            "Test:  [100/417]  eta: 0:18:16  class_error: 7.04  loss: 7.2653 (7.2989)  loss_ce: 0.4126 (0.4229)  loss_bbox: 0.2172 (0.2210)  loss_giou: 0.5351 (0.5186)  loss_ce_0: 0.4621 (0.4830)  loss_bbox_0: 0.2701 (0.2838)  loss_giou_0: 0.6635 (0.6485)  loss_ce_1: 0.4477 (0.4533)  loss_bbox_1: 0.2135 (0.2287)  loss_giou_1: 0.5485 (0.5356)  loss_ce_2: 0.4266 (0.4357)  loss_bbox_2: 0.2124 (0.2200)  loss_giou_2: 0.5377 (0.5199)  loss_ce_3: 0.4126 (0.4243)  loss_bbox_3: 0.2188 (0.2215)  loss_giou_3: 0.5398 (0.5196)  loss_ce_4: 0.4083 (0.4246)  loss_bbox_4: 0.2184 (0.2202)  loss_giou_4: 0.5383 (0.5178)  loss_ce_unscaled: 0.2063 (0.2115)  class_error_unscaled: 10.3774 (9.3523)  loss_bbox_unscaled: 0.0434 (0.0442)  loss_giou_unscaled: 0.2675 (0.2593)  cardinality_error_unscaled: 292.0833 (292.4670)  loss_ce_0_unscaled: 0.2311 (0.2415)  loss_bbox_0_unscaled: 0.0540 (0.0568)  loss_giou_0_unscaled: 0.3318 (0.3243)  cardinality_error_0_unscaled: 291.8333 (292.3325)  loss_ce_1_unscaled: 0.2238 (0.2266)  loss_bbox_1_unscaled: 0.0427 (0.0457)  loss_giou_1_unscaled: 0.2742 (0.2678)  cardinality_error_1_unscaled: 291.7500 (292.3036)  loss_ce_2_unscaled: 0.2133 (0.2179)  loss_bbox_2_unscaled: 0.0425 (0.0440)  loss_giou_2_unscaled: 0.2688 (0.2600)  cardinality_error_2_unscaled: 291.7500 (292.0429)  loss_ce_3_unscaled: 0.2063 (0.2122)  loss_bbox_3_unscaled: 0.0438 (0.0443)  loss_giou_3_unscaled: 0.2699 (0.2598)  cardinality_error_3_unscaled: 291.4167 (291.9794)  loss_ce_4_unscaled: 0.2042 (0.2123)  loss_bbox_4_unscaled: 0.0437 (0.0440)  loss_giou_4_unscaled: 0.2691 (0.2589)  cardinality_error_4_unscaled: 291.7500 (292.3630)  time: 3.0897  data: 0.0293  max mem: 8138\n",
            "Test:  [110/417]  eta: 0:17:30  class_error: 17.14  loss: 6.9708 (7.3200)  loss_ce: 0.3917 (0.4250)  loss_bbox: 0.2200 (0.2215)  loss_giou: 0.4801 (0.5191)  loss_ce_0: 0.4760 (0.4863)  loss_bbox_0: 0.2701 (0.2841)  loss_giou_0: 0.6371 (0.6490)  loss_ce_1: 0.4499 (0.4559)  loss_bbox_1: 0.2245 (0.2293)  loss_giou_1: 0.4875 (0.5364)  loss_ce_2: 0.4223 (0.4379)  loss_bbox_2: 0.2142 (0.2206)  loss_giou_2: 0.4940 (0.5205)  loss_ce_3: 0.3984 (0.4260)  loss_bbox_3: 0.2217 (0.2222)  loss_giou_3: 0.4829 (0.5207)  loss_ce_4: 0.3937 (0.4264)  loss_bbox_4: 0.2157 (0.2207)  loss_giou_4: 0.4798 (0.5186)  loss_ce_unscaled: 0.1958 (0.2125)  class_error_unscaled: 8.9286 (9.4792)  loss_bbox_unscaled: 0.0440 (0.0443)  loss_giou_unscaled: 0.2401 (0.2595)  cardinality_error_unscaled: 292.0833 (292.4415)  loss_ce_0_unscaled: 0.2380 (0.2432)  loss_bbox_0_unscaled: 0.0540 (0.0568)  loss_giou_0_unscaled: 0.3186 (0.3245)  cardinality_error_0_unscaled: 291.8333 (292.3056)  loss_ce_1_unscaled: 0.2250 (0.2279)  loss_bbox_1_unscaled: 0.0449 (0.0459)  loss_giou_1_unscaled: 0.2437 (0.2682)  cardinality_error_1_unscaled: 291.7500 (292.2778)  loss_ce_2_unscaled: 0.2112 (0.2190)  loss_bbox_2_unscaled: 0.0428 (0.0441)  loss_giou_2_unscaled: 0.2470 (0.2602)  cardinality_error_2_unscaled: 291.7500 (292.0128)  loss_ce_3_unscaled: 0.1992 (0.2130)  loss_bbox_3_unscaled: 0.0443 (0.0444)  loss_giou_3_unscaled: 0.2414 (0.2603)  cardinality_error_3_unscaled: 291.4167 (291.9527)  loss_ce_4_unscaled: 0.1969 (0.2132)  loss_bbox_4_unscaled: 0.0431 (0.0441)  loss_giou_4_unscaled: 0.2399 (0.2593)  cardinality_error_4_unscaled: 291.7500 (292.3356)  time: 3.0654  data: 0.0294  max mem: 8138\n",
            "Test:  [120/417]  eta: 0:16:44  class_error: 12.73  loss: 7.4238 (7.3041)  loss_ce: 0.4204 (0.4227)  loss_bbox: 0.2114 (0.2213)  loss_giou: 0.4768 (0.5190)  loss_ce_0: 0.4864 (0.4853)  loss_bbox_0: 0.2555 (0.2824)  loss_giou_0: 0.6238 (0.6489)  loss_ce_1: 0.4499 (0.4543)  loss_bbox_1: 0.2093 (0.2282)  loss_giou_1: 0.5018 (0.5364)  loss_ce_2: 0.4357 (0.4359)  loss_bbox_2: 0.2077 (0.2199)  loss_giou_2: 0.4829 (0.5204)  loss_ce_3: 0.4263 (0.4243)  loss_bbox_3: 0.2022 (0.2216)  loss_giou_3: 0.4828 (0.5205)  loss_ce_4: 0.4228 (0.4244)  loss_bbox_4: 0.2157 (0.2203)  loss_giou_4: 0.4766 (0.5184)  loss_ce_unscaled: 0.2102 (0.2113)  class_error_unscaled: 9.9010 (9.5479)  loss_bbox_unscaled: 0.0423 (0.0443)  loss_giou_unscaled: 0.2384 (0.2595)  cardinality_error_unscaled: 291.4167 (292.3754)  loss_ce_0_unscaled: 0.2432 (0.2427)  loss_bbox_0_unscaled: 0.0511 (0.0565)  loss_giou_0_unscaled: 0.3119 (0.3244)  cardinality_error_0_unscaled: 291.1667 (292.2259)  loss_ce_1_unscaled: 0.2250 (0.2272)  loss_bbox_1_unscaled: 0.0419 (0.0456)  loss_giou_1_unscaled: 0.2509 (0.2682)  cardinality_error_1_unscaled: 291.2500 (292.1805)  loss_ce_2_unscaled: 0.2178 (0.2179)  loss_bbox_2_unscaled: 0.0415 (0.0440)  loss_giou_2_unscaled: 0.2414 (0.2602)  cardinality_error_2_unscaled: 291.0833 (291.8995)  loss_ce_3_unscaled: 0.2131 (0.2121)  loss_bbox_3_unscaled: 0.0404 (0.0443)  loss_giou_3_unscaled: 0.2414 (0.2603)  cardinality_error_3_unscaled: 291.2500 (291.8733)  loss_ce_4_unscaled: 0.2114 (0.2122)  loss_bbox_4_unscaled: 0.0431 (0.0441)  loss_giou_4_unscaled: 0.2383 (0.2592)  cardinality_error_4_unscaled: 291.3333 (292.2769)  time: 3.0017  data: 0.0292  max mem: 8138\n",
            "Test:  [130/417]  eta: 0:16:02  class_error: 19.67  loss: 7.1454 (7.2830)  loss_ce: 0.4040 (0.4212)  loss_bbox: 0.2114 (0.2214)  loss_giou: 0.4861 (0.5173)  loss_ce_0: 0.4605 (0.4841)  loss_bbox_0: 0.2555 (0.2822)  loss_giou_0: 0.6249 (0.6465)  loss_ce_1: 0.4288 (0.4530)  loss_bbox_1: 0.2055 (0.2278)  loss_giou_1: 0.5059 (0.5341)  loss_ce_2: 0.4084 (0.4342)  loss_bbox_2: 0.2077 (0.2201)  loss_giou_2: 0.4899 (0.5185)  loss_ce_3: 0.4008 (0.4225)  loss_bbox_3: 0.2052 (0.2217)  loss_giou_3: 0.4874 (0.5188)  loss_ce_4: 0.3966 (0.4226)  loss_bbox_4: 0.2166 (0.2204)  loss_giou_4: 0.4836 (0.5165)  loss_ce_unscaled: 0.2020 (0.2106)  class_error_unscaled: 11.2150 (9.7804)  loss_bbox_unscaled: 0.0423 (0.0443)  loss_giou_unscaled: 0.2431 (0.2586)  cardinality_error_unscaled: 291.5000 (292.3353)  loss_ce_0_unscaled: 0.2303 (0.2421)  loss_bbox_0_unscaled: 0.0511 (0.0564)  loss_giou_0_unscaled: 0.3124 (0.3232)  cardinality_error_0_unscaled: 291.0000 (292.1826)  loss_ce_1_unscaled: 0.2144 (0.2265)  loss_bbox_1_unscaled: 0.0411 (0.0456)  loss_giou_1_unscaled: 0.2529 (0.2670)  cardinality_error_1_unscaled: 290.9167 (292.1400)  loss_ce_2_unscaled: 0.2042 (0.2171)  loss_bbox_2_unscaled: 0.0415 (0.0440)  loss_giou_2_unscaled: 0.2449 (0.2592)  cardinality_error_2_unscaled: 290.8333 (291.8645)  loss_ce_3_unscaled: 0.2004 (0.2113)  loss_bbox_3_unscaled: 0.0410 (0.0443)  loss_giou_3_unscaled: 0.2437 (0.2594)  cardinality_error_3_unscaled: 291.5000 (291.8295)  loss_ce_4_unscaled: 0.1983 (0.2113)  loss_bbox_4_unscaled: 0.0433 (0.0441)  loss_giou_4_unscaled: 0.2418 (0.2583)  cardinality_error_4_unscaled: 291.3333 (292.2297)  time: 2.9624  data: 0.0290  max mem: 8138\n",
            "Test:  [140/417]  eta: 0:15:24  class_error: 17.00  loss: 7.2400 (7.2904)  loss_ce: 0.4040 (0.4222)  loss_bbox: 0.2082 (0.2213)  loss_giou: 0.5151 (0.5175)  loss_ce_0: 0.4678 (0.4843)  loss_bbox_0: 0.2790 (0.2821)  loss_giou_0: 0.6396 (0.6468)  loss_ce_1: 0.4497 (0.4541)  loss_bbox_1: 0.2282 (0.2281)  loss_giou_1: 0.5145 (0.5342)  loss_ce_2: 0.4206 (0.4356)  loss_bbox_2: 0.2063 (0.2201)  loss_giou_2: 0.5091 (0.5186)  loss_ce_3: 0.4101 (0.4241)  loss_bbox_3: 0.2123 (0.2217)  loss_giou_3: 0.5143 (0.5191)  loss_ce_4: 0.4066 (0.4235)  loss_bbox_4: 0.2070 (0.2203)  loss_giou_4: 0.5143 (0.5168)  loss_ce_unscaled: 0.2020 (0.2111)  class_error_unscaled: 11.4286 (9.9604)  loss_bbox_unscaled: 0.0416 (0.0443)  loss_giou_unscaled: 0.2576 (0.2587)  cardinality_error_unscaled: 292.0833 (292.2991)  loss_ce_0_unscaled: 0.2339 (0.2422)  loss_bbox_0_unscaled: 0.0558 (0.0564)  loss_giou_0_unscaled: 0.3198 (0.3234)  cardinality_error_0_unscaled: 292.0000 (292.1436)  loss_ce_1_unscaled: 0.2249 (0.2270)  loss_bbox_1_unscaled: 0.0456 (0.0456)  loss_giou_1_unscaled: 0.2572 (0.2671)  cardinality_error_1_unscaled: 291.9167 (292.1076)  loss_ce_2_unscaled: 0.2103 (0.2178)  loss_bbox_2_unscaled: 0.0413 (0.0440)  loss_giou_2_unscaled: 0.2546 (0.2593)  cardinality_error_2_unscaled: 291.7500 (291.8316)  loss_ce_3_unscaled: 0.2050 (0.2120)  loss_bbox_3_unscaled: 0.0425 (0.0443)  loss_giou_3_unscaled: 0.2571 (0.2595)  cardinality_error_3_unscaled: 291.5000 (291.7784)  loss_ce_4_unscaled: 0.2033 (0.2118)  loss_bbox_4_unscaled: 0.0414 (0.0441)  loss_giou_4_unscaled: 0.2572 (0.2584)  cardinality_error_4_unscaled: 291.7500 (292.1933)  time: 3.0688  data: 0.0298  max mem: 8138\n",
            "Test:  [150/417]  eta: 0:14:46  class_error: 6.00  loss: 7.2462 (7.2869)  loss_ce: 0.4147 (0.4224)  loss_bbox: 0.2082 (0.2213)  loss_giou: 0.5148 (0.5165)  loss_ce_0: 0.4678 (0.4842)  loss_bbox_0: 0.2788 (0.2827)  loss_giou_0: 0.6396 (0.6459)  loss_ce_1: 0.4552 (0.4543)  loss_bbox_1: 0.2169 (0.2282)  loss_giou_1: 0.5203 (0.5332)  loss_ce_2: 0.4254 (0.4362)  loss_bbox_2: 0.2063 (0.2202)  loss_giou_2: 0.5031 (0.5175)  loss_ce_3: 0.4160 (0.4246)  loss_bbox_3: 0.2100 (0.2217)  loss_giou_3: 0.5072 (0.5180)  loss_ce_4: 0.4115 (0.4239)  loss_bbox_4: 0.2070 (0.2204)  loss_giou_4: 0.5150 (0.5158)  loss_ce_unscaled: 0.2074 (0.2112)  class_error_unscaled: 8.5714 (9.9742)  loss_bbox_unscaled: 0.0416 (0.0443)  loss_giou_unscaled: 0.2574 (0.2582)  cardinality_error_unscaled: 292.9167 (292.3262)  loss_ce_0_unscaled: 0.2339 (0.2421)  loss_bbox_0_unscaled: 0.0558 (0.0565)  loss_giou_0_unscaled: 0.3198 (0.3229)  cardinality_error_0_unscaled: 292.5833 (292.1661)  loss_ce_1_unscaled: 0.2276 (0.2272)  loss_bbox_1_unscaled: 0.0434 (0.0456)  loss_giou_1_unscaled: 0.2601 (0.2666)  cardinality_error_1_unscaled: 292.5000 (292.1308)  loss_ce_2_unscaled: 0.2127 (0.2181)  loss_bbox_2_unscaled: 0.0413 (0.0440)  loss_giou_2_unscaled: 0.2516 (0.2587)  cardinality_error_2_unscaled: 292.1667 (291.8587)  loss_ce_3_unscaled: 0.2080 (0.2123)  loss_bbox_3_unscaled: 0.0420 (0.0443)  loss_giou_3_unscaled: 0.2536 (0.2590)  cardinality_error_3_unscaled: 292.0833 (291.8041)  loss_ce_4_unscaled: 0.2057 (0.2120)  loss_bbox_4_unscaled: 0.0414 (0.0441)  loss_giou_4_unscaled: 0.2575 (0.2579)  cardinality_error_4_unscaled: 292.8333 (292.2175)  time: 3.0967  data: 0.0293  max mem: 8138\n",
            "Test:  [160/417]  eta: 0:14:08  class_error: 11.00  loss: 7.0579 (7.2868)  loss_ce: 0.4070 (0.4224)  loss_bbox: 0.2105 (0.2211)  loss_giou: 0.5044 (0.5168)  loss_ce_0: 0.4573 (0.4834)  loss_bbox_0: 0.2826 (0.2820)  loss_giou_0: 0.6359 (0.6459)  loss_ce_1: 0.4410 (0.4537)  loss_bbox_1: 0.2162 (0.2282)  loss_giou_1: 0.5214 (0.5338)  loss_ce_2: 0.4195 (0.4366)  loss_bbox_2: 0.2178 (0.2201)  loss_giou_2: 0.5041 (0.5180)  loss_ce_3: 0.4006 (0.4247)  loss_bbox_3: 0.2165 (0.2216)  loss_giou_3: 0.5055 (0.5183)  loss_ce_4: 0.4027 (0.4239)  loss_bbox_4: 0.2154 (0.2203)  loss_giou_4: 0.5039 (0.5161)  loss_ce_unscaled: 0.2035 (0.2112)  class_error_unscaled: 8.5714 (10.0223)  loss_bbox_unscaled: 0.0421 (0.0442)  loss_giou_unscaled: 0.2522 (0.2584)  cardinality_error_unscaled: 291.9167 (292.3189)  loss_ce_0_unscaled: 0.2286 (0.2417)  loss_bbox_0_unscaled: 0.0565 (0.0564)  loss_giou_0_unscaled: 0.3179 (0.3229)  cardinality_error_0_unscaled: 292.0833 (292.1698)  loss_ce_1_unscaled: 0.2205 (0.2268)  loss_bbox_1_unscaled: 0.0432 (0.0456)  loss_giou_1_unscaled: 0.2607 (0.2669)  cardinality_error_1_unscaled: 292.0833 (292.1310)  loss_ce_2_unscaled: 0.2097 (0.2183)  loss_bbox_2_unscaled: 0.0436 (0.0440)  loss_giou_2_unscaled: 0.2520 (0.2590)  cardinality_error_2_unscaled: 291.9167 (291.8716)  loss_ce_3_unscaled: 0.2003 (0.2124)  loss_bbox_3_unscaled: 0.0433 (0.0443)  loss_giou_3_unscaled: 0.2527 (0.2591)  cardinality_error_3_unscaled: 292.0000 (291.8220)  loss_ce_4_unscaled: 0.2014 (0.2120)  loss_bbox_4_unscaled: 0.0431 (0.0441)  loss_giou_4_unscaled: 0.2519 (0.2581)  cardinality_error_4_unscaled: 291.6667 (292.2169)  time: 3.0441  data: 0.0285  max mem: 8138\n",
            "Test:  [170/417]  eta: 0:13:29  class_error: 10.11  loss: 7.4181 (7.3146)  loss_ce: 0.4070 (0.4229)  loss_bbox: 0.2265 (0.2230)  loss_giou: 0.5130 (0.5189)  loss_ce_0: 0.4685 (0.4847)  loss_bbox_0: 0.2759 (0.2832)  loss_giou_0: 0.6464 (0.6476)  loss_ce_1: 0.4410 (0.4548)  loss_bbox_1: 0.2226 (0.2299)  loss_giou_1: 0.5253 (0.5359)  loss_ce_2: 0.4247 (0.4378)  loss_bbox_2: 0.2178 (0.2216)  loss_giou_2: 0.5228 (0.5202)  loss_ce_3: 0.4006 (0.4252)  loss_bbox_3: 0.2270 (0.2236)  loss_giou_3: 0.5144 (0.5204)  loss_ce_4: 0.4027 (0.4244)  loss_bbox_4: 0.2192 (0.2221)  loss_giou_4: 0.5059 (0.5183)  loss_ce_unscaled: 0.2035 (0.2115)  class_error_unscaled: 9.5238 (9.9755)  loss_bbox_unscaled: 0.0453 (0.0446)  loss_giou_unscaled: 0.2565 (0.2595)  cardinality_error_unscaled: 291.5000 (292.2690)  loss_ce_0_unscaled: 0.2343 (0.2424)  loss_bbox_0_unscaled: 0.0552 (0.0566)  loss_giou_0_unscaled: 0.3232 (0.3238)  cardinality_error_0_unscaled: 291.5833 (292.1097)  loss_ce_1_unscaled: 0.2205 (0.2274)  loss_bbox_1_unscaled: 0.0445 (0.0460)  loss_giou_1_unscaled: 0.2627 (0.2679)  cardinality_error_1_unscaled: 291.6667 (292.0707)  loss_ce_2_unscaled: 0.2124 (0.2189)  loss_bbox_2_unscaled: 0.0436 (0.0443)  loss_giou_2_unscaled: 0.2614 (0.2601)  cardinality_error_2_unscaled: 291.6667 (291.8182)  loss_ce_3_unscaled: 0.2003 (0.2126)  loss_bbox_3_unscaled: 0.0454 (0.0447)  loss_giou_3_unscaled: 0.2572 (0.2602)  cardinality_error_3_unscaled: 291.3333 (291.7544)  loss_ce_4_unscaled: 0.2014 (0.2122)  loss_bbox_4_unscaled: 0.0438 (0.0444)  loss_giou_4_unscaled: 0.2529 (0.2592)  cardinality_error_4_unscaled: 291.5833 (292.1530)  time: 2.9694  data: 0.0290  max mem: 8138\n",
            "Test:  [180/417]  eta: 0:12:55  class_error: 14.52  loss: 7.7497 (7.3097)  loss_ce: 0.4090 (0.4224)  loss_bbox: 0.2317 (0.2244)  loss_giou: 0.5144 (0.5177)  loss_ce_0: 0.4835 (0.4828)  loss_bbox_0: 0.2892 (0.2849)  loss_giou_0: 0.6073 (0.6452)  loss_ce_1: 0.4445 (0.4540)  loss_bbox_1: 0.2360 (0.2313)  loss_giou_1: 0.5252 (0.5341)  loss_ce_2: 0.4259 (0.4369)  loss_bbox_2: 0.2283 (0.2232)  loss_giou_2: 0.5120 (0.5191)  loss_ce_3: 0.4130 (0.4249)  loss_bbox_3: 0.2270 (0.2251)  loss_giou_3: 0.5218 (0.5193)  loss_ce_4: 0.4053 (0.4238)  loss_bbox_4: 0.2341 (0.2237)  loss_giou_4: 0.5135 (0.5171)  loss_ce_unscaled: 0.2045 (0.2112)  class_error_unscaled: 8.4906 (9.9803)  loss_bbox_unscaled: 0.0463 (0.0449)  loss_giou_unscaled: 0.2572 (0.2588)  cardinality_error_unscaled: 292.0833 (292.2463)  loss_ce_0_unscaled: 0.2418 (0.2414)  loss_bbox_0_unscaled: 0.0578 (0.0570)  loss_giou_0_unscaled: 0.3036 (0.3226)  cardinality_error_0_unscaled: 292.0000 (292.0898)  loss_ce_1_unscaled: 0.2222 (0.2270)  loss_bbox_1_unscaled: 0.0472 (0.0463)  loss_giou_1_unscaled: 0.2626 (0.2671)  cardinality_error_1_unscaled: 291.6667 (292.0327)  loss_ce_2_unscaled: 0.2129 (0.2184)  loss_bbox_2_unscaled: 0.0457 (0.0446)  loss_giou_2_unscaled: 0.2560 (0.2595)  cardinality_error_2_unscaled: 291.3333 (291.7804)  loss_ce_3_unscaled: 0.2065 (0.2124)  loss_bbox_3_unscaled: 0.0454 (0.0450)  loss_giou_3_unscaled: 0.2609 (0.2596)  cardinality_error_3_unscaled: 291.3333 (291.7100)  loss_ce_4_unscaled: 0.2026 (0.2119)  loss_bbox_4_unscaled: 0.0468 (0.0447)  loss_giou_4_unscaled: 0.2568 (0.2586)  cardinality_error_4_unscaled: 292.0833 (292.1280)  time: 3.0501  data: 0.0293  max mem: 8138\n",
            "Test:  [190/417]  eta: 0:12:20  class_error: 10.00  loss: 7.1472 (7.3218)  loss_ce: 0.4342 (0.4252)  loss_bbox: 0.2472 (0.2247)  loss_giou: 0.4896 (0.5168)  loss_ce_0: 0.4723 (0.4850)  loss_bbox_0: 0.3021 (0.2854)  loss_giou_0: 0.5927 (0.6441)  loss_ce_1: 0.4546 (0.4567)  loss_bbox_1: 0.2502 (0.2316)  loss_giou_1: 0.4891 (0.5330)  loss_ce_2: 0.4557 (0.4398)  loss_bbox_2: 0.2467 (0.2234)  loss_giou_2: 0.4802 (0.5179)  loss_ce_3: 0.4326 (0.4279)  loss_bbox_3: 0.2534 (0.2254)  loss_giou_3: 0.4806 (0.5182)  loss_ce_4: 0.4340 (0.4263)  loss_bbox_4: 0.2462 (0.2241)  loss_giou_4: 0.4844 (0.5163)  loss_ce_unscaled: 0.2171 (0.2126)  class_error_unscaled: 10.2041 (10.1227)  loss_bbox_unscaled: 0.0494 (0.0449)  loss_giou_unscaled: 0.2448 (0.2584)  cardinality_error_unscaled: 293.6667 (292.2810)  loss_ce_0_unscaled: 0.2361 (0.2425)  loss_bbox_0_unscaled: 0.0604 (0.0571)  loss_giou_0_unscaled: 0.2964 (0.3221)  cardinality_error_0_unscaled: 293.0000 (292.1148)  loss_ce_1_unscaled: 0.2273 (0.2284)  loss_bbox_1_unscaled: 0.0500 (0.0463)  loss_giou_1_unscaled: 0.2446 (0.2665)  cardinality_error_1_unscaled: 292.9167 (292.0668)  loss_ce_2_unscaled: 0.2279 (0.2199)  loss_bbox_2_unscaled: 0.0493 (0.0447)  loss_giou_2_unscaled: 0.2401 (0.2590)  cardinality_error_2_unscaled: 292.4167 (291.8024)  loss_ce_3_unscaled: 0.2163 (0.2140)  loss_bbox_3_unscaled: 0.0507 (0.0451)  loss_giou_3_unscaled: 0.2403 (0.2591)  cardinality_error_3_unscaled: 292.5000 (291.7413)  loss_ce_4_unscaled: 0.2170 (0.2131)  loss_bbox_4_unscaled: 0.0492 (0.0448)  loss_giou_4_unscaled: 0.2422 (0.2582)  cardinality_error_4_unscaled: 293.4167 (292.1610)  time: 3.1238  data: 0.0295  max mem: 8138\n",
            "Test:  [200/417]  eta: 0:11:44  class_error: 3.64  loss: 7.1126 (7.3097)  loss_ce: 0.4245 (0.4237)  loss_bbox: 0.2323 (0.2246)  loss_giou: 0.4897 (0.5163)  loss_ce_0: 0.4616 (0.4825)  loss_bbox_0: 0.2852 (0.2855)  loss_giou_0: 0.6010 (0.6444)  loss_ce_1: 0.4405 (0.4552)  loss_bbox_1: 0.2330 (0.2313)  loss_giou_1: 0.5084 (0.5328)  loss_ce_2: 0.4444 (0.4386)  loss_bbox_2: 0.2206 (0.2232)  loss_giou_2: 0.4924 (0.5175)  loss_ce_3: 0.4298 (0.4266)  loss_bbox_3: 0.2194 (0.2252)  loss_giou_3: 0.4887 (0.5178)  loss_ce_4: 0.4194 (0.4248)  loss_bbox_4: 0.2235 (0.2240)  loss_giou_4: 0.4844 (0.5158)  loss_ce_unscaled: 0.2123 (0.2119)  class_error_unscaled: 8.8235 (9.9935)  loss_bbox_unscaled: 0.0465 (0.0449)  loss_giou_unscaled: 0.2448 (0.2581)  cardinality_error_unscaled: 292.9167 (292.2993)  loss_ce_0_unscaled: 0.2308 (0.2412)  loss_bbox_0_unscaled: 0.0570 (0.0571)  loss_giou_0_unscaled: 0.3005 (0.3222)  cardinality_error_0_unscaled: 292.7500 (292.1269)  loss_ce_1_unscaled: 0.2202 (0.2276)  loss_bbox_1_unscaled: 0.0466 (0.0463)  loss_giou_1_unscaled: 0.2542 (0.2664)  cardinality_error_1_unscaled: 292.8333 (292.0829)  loss_ce_2_unscaled: 0.2222 (0.2193)  loss_bbox_2_unscaled: 0.0441 (0.0446)  loss_giou_2_unscaled: 0.2462 (0.2588)  cardinality_error_2_unscaled: 292.5000 (291.8288)  loss_ce_3_unscaled: 0.2149 (0.2133)  loss_bbox_3_unscaled: 0.0439 (0.0450)  loss_giou_3_unscaled: 0.2444 (0.2589)  cardinality_error_3_unscaled: 292.5000 (291.7683)  loss_ce_4_unscaled: 0.2097 (0.2124)  loss_bbox_4_unscaled: 0.0447 (0.0448)  loss_giou_4_unscaled: 0.2422 (0.2579)  cardinality_error_4_unscaled: 292.7500 (292.1820)  time: 3.0100  data: 0.0286  max mem: 8138\n",
            "Test:  [210/417]  eta: 0:11:09  class_error: 5.41  loss: 7.1943 (7.3194)  loss_ce: 0.3974 (0.4231)  loss_bbox: 0.2198 (0.2243)  loss_giou: 0.5144 (0.5188)  loss_ce_0: 0.4395 (0.4815)  loss_bbox_0: 0.2852 (0.2850)  loss_giou_0: 0.6500 (0.6468)  loss_ce_1: 0.4197 (0.4546)  loss_bbox_1: 0.2330 (0.2313)  loss_giou_1: 0.5226 (0.5352)  loss_ce_2: 0.4085 (0.4382)  loss_bbox_2: 0.2206 (0.2231)  loss_giou_2: 0.5153 (0.5199)  loss_ce_3: 0.4127 (0.4265)  loss_bbox_3: 0.2145 (0.2248)  loss_giou_3: 0.5114 (0.5201)  loss_ce_4: 0.4060 (0.4242)  loss_bbox_4: 0.2199 (0.2237)  loss_giou_4: 0.5114 (0.5183)  loss_ce_unscaled: 0.1987 (0.2115)  class_error_unscaled: 6.2500 (9.9196)  loss_bbox_unscaled: 0.0440 (0.0449)  loss_giou_unscaled: 0.2572 (0.2594)  cardinality_error_unscaled: 291.8333 (292.2856)  loss_ce_0_unscaled: 0.2197 (0.2407)  loss_bbox_0_unscaled: 0.0570 (0.0570)  loss_giou_0_unscaled: 0.3250 (0.3234)  cardinality_error_0_unscaled: 291.5833 (292.1193)  loss_ce_1_unscaled: 0.2098 (0.2273)  loss_bbox_1_unscaled: 0.0466 (0.0463)  loss_giou_1_unscaled: 0.2613 (0.2676)  cardinality_error_1_unscaled: 291.5000 (292.0668)  loss_ce_2_unscaled: 0.2043 (0.2191)  loss_bbox_2_unscaled: 0.0441 (0.0446)  loss_giou_2_unscaled: 0.2577 (0.2600)  cardinality_error_2_unscaled: 291.5000 (291.8085)  loss_ce_3_unscaled: 0.2064 (0.2132)  loss_bbox_3_unscaled: 0.0429 (0.0450)  loss_giou_3_unscaled: 0.2557 (0.2600)  cardinality_error_3_unscaled: 291.5000 (291.7563)  loss_ce_4_unscaled: 0.2030 (0.2121)  loss_bbox_4_unscaled: 0.0440 (0.0447)  loss_giou_4_unscaled: 0.2557 (0.2591)  cardinality_error_4_unscaled: 291.7500 (292.1702)  time: 2.9831  data: 0.0279  max mem: 8138\n",
            "Test:  [220/417]  eta: 0:10:36  class_error: 14.02  loss: 7.6906 (7.3470)  loss_ce: 0.4306 (0.4246)  loss_bbox: 0.2203 (0.2245)  loss_giou: 0.5516 (0.5217)  loss_ce_0: 0.5091 (0.4834)  loss_bbox_0: 0.2764 (0.2847)  loss_giou_0: 0.6686 (0.6495)  loss_ce_1: 0.4590 (0.4562)  loss_bbox_1: 0.2268 (0.2313)  loss_giou_1: 0.5650 (0.5382)  loss_ce_2: 0.4459 (0.4397)  loss_bbox_2: 0.2291 (0.2232)  loss_giou_2: 0.5621 (0.5230)  loss_ce_3: 0.4258 (0.4278)  loss_bbox_3: 0.2298 (0.2252)  loss_giou_3: 0.5606 (0.5231)  loss_ce_4: 0.4287 (0.4255)  loss_bbox_4: 0.2210 (0.2240)  loss_giou_4: 0.5518 (0.5213)  loss_ce_unscaled: 0.2153 (0.2123)  class_error_unscaled: 9.2105 (9.9153)  loss_bbox_unscaled: 0.0441 (0.0449)  loss_giou_unscaled: 0.2758 (0.2608)  cardinality_error_unscaled: 291.7500 (292.2470)  loss_ce_0_unscaled: 0.2546 (0.2417)  loss_bbox_0_unscaled: 0.0553 (0.0569)  loss_giou_0_unscaled: 0.3343 (0.3248)  cardinality_error_0_unscaled: 291.5833 (292.0754)  loss_ce_1_unscaled: 0.2295 (0.2281)  loss_bbox_1_unscaled: 0.0454 (0.0463)  loss_giou_1_unscaled: 0.2825 (0.2691)  cardinality_error_1_unscaled: 291.3333 (292.0196)  loss_ce_2_unscaled: 0.2229 (0.2198)  loss_bbox_2_unscaled: 0.0458 (0.0446)  loss_giou_2_unscaled: 0.2810 (0.2615)  cardinality_error_2_unscaled: 291.2500 (291.7719)  loss_ce_3_unscaled: 0.2129 (0.2139)  loss_bbox_3_unscaled: 0.0460 (0.0450)  loss_giou_3_unscaled: 0.2803 (0.2615)  cardinality_error_3_unscaled: 291.4167 (291.7259)  loss_ce_4_unscaled: 0.2143 (0.2128)  loss_bbox_4_unscaled: 0.0442 (0.0448)  loss_giou_4_unscaled: 0.2759 (0.2607)  cardinality_error_4_unscaled: 291.4167 (292.1358)  time: 3.0440  data: 0.0284  max mem: 8138\n",
            "Test:  [230/417]  eta: 0:10:01  class_error: 6.25  loss: 7.5074 (7.3516)  loss_ce: 0.4085 (0.4244)  loss_bbox: 0.2203 (0.2244)  loss_giou: 0.5516 (0.5227)  loss_ce_0: 0.5091 (0.4834)  loss_bbox_0: 0.2764 (0.2850)  loss_giou_0: 0.6779 (0.6505)  loss_ce_1: 0.4590 (0.4559)  loss_bbox_1: 0.2140 (0.2315)  loss_giou_1: 0.5738 (0.5394)  loss_ce_2: 0.4459 (0.4395)  loss_bbox_2: 0.2310 (0.2232)  loss_giou_2: 0.5621 (0.5240)  loss_ce_3: 0.4234 (0.4276)  loss_bbox_3: 0.2302 (0.2251)  loss_giou_3: 0.5606 (0.5239)  loss_ce_4: 0.4174 (0.4252)  loss_bbox_4: 0.2294 (0.2241)  loss_giou_4: 0.5518 (0.5222)  loss_ce_unscaled: 0.2043 (0.2122)  class_error_unscaled: 6.8966 (9.8716)  loss_bbox_unscaled: 0.0441 (0.0449)  loss_giou_unscaled: 0.2758 (0.2614)  cardinality_error_unscaled: 291.9167 (292.2547)  loss_ce_0_unscaled: 0.2546 (0.2417)  loss_bbox_0_unscaled: 0.0553 (0.0570)  loss_giou_0_unscaled: 0.3390 (0.3252)  cardinality_error_0_unscaled: 291.5833 (292.0891)  loss_ce_1_unscaled: 0.2295 (0.2279)  loss_bbox_1_unscaled: 0.0428 (0.0463)  loss_giou_1_unscaled: 0.2869 (0.2697)  cardinality_error_1_unscaled: 291.4167 (292.0235)  loss_ce_2_unscaled: 0.2229 (0.2197)  loss_bbox_2_unscaled: 0.0462 (0.0446)  loss_giou_2_unscaled: 0.2810 (0.2620)  cardinality_error_2_unscaled: 291.2500 (291.7619)  loss_ce_3_unscaled: 0.2117 (0.2138)  loss_bbox_3_unscaled: 0.0460 (0.0450)  loss_giou_3_unscaled: 0.2803 (0.2620)  cardinality_error_3_unscaled: 291.7500 (291.6811)  loss_ce_4_unscaled: 0.2087 (0.2126)  loss_bbox_4_unscaled: 0.0459 (0.0448)  loss_giou_4_unscaled: 0.2759 (0.2611)  cardinality_error_4_unscaled: 292.0833 (292.1367)  time: 3.0061  data: 0.0283  max mem: 8138\n",
            "Test:  [240/417]  eta: 0:09:28  class_error: 5.83  loss: 7.0140 (7.3389)  loss_ce: 0.3681 (0.4231)  loss_bbox: 0.2147 (0.2244)  loss_giou: 0.4984 (0.5219)  loss_ce_0: 0.4215 (0.4820)  loss_bbox_0: 0.2923 (0.2851)  loss_giou_0: 0.6591 (0.6505)  loss_ce_1: 0.3978 (0.4542)  loss_bbox_1: 0.2358 (0.2315)  loss_giou_1: 0.5173 (0.5387)  loss_ce_2: 0.3819 (0.4378)  loss_bbox_2: 0.2216 (0.2232)  loss_giou_2: 0.4960 (0.5230)  loss_ce_3: 0.3746 (0.4263)  loss_bbox_3: 0.2217 (0.2250)  loss_giou_3: 0.4948 (0.5229)  loss_ce_4: 0.3745 (0.4237)  loss_bbox_4: 0.2211 (0.2241)  loss_giou_4: 0.5002 (0.5214)  loss_ce_unscaled: 0.1840 (0.2115)  class_error_unscaled: 6.3830 (9.8307)  loss_bbox_unscaled: 0.0429 (0.0449)  loss_giou_unscaled: 0.2492 (0.2609)  cardinality_error_unscaled: 291.9167 (292.2486)  loss_ce_0_unscaled: 0.2107 (0.2410)  loss_bbox_0_unscaled: 0.0585 (0.0570)  loss_giou_0_unscaled: 0.3296 (0.3253)  cardinality_error_0_unscaled: 291.9167 (292.0861)  loss_ce_1_unscaled: 0.1989 (0.2271)  loss_bbox_1_unscaled: 0.0472 (0.0463)  loss_giou_1_unscaled: 0.2587 (0.2693)  cardinality_error_1_unscaled: 291.7500 (292.0180)  loss_ce_2_unscaled: 0.1909 (0.2189)  loss_bbox_2_unscaled: 0.0443 (0.0446)  loss_giou_2_unscaled: 0.2480 (0.2615)  cardinality_error_2_unscaled: 291.4167 (291.7621)  loss_ce_3_unscaled: 0.1873 (0.2132)  loss_bbox_3_unscaled: 0.0443 (0.0450)  loss_giou_3_unscaled: 0.2474 (0.2615)  cardinality_error_3_unscaled: 291.4167 (291.6660)  loss_ce_4_unscaled: 0.1872 (0.2118)  loss_bbox_4_unscaled: 0.0442 (0.0448)  loss_giou_4_unscaled: 0.2501 (0.2607)  cardinality_error_4_unscaled: 291.8333 (292.1269)  time: 2.9981  data: 0.0284  max mem: 8138\n",
            "Test:  [250/417]  eta: 0:08:55  class_error: 18.87  loss: 7.2356 (7.3341)  loss_ce: 0.3884 (0.4226)  loss_bbox: 0.2147 (0.2240)  loss_giou: 0.5060 (0.5219)  loss_ce_0: 0.4649 (0.4818)  loss_bbox_0: 0.2899 (0.2849)  loss_giou_0: 0.6591 (0.6506)  loss_ce_1: 0.4124 (0.4538)  loss_bbox_1: 0.2326 (0.2311)  loss_giou_1: 0.5176 (0.5387)  loss_ce_2: 0.3949 (0.4371)  loss_bbox_2: 0.2216 (0.2229)  loss_giou_2: 0.5069 (0.5231)  loss_ce_3: 0.3909 (0.4256)  loss_bbox_3: 0.2217 (0.2246)  loss_giou_3: 0.5024 (0.5230)  loss_ce_4: 0.3839 (0.4231)  loss_bbox_4: 0.2211 (0.2238)  loss_giou_4: 0.5033 (0.5215)  loss_ce_unscaled: 0.1942 (0.2113)  class_error_unscaled: 7.4468 (9.8157)  loss_bbox_unscaled: 0.0429 (0.0448)  loss_giou_unscaled: 0.2530 (0.2609)  cardinality_error_unscaled: 292.2500 (292.2786)  loss_ce_0_unscaled: 0.2325 (0.2409)  loss_bbox_0_unscaled: 0.0580 (0.0570)  loss_giou_0_unscaled: 0.3296 (0.3253)  cardinality_error_0_unscaled: 292.1667 (292.1165)  loss_ce_1_unscaled: 0.2062 (0.2269)  loss_bbox_1_unscaled: 0.0465 (0.0462)  loss_giou_1_unscaled: 0.2588 (0.2693)  cardinality_error_1_unscaled: 292.1667 (292.0548)  loss_ce_2_unscaled: 0.1974 (0.2186)  loss_bbox_2_unscaled: 0.0443 (0.0446)  loss_giou_2_unscaled: 0.2535 (0.2615)  cardinality_error_2_unscaled: 291.9167 (291.8074)  loss_ce_3_unscaled: 0.1954 (0.2128)  loss_bbox_3_unscaled: 0.0443 (0.0449)  loss_giou_3_unscaled: 0.2512 (0.2615)  cardinality_error_3_unscaled: 291.5000 (291.7175)  loss_ce_4_unscaled: 0.1920 (0.2116)  loss_bbox_4_unscaled: 0.0442 (0.0448)  loss_giou_4_unscaled: 0.2517 (0.2607)  cardinality_error_4_unscaled: 292.1667 (292.1614)  time: 3.0948  data: 0.0290  max mem: 8138\n",
            "Test:  [260/417]  eta: 0:08:22  class_error: 6.43  loss: 7.3784 (7.3430)  loss_ce: 0.3971 (0.4227)  loss_bbox: 0.2366 (0.2253)  loss_giou: 0.5123 (0.5220)  loss_ce_0: 0.4806 (0.4817)  loss_bbox_0: 0.2921 (0.2863)  loss_giou_0: 0.6441 (0.6510)  loss_ce_1: 0.4487 (0.4537)  loss_bbox_1: 0.2481 (0.2323)  loss_giou_1: 0.5310 (0.5391)  loss_ce_2: 0.4124 (0.4370)  loss_bbox_2: 0.2337 (0.2242)  loss_giou_2: 0.5229 (0.5234)  loss_ce_3: 0.4128 (0.4256)  loss_bbox_3: 0.2379 (0.2259)  loss_giou_3: 0.5180 (0.5233)  loss_ce_4: 0.3980 (0.4231)  loss_bbox_4: 0.2365 (0.2251)  loss_giou_4: 0.5141 (0.5215)  loss_ce_unscaled: 0.1986 (0.2113)  class_error_unscaled: 6.4286 (9.8534)  loss_bbox_unscaled: 0.0473 (0.0451)  loss_giou_unscaled: 0.2562 (0.2610)  cardinality_error_unscaled: 293.5833 (292.3062)  loss_ce_0_unscaled: 0.2403 (0.2409)  loss_bbox_0_unscaled: 0.0584 (0.0573)  loss_giou_0_unscaled: 0.3221 (0.3255)  cardinality_error_0_unscaled: 292.5000 (292.1383)  loss_ce_1_unscaled: 0.2244 (0.2268)  loss_bbox_1_unscaled: 0.0496 (0.0465)  loss_giou_1_unscaled: 0.2655 (0.2696)  cardinality_error_1_unscaled: 292.5833 (292.0766)  loss_ce_2_unscaled: 0.2062 (0.2185)  loss_bbox_2_unscaled: 0.0467 (0.0448)  loss_giou_2_unscaled: 0.2614 (0.2617)  cardinality_error_2_unscaled: 292.8333 (291.8337)  loss_ce_3_unscaled: 0.2064 (0.2128)  loss_bbox_3_unscaled: 0.0476 (0.0452)  loss_giou_3_unscaled: 0.2590 (0.2616)  cardinality_error_3_unscaled: 292.5000 (291.7388)  loss_ce_4_unscaled: 0.1990 (0.2115)  loss_bbox_4_unscaled: 0.0473 (0.0450)  loss_giou_4_unscaled: 0.2570 (0.2608)  cardinality_error_4_unscaled: 293.5000 (292.1919)  time: 3.0668  data: 0.0287  max mem: 8138\n",
            "Test:  [270/417]  eta: 0:07:49  class_error: 6.41  loss: 7.0974 (7.3336)  loss_ce: 0.4248 (0.4232)  loss_bbox: 0.2330 (0.2248)  loss_giou: 0.4847 (0.5203)  loss_ce_0: 0.4889 (0.4825)  loss_bbox_0: 0.2921 (0.2859)  loss_giou_0: 0.6238 (0.6492)  loss_ce_1: 0.4539 (0.4542)  loss_bbox_1: 0.2322 (0.2320)  loss_giou_1: 0.5177 (0.5377)  loss_ce_2: 0.4409 (0.4375)  loss_bbox_2: 0.2290 (0.2237)  loss_giou_2: 0.5045 (0.5217)  loss_ce_3: 0.4308 (0.4258)  loss_bbox_3: 0.2333 (0.2255)  loss_giou_3: 0.4971 (0.5217)  loss_ce_4: 0.4264 (0.4234)  loss_bbox_4: 0.2316 (0.2246)  loss_giou_4: 0.4848 (0.5199)  loss_ce_unscaled: 0.2124 (0.2116)  class_error_unscaled: 8.0645 (9.7607)  loss_bbox_unscaled: 0.0466 (0.0450)  loss_giou_unscaled: 0.2423 (0.2602)  cardinality_error_unscaled: 292.0833 (292.2749)  loss_ce_0_unscaled: 0.2444 (0.2412)  loss_bbox_0_unscaled: 0.0584 (0.0572)  loss_giou_0_unscaled: 0.3119 (0.3246)  cardinality_error_0_unscaled: 291.7500 (292.1049)  loss_ce_1_unscaled: 0.2270 (0.2271)  loss_bbox_1_unscaled: 0.0464 (0.0464)  loss_giou_1_unscaled: 0.2589 (0.2688)  cardinality_error_1_unscaled: 292.0833 (292.0446)  loss_ce_2_unscaled: 0.2204 (0.2187)  loss_bbox_2_unscaled: 0.0458 (0.0447)  loss_giou_2_unscaled: 0.2523 (0.2609)  cardinality_error_2_unscaled: 291.5000 (291.7792)  loss_ce_3_unscaled: 0.2154 (0.2129)  loss_bbox_3_unscaled: 0.0467 (0.0451)  loss_giou_3_unscaled: 0.2485 (0.2608)  cardinality_error_3_unscaled: 291.8333 (291.6879)  loss_ce_4_unscaled: 0.2132 (0.2117)  loss_bbox_4_unscaled: 0.0463 (0.0449)  loss_giou_4_unscaled: 0.2424 (0.2600)  cardinality_error_4_unscaled: 292.0833 (292.1581)  time: 3.0637  data: 0.0288  max mem: 8138\n",
            "Test:  [280/417]  eta: 0:07:16  class_error: 8.21  loss: 7.2612 (7.3503)  loss_ce: 0.4541 (0.4241)  loss_bbox: 0.2246 (0.2251)  loss_giou: 0.5186 (0.5220)  loss_ce_0: 0.4905 (0.4831)  loss_bbox_0: 0.2723 (0.2859)  loss_giou_0: 0.6531 (0.6509)  loss_ce_1: 0.4814 (0.4551)  loss_bbox_1: 0.2224 (0.2323)  loss_giou_1: 0.5362 (0.5393)  loss_ce_2: 0.4506 (0.4384)  loss_bbox_2: 0.2135 (0.2242)  loss_giou_2: 0.5132 (0.5234)  loss_ce_3: 0.4448 (0.4267)  loss_bbox_3: 0.2190 (0.2259)  loss_giou_3: 0.5133 (0.5232)  loss_ce_4: 0.4447 (0.4243)  loss_bbox_4: 0.2167 (0.2249)  loss_giou_4: 0.5148 (0.5216)  loss_ce_unscaled: 0.2271 (0.2121)  class_error_unscaled: 8.1081 (9.7612)  loss_bbox_unscaled: 0.0449 (0.0450)  loss_giou_unscaled: 0.2593 (0.2610)  cardinality_error_unscaled: 290.8333 (292.2106)  loss_ce_0_unscaled: 0.2452 (0.2415)  loss_bbox_0_unscaled: 0.0545 (0.0572)  loss_giou_0_unscaled: 0.3266 (0.3255)  cardinality_error_0_unscaled: 290.8333 (292.0400)  loss_ce_1_unscaled: 0.2407 (0.2275)  loss_bbox_1_unscaled: 0.0445 (0.0465)  loss_giou_1_unscaled: 0.2681 (0.2697)  cardinality_error_1_unscaled: 290.5000 (291.9745)  loss_ce_2_unscaled: 0.2253 (0.2192)  loss_bbox_2_unscaled: 0.0427 (0.0448)  loss_giou_2_unscaled: 0.2566 (0.2617)  cardinality_error_2_unscaled: 289.1667 (291.7040)  loss_ce_3_unscaled: 0.2224 (0.2134)  loss_bbox_3_unscaled: 0.0438 (0.0452)  loss_giou_3_unscaled: 0.2567 (0.2616)  cardinality_error_3_unscaled: 289.6667 (291.6097)  loss_ce_4_unscaled: 0.2223 (0.2121)  loss_bbox_4_unscaled: 0.0433 (0.0450)  loss_giou_4_unscaled: 0.2574 (0.2608)  cardinality_error_4_unscaled: 290.5833 (292.0896)  time: 3.0201  data: 0.0283  max mem: 8138\n",
            "Test:  [290/417]  eta: 0:06:44  class_error: 6.06  loss: 7.2848 (7.3370)  loss_ce: 0.4251 (0.4235)  loss_bbox: 0.2227 (0.2247)  loss_giou: 0.5284 (0.5208)  loss_ce_0: 0.4619 (0.4822)  loss_bbox_0: 0.2774 (0.2857)  loss_giou_0: 0.6551 (0.6497)  loss_ce_1: 0.4620 (0.4543)  loss_bbox_1: 0.2275 (0.2319)  loss_giou_1: 0.5362 (0.5382)  loss_ce_2: 0.4400 (0.4378)  loss_bbox_2: 0.2178 (0.2239)  loss_giou_2: 0.5276 (0.5222)  loss_ce_3: 0.4228 (0.4259)  loss_bbox_3: 0.2190 (0.2257)  loss_giou_3: 0.5215 (0.5221)  loss_ce_4: 0.4271 (0.4237)  loss_bbox_4: 0.2191 (0.2245)  loss_giou_4: 0.5280 (0.5204)  loss_ce_unscaled: 0.2125 (0.2117)  class_error_unscaled: 7.1429 (9.7145)  loss_bbox_unscaled: 0.0445 (0.0449)  loss_giou_unscaled: 0.2642 (0.2604)  cardinality_error_unscaled: 291.5833 (292.2443)  loss_ce_0_unscaled: 0.2310 (0.2411)  loss_bbox_0_unscaled: 0.0555 (0.0571)  loss_giou_0_unscaled: 0.3276 (0.3248)  cardinality_error_0_unscaled: 291.4167 (292.0656)  loss_ce_1_unscaled: 0.2310 (0.2271)  loss_bbox_1_unscaled: 0.0455 (0.0464)  loss_giou_1_unscaled: 0.2681 (0.2691)  cardinality_error_1_unscaled: 290.9167 (291.9989)  loss_ce_2_unscaled: 0.2200 (0.2189)  loss_bbox_2_unscaled: 0.0436 (0.0448)  loss_giou_2_unscaled: 0.2638 (0.2611)  cardinality_error_2_unscaled: 290.9167 (291.7343)  loss_ce_3_unscaled: 0.2114 (0.2129)  loss_bbox_3_unscaled: 0.0438 (0.0451)  loss_giou_3_unscaled: 0.2607 (0.2610)  cardinality_error_3_unscaled: 290.8333 (291.6400)  loss_ce_4_unscaled: 0.2135 (0.2118)  loss_bbox_4_unscaled: 0.0438 (0.0449)  loss_giou_4_unscaled: 0.2640 (0.2602)  cardinality_error_4_unscaled: 291.3333 (292.1234)  time: 3.0565  data: 0.0279  max mem: 8138\n",
            "Test:  [300/417]  eta: 0:06:11  class_error: 13.46  loss: 6.9349 (7.3414)  loss_ce: 0.4139 (0.4230)  loss_bbox: 0.2062 (0.2250)  loss_giou: 0.4932 (0.5219)  loss_ce_0: 0.4385 (0.4820)  loss_bbox_0: 0.2538 (0.2853)  loss_giou_0: 0.6275 (0.6499)  loss_ce_1: 0.4488 (0.4538)  loss_bbox_1: 0.2070 (0.2322)  loss_giou_1: 0.5129 (0.5392)  loss_ce_2: 0.4127 (0.4376)  loss_bbox_2: 0.2090 (0.2241)  loss_giou_2: 0.4937 (0.5233)  loss_ce_3: 0.4110 (0.4256)  loss_bbox_3: 0.2084 (0.2257)  loss_giou_3: 0.4917 (0.5232)  loss_ce_4: 0.4103 (0.4234)  loss_bbox_4: 0.2062 (0.2247)  loss_giou_4: 0.4929 (0.5215)  loss_ce_unscaled: 0.2069 (0.2115)  class_error_unscaled: 8.4746 (9.7172)  loss_bbox_unscaled: 0.0412 (0.0450)  loss_giou_unscaled: 0.2466 (0.2610)  cardinality_error_unscaled: 292.8333 (292.2362)  loss_ce_0_unscaled: 0.2193 (0.2410)  loss_bbox_0_unscaled: 0.0508 (0.0571)  loss_giou_0_unscaled: 0.3137 (0.3250)  cardinality_error_0_unscaled: 292.5000 (292.0551)  loss_ce_1_unscaled: 0.2244 (0.2269)  loss_bbox_1_unscaled: 0.0414 (0.0464)  loss_giou_1_unscaled: 0.2564 (0.2696)  cardinality_error_1_unscaled: 292.0000 (291.9934)  loss_ce_2_unscaled: 0.2063 (0.2188)  loss_bbox_2_unscaled: 0.0418 (0.0448)  loss_giou_2_unscaled: 0.2468 (0.2616)  cardinality_error_2_unscaled: 292.0000 (291.7265)  loss_ce_3_unscaled: 0.2055 (0.2128)  loss_bbox_3_unscaled: 0.0417 (0.0451)  loss_giou_3_unscaled: 0.2459 (0.2616)  cardinality_error_3_unscaled: 292.0000 (291.6409)  loss_ce_4_unscaled: 0.2051 (0.2117)  loss_bbox_4_unscaled: 0.0412 (0.0449)  loss_giou_4_unscaled: 0.2465 (0.2607)  cardinality_error_4_unscaled: 292.3333 (292.1132)  time: 3.0858  data: 0.0285  max mem: 8138\n",
            "Test:  [310/417]  eta: 0:05:39  class_error: 10.87  loss: 7.5768 (7.3557)  loss_ce: 0.4396 (0.4247)  loss_bbox: 0.2155 (0.2253)  loss_giou: 0.5087 (0.5225)  loss_ce_0: 0.4961 (0.4836)  loss_bbox_0: 0.2549 (0.2853)  loss_giou_0: 0.6275 (0.6502)  loss_ce_1: 0.4581 (0.4556)  loss_bbox_1: 0.2184 (0.2324)  loss_giou_1: 0.5235 (0.5396)  loss_ce_2: 0.4509 (0.4391)  loss_bbox_2: 0.2159 (0.2244)  loss_giou_2: 0.5078 (0.5238)  loss_ce_3: 0.4389 (0.4276)  loss_bbox_3: 0.2122 (0.2259)  loss_giou_3: 0.5097 (0.5236)  loss_ce_4: 0.4374 (0.4252)  loss_bbox_4: 0.2138 (0.2250)  loss_giou_4: 0.5089 (0.5219)  loss_ce_unscaled: 0.2198 (0.2123)  class_error_unscaled: 10.7143 (9.7483)  loss_bbox_unscaled: 0.0431 (0.0451)  loss_giou_unscaled: 0.2544 (0.2612)  cardinality_error_unscaled: 292.4167 (292.2698)  loss_ce_0_unscaled: 0.2480 (0.2418)  loss_bbox_0_unscaled: 0.0510 (0.0571)  loss_giou_0_unscaled: 0.3137 (0.3251)  cardinality_error_0_unscaled: 292.4167 (292.0911)  loss_ce_1_unscaled: 0.2290 (0.2278)  loss_bbox_1_unscaled: 0.0437 (0.0465)  loss_giou_1_unscaled: 0.2617 (0.2698)  cardinality_error_1_unscaled: 292.4167 (292.0316)  loss_ce_2_unscaled: 0.2255 (0.2196)  loss_bbox_2_unscaled: 0.0432 (0.0449)  loss_giou_2_unscaled: 0.2539 (0.2619)  cardinality_error_2_unscaled: 292.0833 (291.7731)  loss_ce_3_unscaled: 0.2194 (0.2138)  loss_bbox_3_unscaled: 0.0424 (0.0452)  loss_giou_3_unscaled: 0.2548 (0.2618)  cardinality_error_3_unscaled: 292.4167 (291.6886)  loss_ce_4_unscaled: 0.2187 (0.2126)  loss_bbox_4_unscaled: 0.0428 (0.0450)  loss_giou_4_unscaled: 0.2544 (0.2610)  cardinality_error_4_unscaled: 292.4167 (292.1452)  time: 3.0417  data: 0.0285  max mem: 8138\n",
            "Test:  [320/417]  eta: 0:05:07  class_error: 8.96  loss: 7.1754 (7.3515)  loss_ce: 0.4386 (0.4248)  loss_bbox: 0.2127 (0.2247)  loss_giou: 0.5103 (0.5220)  loss_ce_0: 0.4911 (0.4838)  loss_bbox_0: 0.2711 (0.2848)  loss_giou_0: 0.6573 (0.6505)  loss_ce_1: 0.4584 (0.4555)  loss_bbox_1: 0.2195 (0.2318)  loss_giou_1: 0.5332 (0.5396)  loss_ce_2: 0.4436 (0.4390)  loss_bbox_2: 0.2115 (0.2239)  loss_giou_2: 0.5078 (0.5235)  loss_ce_3: 0.4401 (0.4277)  loss_bbox_3: 0.2114 (0.2253)  loss_giou_3: 0.5138 (0.5232)  loss_ce_4: 0.4346 (0.4252)  loss_bbox_4: 0.2129 (0.2245)  loss_giou_4: 0.5124 (0.5216)  loss_ce_unscaled: 0.2193 (0.2124)  class_error_unscaled: 10.6796 (9.8068)  loss_bbox_unscaled: 0.0425 (0.0449)  loss_giou_unscaled: 0.2551 (0.2610)  cardinality_error_unscaled: 292.4167 (292.2523)  loss_ce_0_unscaled: 0.2456 (0.2419)  loss_bbox_0_unscaled: 0.0542 (0.0570)  loss_giou_0_unscaled: 0.3287 (0.3253)  cardinality_error_0_unscaled: 292.4167 (292.0769)  loss_ce_1_unscaled: 0.2292 (0.2278)  loss_bbox_1_unscaled: 0.0439 (0.0464)  loss_giou_1_unscaled: 0.2666 (0.2698)  cardinality_error_1_unscaled: 292.4167 (292.0179)  loss_ce_2_unscaled: 0.2218 (0.2195)  loss_bbox_2_unscaled: 0.0423 (0.0448)  loss_giou_2_unscaled: 0.2539 (0.2618)  cardinality_error_2_unscaled: 292.3333 (291.7627)  loss_ce_3_unscaled: 0.2201 (0.2138)  loss_bbox_3_unscaled: 0.0423 (0.0451)  loss_giou_3_unscaled: 0.2569 (0.2616)  cardinality_error_3_unscaled: 292.3333 (291.6836)  loss_ce_4_unscaled: 0.2173 (0.2126)  loss_bbox_4_unscaled: 0.0426 (0.0449)  loss_giou_4_unscaled: 0.2562 (0.2608)  cardinality_error_4_unscaled: 292.4167 (292.1298)  time: 3.0209  data: 0.0287  max mem: 8138\n",
            "Test:  [330/417]  eta: 0:04:34  class_error: 16.05  loss: 7.6028 (7.3710)  loss_ce: 0.4575 (0.4265)  loss_bbox: 0.2200 (0.2261)  loss_giou: 0.5131 (0.5222)  loss_ce_0: 0.5132 (0.4857)  loss_bbox_0: 0.2823 (0.2865)  loss_giou_0: 0.6431 (0.6502)  loss_ce_1: 0.4673 (0.4576)  loss_bbox_1: 0.2316 (0.2330)  loss_giou_1: 0.5431 (0.5396)  loss_ce_2: 0.4633 (0.4411)  loss_bbox_2: 0.2214 (0.2251)  loss_giou_2: 0.5288 (0.5235)  loss_ce_3: 0.4554 (0.4295)  loss_bbox_3: 0.2248 (0.2266)  loss_giou_3: 0.5245 (0.5234)  loss_ce_4: 0.4528 (0.4267)  loss_bbox_4: 0.2199 (0.2259)  loss_giou_4: 0.5159 (0.5218)  loss_ce_unscaled: 0.2288 (0.2132)  class_error_unscaled: 10.6796 (9.8661)  loss_bbox_unscaled: 0.0440 (0.0452)  loss_giou_unscaled: 0.2566 (0.2611)  cardinality_error_unscaled: 293.0833 (292.2790)  loss_ce_0_unscaled: 0.2566 (0.2428)  loss_bbox_0_unscaled: 0.0565 (0.0573)  loss_giou_0_unscaled: 0.3215 (0.3251)  cardinality_error_0_unscaled: 292.8333 (292.1065)  loss_ce_1_unscaled: 0.2336 (0.2288)  loss_bbox_1_unscaled: 0.0463 (0.0466)  loss_giou_1_unscaled: 0.2716 (0.2698)  cardinality_error_1_unscaled: 292.3333 (292.0463)  loss_ce_2_unscaled: 0.2317 (0.2206)  loss_bbox_2_unscaled: 0.0443 (0.0450)  loss_giou_2_unscaled: 0.2644 (0.2618)  cardinality_error_2_unscaled: 291.5000 (291.7918)  loss_ce_3_unscaled: 0.2277 (0.2148)  loss_bbox_3_unscaled: 0.0450 (0.0453)  loss_giou_3_unscaled: 0.2623 (0.2617)  cardinality_error_3_unscaled: 292.1667 (291.7188)  loss_ce_4_unscaled: 0.2264 (0.2134)  loss_bbox_4_unscaled: 0.0440 (0.0452)  loss_giou_4_unscaled: 0.2579 (0.2609)  cardinality_error_4_unscaled: 292.9167 (292.1586)  time: 2.8988  data: 0.0278  max mem: 8138\n",
            "Test:  [340/417]  eta: 0:04:02  class_error: 10.09  loss: 7.6634 (7.3773)  loss_ce: 0.4719 (0.4272)  loss_bbox: 0.2498 (0.2264)  loss_giou: 0.5449 (0.5223)  loss_ce_0: 0.5132 (0.4861)  loss_bbox_0: 0.3069 (0.2867)  loss_giou_0: 0.6465 (0.6505)  loss_ce_1: 0.4883 (0.4582)  loss_bbox_1: 0.2530 (0.2333)  loss_giou_1: 0.5431 (0.5398)  loss_ce_2: 0.4696 (0.4416)  loss_bbox_2: 0.2570 (0.2254)  loss_giou_2: 0.5291 (0.5237)  loss_ce_3: 0.4554 (0.4301)  loss_bbox_3: 0.2515 (0.2269)  loss_giou_3: 0.5402 (0.5235)  loss_ce_4: 0.4600 (0.4273)  loss_bbox_4: 0.2494 (0.2262)  loss_giou_4: 0.5434 (0.5220)  loss_ce_unscaled: 0.2359 (0.2136)  class_error_unscaled: 10.3896 (9.8829)  loss_bbox_unscaled: 0.0500 (0.0453)  loss_giou_unscaled: 0.2725 (0.2611)  cardinality_error_unscaled: 291.7500 (292.2456)  loss_ce_0_unscaled: 0.2566 (0.2431)  loss_bbox_0_unscaled: 0.0614 (0.0573)  loss_giou_0_unscaled: 0.3233 (0.3253)  cardinality_error_0_unscaled: 292.0000 (292.0750)  loss_ce_1_unscaled: 0.2441 (0.2291)  loss_bbox_1_unscaled: 0.0506 (0.0467)  loss_giou_1_unscaled: 0.2716 (0.2699)  cardinality_error_1_unscaled: 291.7500 (292.0073)  loss_ce_2_unscaled: 0.2348 (0.2208)  loss_bbox_2_unscaled: 0.0514 (0.0451)  loss_giou_2_unscaled: 0.2645 (0.2618)  cardinality_error_2_unscaled: 291.2500 (291.7605)  loss_ce_3_unscaled: 0.2277 (0.2151)  loss_bbox_3_unscaled: 0.0503 (0.0454)  loss_giou_3_unscaled: 0.2701 (0.2618)  cardinality_error_3_unscaled: 291.5833 (291.6877)  loss_ce_4_unscaled: 0.2300 (0.2137)  loss_bbox_4_unscaled: 0.0499 (0.0452)  loss_giou_4_unscaled: 0.2717 (0.2610)  cardinality_error_4_unscaled: 291.8333 (292.1310)  time: 2.8725  data: 0.0265  max mem: 8138\n",
            "Test:  [350/417]  eta: 0:03:30  class_error: 10.53  loss: 7.5059 (7.3811)  loss_ce: 0.4350 (0.4274)  loss_bbox: 0.2230 (0.2265)  loss_giou: 0.5200 (0.5226)  loss_ce_0: 0.4996 (0.4864)  loss_bbox_0: 0.2847 (0.2868)  loss_giou_0: 0.6629 (0.6508)  loss_ce_1: 0.4623 (0.4583)  loss_bbox_1: 0.2302 (0.2336)  loss_giou_1: 0.5322 (0.5403)  loss_ce_2: 0.4497 (0.4418)  loss_bbox_2: 0.2144 (0.2255)  loss_giou_2: 0.5291 (0.5240)  loss_ce_3: 0.4372 (0.4303)  loss_bbox_3: 0.2226 (0.2269)  loss_giou_3: 0.5204 (0.5239)  loss_ce_4: 0.4410 (0.4275)  loss_bbox_4: 0.2295 (0.2262)  loss_giou_4: 0.5277 (0.5223)  loss_ce_unscaled: 0.2175 (0.2137)  class_error_unscaled: 8.9552 (9.8651)  loss_bbox_unscaled: 0.0446 (0.0453)  loss_giou_unscaled: 0.2600 (0.2613)  cardinality_error_unscaled: 291.3333 (292.2500)  loss_ce_0_unscaled: 0.2498 (0.2432)  loss_bbox_0_unscaled: 0.0569 (0.0574)  loss_giou_0_unscaled: 0.3314 (0.3254)  cardinality_error_0_unscaled: 291.2500 (292.0786)  loss_ce_1_unscaled: 0.2311 (0.2292)  loss_bbox_1_unscaled: 0.0460 (0.0467)  loss_giou_1_unscaled: 0.2661 (0.2701)  cardinality_error_1_unscaled: 291.0833 (292.0140)  loss_ce_2_unscaled: 0.2248 (0.2209)  loss_bbox_2_unscaled: 0.0429 (0.0451)  loss_giou_2_unscaled: 0.2645 (0.2620)  cardinality_error_2_unscaled: 290.9167 (291.7688)  loss_ce_3_unscaled: 0.2186 (0.2152)  loss_bbox_3_unscaled: 0.0445 (0.0454)  loss_giou_3_unscaled: 0.2602 (0.2619)  cardinality_error_3_unscaled: 290.9167 (291.7002)  loss_ce_4_unscaled: 0.2205 (0.2137)  loss_bbox_4_unscaled: 0.0459 (0.0452)  loss_giou_4_unscaled: 0.2638 (0.2612)  cardinality_error_4_unscaled: 291.2500 (292.1337)  time: 2.9581  data: 0.0277  max mem: 8138\n",
            "Test:  [360/417]  eta: 0:02:59  class_error: 6.25  loss: 7.2050 (7.3758)  loss_ce: 0.4179 (0.4266)  loss_bbox: 0.2167 (0.2264)  loss_giou: 0.5058 (0.5225)  loss_ce_0: 0.4550 (0.4854)  loss_bbox_0: 0.2770 (0.2868)  loss_giou_0: 0.6629 (0.6513)  loss_ce_1: 0.4395 (0.4577)  loss_bbox_1: 0.2300 (0.2334)  loss_giou_1: 0.5276 (0.5403)  loss_ce_2: 0.4259 (0.4411)  loss_bbox_2: 0.2144 (0.2253)  loss_giou_2: 0.5034 (0.5240)  loss_ce_3: 0.4167 (0.4294)  loss_bbox_3: 0.2172 (0.2268)  loss_giou_3: 0.5016 (0.5239)  loss_ce_4: 0.4147 (0.4266)  loss_bbox_4: 0.2135 (0.2261)  loss_giou_4: 0.5030 (0.5224)  loss_ce_unscaled: 0.2089 (0.2133)  class_error_unscaled: 7.5000 (9.7958)  loss_bbox_unscaled: 0.0433 (0.0453)  loss_giou_unscaled: 0.2529 (0.2613)  cardinality_error_unscaled: 292.2500 (292.2313)  loss_ce_0_unscaled: 0.2275 (0.2427)  loss_bbox_0_unscaled: 0.0554 (0.0574)  loss_giou_0_unscaled: 0.3314 (0.3256)  cardinality_error_0_unscaled: 292.2500 (292.0640)  loss_ce_1_unscaled: 0.2197 (0.2288)  loss_bbox_1_unscaled: 0.0460 (0.0467)  loss_giou_1_unscaled: 0.2638 (0.2701)  cardinality_error_1_unscaled: 292.2500 (292.0037)  loss_ce_2_unscaled: 0.2130 (0.2205)  loss_bbox_2_unscaled: 0.0429 (0.0451)  loss_giou_2_unscaled: 0.2517 (0.2620)  cardinality_error_2_unscaled: 291.5833 (291.7530)  loss_ce_3_unscaled: 0.2083 (0.2147)  loss_bbox_3_unscaled: 0.0434 (0.0454)  loss_giou_3_unscaled: 0.2508 (0.2619)  cardinality_error_3_unscaled: 291.0000 (291.6784)  loss_ce_4_unscaled: 0.2074 (0.2133)  loss_bbox_4_unscaled: 0.0427 (0.0452)  loss_giou_4_unscaled: 0.2515 (0.2612)  cardinality_error_4_unscaled: 292.1667 (292.1124)  time: 3.0641  data: 0.0295  max mem: 8138\n",
            "Test:  [370/417]  eta: 0:02:27  class_error: 4.17  loss: 7.1783 (7.3735)  loss_ce: 0.4116 (0.4263)  loss_bbox: 0.2213 (0.2266)  loss_giou: 0.4939 (0.5222)  loss_ce_0: 0.4631 (0.4851)  loss_bbox_0: 0.2806 (0.2868)  loss_giou_0: 0.6333 (0.6509)  loss_ce_1: 0.4395 (0.4576)  loss_bbox_1: 0.2300 (0.2336)  loss_giou_1: 0.5233 (0.5399)  loss_ce_2: 0.4259 (0.4410)  loss_bbox_2: 0.2268 (0.2254)  loss_giou_2: 0.5019 (0.5235)  loss_ce_3: 0.4034 (0.4294)  loss_bbox_3: 0.2229 (0.2269)  loss_giou_3: 0.4947 (0.5235)  loss_ce_4: 0.4096 (0.4264)  loss_bbox_4: 0.2213 (0.2264)  loss_giou_4: 0.4923 (0.5220)  loss_ce_unscaled: 0.2058 (0.2132)  class_error_unscaled: 7.6923 (9.7284)  loss_bbox_unscaled: 0.0443 (0.0453)  loss_giou_unscaled: 0.2469 (0.2611)  cardinality_error_unscaled: 292.7500 (292.2505)  loss_ce_0_unscaled: 0.2316 (0.2426)  loss_bbox_0_unscaled: 0.0561 (0.0574)  loss_giou_0_unscaled: 0.3166 (0.3255)  cardinality_error_0_unscaled: 292.8333 (292.0768)  loss_ce_1_unscaled: 0.2197 (0.2288)  loss_bbox_1_unscaled: 0.0460 (0.0467)  loss_giou_1_unscaled: 0.2616 (0.2699)  cardinality_error_1_unscaled: 292.7500 (292.0151)  loss_ce_2_unscaled: 0.2130 (0.2205)  loss_bbox_2_unscaled: 0.0454 (0.0451)  loss_giou_2_unscaled: 0.2509 (0.2618)  cardinality_error_2_unscaled: 291.2500 (291.7529)  loss_ce_3_unscaled: 0.2017 (0.2147)  loss_bbox_3_unscaled: 0.0446 (0.0454)  loss_giou_3_unscaled: 0.2474 (0.2617)  cardinality_error_3_unscaled: 290.9167 (291.6703)  loss_ce_4_unscaled: 0.2048 (0.2132)  loss_bbox_4_unscaled: 0.0443 (0.0453)  loss_giou_4_unscaled: 0.2461 (0.2610)  cardinality_error_4_unscaled: 292.4167 (292.1301)  time: 3.0561  data: 0.0295  max mem: 8138\n",
            "Test:  [380/417]  eta: 0:01:56  class_error: 13.61  loss: 7.4789 (7.3865)  loss_ce: 0.4193 (0.4270)  loss_bbox: 0.2282 (0.2266)  loss_giou: 0.5150 (0.5239)  loss_ce_0: 0.4710 (0.4856)  loss_bbox_0: 0.2921 (0.2867)  loss_giou_0: 0.6393 (0.6519)  loss_ce_1: 0.4582 (0.4583)  loss_bbox_1: 0.2348 (0.2336)  loss_giou_1: 0.5322 (0.5413)  loss_ce_2: 0.4530 (0.4417)  loss_bbox_2: 0.2294 (0.2255)  loss_giou_2: 0.5141 (0.5251)  loss_ce_3: 0.4409 (0.4301)  loss_bbox_3: 0.2235 (0.2269)  loss_giou_3: 0.5162 (0.5251)  loss_ce_4: 0.4138 (0.4271)  loss_bbox_4: 0.2280 (0.2263)  loss_giou_4: 0.5153 (0.5237)  loss_ce_unscaled: 0.2097 (0.2135)  class_error_unscaled: 8.6021 (9.7417)  loss_bbox_unscaled: 0.0456 (0.0453)  loss_giou_unscaled: 0.2575 (0.2620)  cardinality_error_unscaled: 293.1667 (292.2432)  loss_ce_0_unscaled: 0.2355 (0.2428)  loss_bbox_0_unscaled: 0.0584 (0.0573)  loss_giou_0_unscaled: 0.3196 (0.3259)  cardinality_error_0_unscaled: 292.9167 (292.0748)  loss_ce_1_unscaled: 0.2291 (0.2292)  loss_bbox_1_unscaled: 0.0470 (0.0467)  loss_giou_1_unscaled: 0.2661 (0.2707)  cardinality_error_1_unscaled: 292.8333 (292.0129)  loss_ce_2_unscaled: 0.2265 (0.2208)  loss_bbox_2_unscaled: 0.0459 (0.0451)  loss_giou_2_unscaled: 0.2570 (0.2626)  cardinality_error_2_unscaled: 292.5000 (291.7483)  loss_ce_3_unscaled: 0.2205 (0.2150)  loss_bbox_3_unscaled: 0.0447 (0.0454)  loss_giou_3_unscaled: 0.2581 (0.2626)  cardinality_error_3_unscaled: 292.5000 (291.6649)  loss_ce_4_unscaled: 0.2069 (0.2136)  loss_bbox_4_unscaled: 0.0456 (0.0453)  loss_giou_4_unscaled: 0.2576 (0.2618)  cardinality_error_4_unscaled: 292.7500 (292.1218)  time: 2.9702  data: 0.0292  max mem: 8138\n",
            "Test:  [390/417]  eta: 0:01:24  class_error: 12.64  loss: 7.7141 (7.3906)  loss_ce: 0.4274 (0.4272)  loss_bbox: 0.2232 (0.2264)  loss_giou: 0.5638 (0.5244)  loss_ce_0: 0.4959 (0.4863)  loss_bbox_0: 0.2953 (0.2864)  loss_giou_0: 0.6743 (0.6524)  loss_ce_1: 0.4672 (0.4589)  loss_bbox_1: 0.2348 (0.2333)  loss_giou_1: 0.5735 (0.5419)  loss_ce_2: 0.4600 (0.4422)  loss_bbox_2: 0.2268 (0.2253)  loss_giou_2: 0.5494 (0.5256)  loss_ce_3: 0.4431 (0.4303)  loss_bbox_3: 0.2208 (0.2267)  loss_giou_3: 0.5542 (0.5256)  loss_ce_4: 0.4421 (0.4274)  loss_bbox_4: 0.2058 (0.2261)  loss_giou_4: 0.5577 (0.5242)  loss_ce_unscaled: 0.2137 (0.2136)  class_error_unscaled: 10.1449 (9.8054)  loss_bbox_unscaled: 0.0446 (0.0453)  loss_giou_unscaled: 0.2819 (0.2622)  cardinality_error_unscaled: 292.6667 (292.2449)  loss_ce_0_unscaled: 0.2480 (0.2431)  loss_bbox_0_unscaled: 0.0591 (0.0573)  loss_giou_0_unscaled: 0.3371 (0.3262)  cardinality_error_0_unscaled: 292.5000 (292.0799)  loss_ce_1_unscaled: 0.2336 (0.2294)  loss_bbox_1_unscaled: 0.0470 (0.0467)  loss_giou_1_unscaled: 0.2867 (0.2710)  cardinality_error_1_unscaled: 292.5000 (292.0181)  loss_ce_2_unscaled: 0.2300 (0.2211)  loss_bbox_2_unscaled: 0.0454 (0.0451)  loss_giou_2_unscaled: 0.2747 (0.2628)  cardinality_error_2_unscaled: 292.4167 (291.7500)  loss_ce_3_unscaled: 0.2216 (0.2152)  loss_bbox_3_unscaled: 0.0442 (0.0453)  loss_giou_3_unscaled: 0.2771 (0.2628)  cardinality_error_3_unscaled: 292.5000 (291.6701)  loss_ce_4_unscaled: 0.2210 (0.2137)  loss_bbox_4_unscaled: 0.0412 (0.0452)  loss_giou_4_unscaled: 0.2788 (0.2621)  cardinality_error_4_unscaled: 292.5833 (292.1230)  time: 2.9182  data: 0.0283  max mem: 8138\n",
            "Test:  [400/417]  eta: 0:00:53  class_error: 14.58  loss: 7.7823 (7.3924)  loss_ce: 0.4220 (0.4278)  loss_bbox: 0.2037 (0.2261)  loss_giou: 0.5366 (0.5242)  loss_ce_0: 0.4836 (0.4867)  loss_bbox_0: 0.2789 (0.2863)  loss_giou_0: 0.6743 (0.6524)  loss_ce_1: 0.4671 (0.4592)  loss_bbox_1: 0.2140 (0.2333)  loss_giou_1: 0.5690 (0.5422)  loss_ce_2: 0.4304 (0.4426)  loss_bbox_2: 0.2013 (0.2252)  loss_giou_2: 0.5382 (0.5255)  loss_ce_3: 0.4213 (0.4309)  loss_bbox_3: 0.2032 (0.2266)  loss_giou_3: 0.5351 (0.5255)  loss_ce_4: 0.4196 (0.4279)  loss_bbox_4: 0.2036 (0.2259)  loss_giou_4: 0.5324 (0.5240)  loss_ce_unscaled: 0.2110 (0.2139)  class_error_unscaled: 11.3924 (9.8011)  loss_bbox_unscaled: 0.0407 (0.0452)  loss_giou_unscaled: 0.2683 (0.2621)  cardinality_error_unscaled: 292.2500 (292.2554)  loss_ce_0_unscaled: 0.2418 (0.2434)  loss_bbox_0_unscaled: 0.0558 (0.0573)  loss_giou_0_unscaled: 0.3371 (0.3262)  cardinality_error_0_unscaled: 291.9167 (292.0842)  loss_ce_1_unscaled: 0.2335 (0.2296)  loss_bbox_1_unscaled: 0.0428 (0.0467)  loss_giou_1_unscaled: 0.2845 (0.2711)  cardinality_error_1_unscaled: 291.8333 (292.0285)  loss_ce_2_unscaled: 0.2152 (0.2213)  loss_bbox_2_unscaled: 0.0403 (0.0450)  loss_giou_2_unscaled: 0.2691 (0.2628)  cardinality_error_2_unscaled: 291.3333 (291.7519)  loss_ce_3_unscaled: 0.2106 (0.2154)  loss_bbox_3_unscaled: 0.0406 (0.0453)  loss_giou_3_unscaled: 0.2675 (0.2627)  cardinality_error_3_unscaled: 290.9167 (291.6692)  loss_ce_4_unscaled: 0.2098 (0.2139)  loss_bbox_4_unscaled: 0.0407 (0.0452)  loss_giou_4_unscaled: 0.2662 (0.2620)  cardinality_error_4_unscaled: 292.1667 (292.1338)  time: 2.9970  data: 0.0279  max mem: 8138\n",
            "Test:  [410/417]  eta: 0:00:21  class_error: 24.17  loss: 8.1189 (7.4151)  loss_ce: 0.4610 (0.4293)  loss_bbox: 0.2254 (0.2265)  loss_giou: 0.5510 (0.5261)  loss_ce_0: 0.5234 (0.4882)  loss_bbox_0: 0.2828 (0.2866)  loss_giou_0: 0.6748 (0.6543)  loss_ce_1: 0.4963 (0.4608)  loss_bbox_1: 0.2377 (0.2338)  loss_giou_1: 0.5739 (0.5442)  loss_ce_2: 0.4765 (0.4443)  loss_bbox_2: 0.2278 (0.2256)  loss_giou_2: 0.5534 (0.5273)  loss_ce_3: 0.4595 (0.4324)  loss_bbox_3: 0.2256 (0.2269)  loss_giou_3: 0.5469 (0.5273)  loss_ce_4: 0.4605 (0.4293)  loss_bbox_4: 0.2301 (0.2263)  loss_giou_4: 0.5552 (0.5259)  loss_ce_unscaled: 0.2305 (0.2146)  class_error_unscaled: 12.3288 (9.9160)  loss_bbox_unscaled: 0.0451 (0.0453)  loss_giou_unscaled: 0.2755 (0.2631)  cardinality_error_unscaled: 292.2500 (292.2516)  loss_ce_0_unscaled: 0.2617 (0.2441)  loss_bbox_0_unscaled: 0.0566 (0.0573)  loss_giou_0_unscaled: 0.3374 (0.3272)  cardinality_error_0_unscaled: 291.9167 (292.0687)  loss_ce_1_unscaled: 0.2482 (0.2304)  loss_bbox_1_unscaled: 0.0475 (0.0468)  loss_giou_1_unscaled: 0.2869 (0.2721)  cardinality_error_1_unscaled: 291.8333 (292.0164)  loss_ce_2_unscaled: 0.2383 (0.2221)  loss_bbox_2_unscaled: 0.0456 (0.0451)  loss_giou_2_unscaled: 0.2767 (0.2637)  cardinality_error_2_unscaled: 291.9167 (291.7480)  loss_ce_3_unscaled: 0.2298 (0.2162)  loss_bbox_3_unscaled: 0.0451 (0.0454)  loss_giou_3_unscaled: 0.2734 (0.2637)  cardinality_error_3_unscaled: 291.1667 (291.6620)  loss_ce_4_unscaled: 0.2302 (0.2147)  loss_bbox_4_unscaled: 0.0460 (0.0453)  loss_giou_4_unscaled: 0.2776 (0.2630)  cardinality_error_4_unscaled: 292.1667 (292.1290)  time: 3.0060  data: 0.0282  max mem: 8138\n",
            "Test:  [416/417]  eta: 0:00:03  class_error: 7.32  loss: 7.9235 (7.4134)  loss_ce: 0.4384 (0.4290)  loss_bbox: 0.2265 (0.2265)  loss_giou: 0.5550 (0.5261)  loss_ce_0: 0.5234 (0.4883)  loss_bbox_0: 0.2789 (0.2866)  loss_giou_0: 0.6637 (0.6542)  loss_ce_1: 0.4881 (0.4606)  loss_bbox_1: 0.2376 (0.2338)  loss_giou_1: 0.5739 (0.5441)  loss_ce_2: 0.4567 (0.4440)  loss_bbox_2: 0.2329 (0.2256)  loss_giou_2: 0.5534 (0.5273)  loss_ce_3: 0.4415 (0.4321)  loss_bbox_3: 0.2256 (0.2269)  loss_giou_3: 0.5508 (0.5273)  loss_ce_4: 0.4345 (0.4290)  loss_bbox_4: 0.2361 (0.2263)  loss_giou_4: 0.5583 (0.5259)  loss_ce_unscaled: 0.2192 (0.2145)  class_error_unscaled: 10.8333 (9.9353)  loss_bbox_unscaled: 0.0453 (0.0453)  loss_giou_unscaled: 0.2775 (0.2630)  cardinality_error_unscaled: 292.7500 (292.2745)  loss_ce_0_unscaled: 0.2617 (0.2441)  loss_bbox_0_unscaled: 0.0558 (0.0573)  loss_giou_0_unscaled: 0.3319 (0.3271)  cardinality_error_0_unscaled: 292.3333 (292.0907)  loss_ce_1_unscaled: 0.2441 (0.2303)  loss_bbox_1_unscaled: 0.0475 (0.0468)  loss_giou_1_unscaled: 0.2869 (0.2720)  cardinality_error_1_unscaled: 292.7500 (292.0405)  loss_ce_2_unscaled: 0.2284 (0.2220)  loss_bbox_2_unscaled: 0.0466 (0.0451)  loss_giou_2_unscaled: 0.2767 (0.2637)  cardinality_error_2_unscaled: 292.2500 (291.7707)  loss_ce_3_unscaled: 0.2208 (0.2161)  loss_bbox_3_unscaled: 0.0451 (0.0454)  loss_giou_3_unscaled: 0.2754 (0.2636)  cardinality_error_3_unscaled: 291.7500 (291.6833)  loss_ce_4_unscaled: 0.2173 (0.2145)  loss_bbox_4_unscaled: 0.0472 (0.0453)  loss_giou_4_unscaled: 0.2791 (0.2629)  cardinality_error_4_unscaled: 292.5000 (292.1526)  time: 3.0018  data: 0.0279  max mem: 8138\n",
            "Test: Total time: 0:21:41 (3.1222 s / it)\n",
            "Averaged stats: class_error: 7.32  loss: 7.9235 (7.4134)  loss_ce: 0.4384 (0.4290)  loss_bbox: 0.2265 (0.2265)  loss_giou: 0.5550 (0.5261)  loss_ce_0: 0.5234 (0.4883)  loss_bbox_0: 0.2789 (0.2866)  loss_giou_0: 0.6637 (0.6542)  loss_ce_1: 0.4881 (0.4606)  loss_bbox_1: 0.2376 (0.2338)  loss_giou_1: 0.5739 (0.5441)  loss_ce_2: 0.4567 (0.4440)  loss_bbox_2: 0.2329 (0.2256)  loss_giou_2: 0.5534 (0.5273)  loss_ce_3: 0.4415 (0.4321)  loss_bbox_3: 0.2256 (0.2269)  loss_giou_3: 0.5508 (0.5273)  loss_ce_4: 0.4345 (0.4290)  loss_bbox_4: 0.2361 (0.2263)  loss_giou_4: 0.5583 (0.5259)  loss_ce_unscaled: 0.2192 (0.2145)  class_error_unscaled: 10.8333 (9.9353)  loss_bbox_unscaled: 0.0453 (0.0453)  loss_giou_unscaled: 0.2775 (0.2630)  cardinality_error_unscaled: 292.7500 (292.2745)  loss_ce_0_unscaled: 0.2617 (0.2441)  loss_bbox_0_unscaled: 0.0558 (0.0573)  loss_giou_0_unscaled: 0.3319 (0.3271)  cardinality_error_0_unscaled: 292.3333 (292.0907)  loss_ce_1_unscaled: 0.2441 (0.2303)  loss_bbox_1_unscaled: 0.0475 (0.0468)  loss_giou_1_unscaled: 0.2869 (0.2720)  cardinality_error_1_unscaled: 292.7500 (292.0405)  loss_ce_2_unscaled: 0.2284 (0.2220)  loss_bbox_2_unscaled: 0.0466 (0.0451)  loss_giou_2_unscaled: 0.2767 (0.2637)  cardinality_error_2_unscaled: 292.2500 (291.7707)  loss_ce_3_unscaled: 0.2208 (0.2161)  loss_bbox_3_unscaled: 0.0451 (0.0454)  loss_giou_3_unscaled: 0.2754 (0.2636)  cardinality_error_3_unscaled: 291.7500 (291.6833)  loss_ce_4_unscaled: 0.2173 (0.2145)  loss_bbox_4_unscaled: 0.0472 (0.0453)  loss_giou_4_unscaled: 0.2791 (0.2629)  cardinality_error_4_unscaled: 292.5000 (292.1526)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=12.30s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.412\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.604\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.442\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.241\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.442\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.558\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.341\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.560\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.602\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.383\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.646\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py \\\n",
        "    --output_dir exps/deform \\\n",
        "    --coco_path ../COCODIR \\\n",
        "    --batch_size 12 \\\n",
        "    --resume ./pth/mosaic.pth \\\n",
        "    --with_box_refine \\\n",
        "    --eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulYjmyQ_W6IJ",
        "outputId": "74f7c1a1-31df-4b38-da2b-27254f9e83af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "git:\n",
            "  sha: 11169a60c33333af00a4849f1808023eba96a931, status: has uncommited changes, branch: main\n",
            "\n",
            "Namespace(lr=0.0002, lr_backbone_names=['backbone.0'], lr_backbone=2e-05, lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, batch_size=12, weight_decay=0.0001, epochs=50, lr_drop=40, lr_drop_epochs=None, clip_max_norm=0.1, sgd=False, with_box_refine=True, two_stage=False, frozen_weights=None, backbone='resnet50', dilation=False, position_embedding='sine', position_embedding_scale=6.283185307179586, num_feature_levels=4, enc_layers=6, dec_layers=6, dim_feedforward=1024, hidden_dim=256, dropout=0.1, nheads=8, num_queries=300, dec_n_points=4, enc_n_points=4, masks=False, aux_loss=True, set_cost_class=2, set_cost_bbox=5, set_cost_giou=2, mask_loss_coef=1, dice_loss_coef=1, cls_loss_coef=2, bbox_loss_coef=5, giou_loss_coef=2, focal_alpha=0.25, dataset_file='coco', coco_path='../COCODIR', coco_panoptic_path=None, remove_difficult=False, output_dir='exps/deform', device='cuda', seed=42, resume='./pth/mosaic.pth', start_epoch=0, eval=True, num_workers=2, cache_mode=False, distributed=False)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "number of params: 40627260\n",
            "loading annotations into memory...\n",
            "Done (t=14.94s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=1.89s)\n",
            "creating index...\n",
            "index created!\n",
            "transformer.level_embed\n",
            "transformer.encoder.layers.0.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.0.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.0.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.0.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.0.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.0.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.0.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.0.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.0.norm1.weight\n",
            "transformer.encoder.layers.0.norm1.bias\n",
            "transformer.encoder.layers.0.linear1.weight\n",
            "transformer.encoder.layers.0.linear1.bias\n",
            "transformer.encoder.layers.0.linear2.weight\n",
            "transformer.encoder.layers.0.linear2.bias\n",
            "transformer.encoder.layers.0.norm2.weight\n",
            "transformer.encoder.layers.0.norm2.bias\n",
            "transformer.encoder.layers.1.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.1.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.1.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.1.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.1.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.1.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.1.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.1.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.1.norm1.weight\n",
            "transformer.encoder.layers.1.norm1.bias\n",
            "transformer.encoder.layers.1.linear1.weight\n",
            "transformer.encoder.layers.1.linear1.bias\n",
            "transformer.encoder.layers.1.linear2.weight\n",
            "transformer.encoder.layers.1.linear2.bias\n",
            "transformer.encoder.layers.1.norm2.weight\n",
            "transformer.encoder.layers.1.norm2.bias\n",
            "transformer.encoder.layers.2.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.2.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.2.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.2.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.2.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.2.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.2.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.2.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.2.norm1.weight\n",
            "transformer.encoder.layers.2.norm1.bias\n",
            "transformer.encoder.layers.2.linear1.weight\n",
            "transformer.encoder.layers.2.linear1.bias\n",
            "transformer.encoder.layers.2.linear2.weight\n",
            "transformer.encoder.layers.2.linear2.bias\n",
            "transformer.encoder.layers.2.norm2.weight\n",
            "transformer.encoder.layers.2.norm2.bias\n",
            "transformer.encoder.layers.3.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.3.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.3.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.3.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.3.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.3.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.3.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.3.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.3.norm1.weight\n",
            "transformer.encoder.layers.3.norm1.bias\n",
            "transformer.encoder.layers.3.linear1.weight\n",
            "transformer.encoder.layers.3.linear1.bias\n",
            "transformer.encoder.layers.3.linear2.weight\n",
            "transformer.encoder.layers.3.linear2.bias\n",
            "transformer.encoder.layers.3.norm2.weight\n",
            "transformer.encoder.layers.3.norm2.bias\n",
            "transformer.encoder.layers.4.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.4.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.4.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.4.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.4.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.4.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.4.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.4.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.4.norm1.weight\n",
            "transformer.encoder.layers.4.norm1.bias\n",
            "transformer.encoder.layers.4.linear1.weight\n",
            "transformer.encoder.layers.4.linear1.bias\n",
            "transformer.encoder.layers.4.linear2.weight\n",
            "transformer.encoder.layers.4.linear2.bias\n",
            "transformer.encoder.layers.4.norm2.weight\n",
            "transformer.encoder.layers.4.norm2.bias\n",
            "transformer.encoder.layers.5.self_attn.sampling_offsets.weight\n",
            "transformer.encoder.layers.5.self_attn.sampling_offsets.bias\n",
            "transformer.encoder.layers.5.self_attn.attention_weights.weight\n",
            "transformer.encoder.layers.5.self_attn.attention_weights.bias\n",
            "transformer.encoder.layers.5.self_attn.value_proj.weight\n",
            "transformer.encoder.layers.5.self_attn.value_proj.bias\n",
            "transformer.encoder.layers.5.self_attn.output_proj.weight\n",
            "transformer.encoder.layers.5.self_attn.output_proj.bias\n",
            "transformer.encoder.layers.5.norm1.weight\n",
            "transformer.encoder.layers.5.norm1.bias\n",
            "transformer.encoder.layers.5.linear1.weight\n",
            "transformer.encoder.layers.5.linear1.bias\n",
            "transformer.encoder.layers.5.linear2.weight\n",
            "transformer.encoder.layers.5.linear2.bias\n",
            "transformer.encoder.layers.5.norm2.weight\n",
            "transformer.encoder.layers.5.norm2.bias\n",
            "transformer.decoder.layers.0.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.0.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.0.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.0.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.0.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.0.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.0.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.0.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.0.norm1.weight\n",
            "transformer.decoder.layers.0.norm1.bias\n",
            "transformer.decoder.layers.0.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.0.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.0.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.0.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.0.norm2.weight\n",
            "transformer.decoder.layers.0.norm2.bias\n",
            "transformer.decoder.layers.0.linear1.weight\n",
            "transformer.decoder.layers.0.linear1.bias\n",
            "transformer.decoder.layers.0.linear2.weight\n",
            "transformer.decoder.layers.0.linear2.bias\n",
            "transformer.decoder.layers.0.norm3.weight\n",
            "transformer.decoder.layers.0.norm3.bias\n",
            "transformer.decoder.layers.1.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.1.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.1.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.1.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.1.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.1.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.1.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.1.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.1.norm1.weight\n",
            "transformer.decoder.layers.1.norm1.bias\n",
            "transformer.decoder.layers.1.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.1.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.1.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.1.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.1.norm2.weight\n",
            "transformer.decoder.layers.1.norm2.bias\n",
            "transformer.decoder.layers.1.linear1.weight\n",
            "transformer.decoder.layers.1.linear1.bias\n",
            "transformer.decoder.layers.1.linear2.weight\n",
            "transformer.decoder.layers.1.linear2.bias\n",
            "transformer.decoder.layers.1.norm3.weight\n",
            "transformer.decoder.layers.1.norm3.bias\n",
            "transformer.decoder.layers.2.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.2.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.2.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.2.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.2.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.2.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.2.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.2.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.2.norm1.weight\n",
            "transformer.decoder.layers.2.norm1.bias\n",
            "transformer.decoder.layers.2.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.2.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.2.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.2.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.2.norm2.weight\n",
            "transformer.decoder.layers.2.norm2.bias\n",
            "transformer.decoder.layers.2.linear1.weight\n",
            "transformer.decoder.layers.2.linear1.bias\n",
            "transformer.decoder.layers.2.linear2.weight\n",
            "transformer.decoder.layers.2.linear2.bias\n",
            "transformer.decoder.layers.2.norm3.weight\n",
            "transformer.decoder.layers.2.norm3.bias\n",
            "transformer.decoder.layers.3.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.3.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.3.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.3.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.3.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.3.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.3.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.3.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.3.norm1.weight\n",
            "transformer.decoder.layers.3.norm1.bias\n",
            "transformer.decoder.layers.3.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.3.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.3.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.3.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.3.norm2.weight\n",
            "transformer.decoder.layers.3.norm2.bias\n",
            "transformer.decoder.layers.3.linear1.weight\n",
            "transformer.decoder.layers.3.linear1.bias\n",
            "transformer.decoder.layers.3.linear2.weight\n",
            "transformer.decoder.layers.3.linear2.bias\n",
            "transformer.decoder.layers.3.norm3.weight\n",
            "transformer.decoder.layers.3.norm3.bias\n",
            "transformer.decoder.layers.4.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.4.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.4.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.4.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.4.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.4.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.4.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.4.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.4.norm1.weight\n",
            "transformer.decoder.layers.4.norm1.bias\n",
            "transformer.decoder.layers.4.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.4.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.4.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.4.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.4.norm2.weight\n",
            "transformer.decoder.layers.4.norm2.bias\n",
            "transformer.decoder.layers.4.linear1.weight\n",
            "transformer.decoder.layers.4.linear1.bias\n",
            "transformer.decoder.layers.4.linear2.weight\n",
            "transformer.decoder.layers.4.linear2.bias\n",
            "transformer.decoder.layers.4.norm3.weight\n",
            "transformer.decoder.layers.4.norm3.bias\n",
            "transformer.decoder.layers.5.cross_attn.sampling_offsets.weight\n",
            "transformer.decoder.layers.5.cross_attn.sampling_offsets.bias\n",
            "transformer.decoder.layers.5.cross_attn.attention_weights.weight\n",
            "transformer.decoder.layers.5.cross_attn.attention_weights.bias\n",
            "transformer.decoder.layers.5.cross_attn.value_proj.weight\n",
            "transformer.decoder.layers.5.cross_attn.value_proj.bias\n",
            "transformer.decoder.layers.5.cross_attn.output_proj.weight\n",
            "transformer.decoder.layers.5.cross_attn.output_proj.bias\n",
            "transformer.decoder.layers.5.norm1.weight\n",
            "transformer.decoder.layers.5.norm1.bias\n",
            "transformer.decoder.layers.5.self_attn.in_proj_weight\n",
            "transformer.decoder.layers.5.self_attn.in_proj_bias\n",
            "transformer.decoder.layers.5.self_attn.out_proj.weight\n",
            "transformer.decoder.layers.5.self_attn.out_proj.bias\n",
            "transformer.decoder.layers.5.norm2.weight\n",
            "transformer.decoder.layers.5.norm2.bias\n",
            "transformer.decoder.layers.5.linear1.weight\n",
            "transformer.decoder.layers.5.linear1.bias\n",
            "transformer.decoder.layers.5.linear2.weight\n",
            "transformer.decoder.layers.5.linear2.bias\n",
            "transformer.decoder.layers.5.norm3.weight\n",
            "transformer.decoder.layers.5.norm3.bias\n",
            "transformer.decoder.bbox_embed.0.layers.0.weight\n",
            "transformer.decoder.bbox_embed.0.layers.0.bias\n",
            "transformer.decoder.bbox_embed.0.layers.1.weight\n",
            "transformer.decoder.bbox_embed.0.layers.1.bias\n",
            "transformer.decoder.bbox_embed.0.layers.2.weight\n",
            "transformer.decoder.bbox_embed.0.layers.2.bias\n",
            "transformer.decoder.bbox_embed.1.layers.0.weight\n",
            "transformer.decoder.bbox_embed.1.layers.0.bias\n",
            "transformer.decoder.bbox_embed.1.layers.1.weight\n",
            "transformer.decoder.bbox_embed.1.layers.1.bias\n",
            "transformer.decoder.bbox_embed.1.layers.2.weight\n",
            "transformer.decoder.bbox_embed.1.layers.2.bias\n",
            "transformer.decoder.bbox_embed.2.layers.0.weight\n",
            "transformer.decoder.bbox_embed.2.layers.0.bias\n",
            "transformer.decoder.bbox_embed.2.layers.1.weight\n",
            "transformer.decoder.bbox_embed.2.layers.1.bias\n",
            "transformer.decoder.bbox_embed.2.layers.2.weight\n",
            "transformer.decoder.bbox_embed.2.layers.2.bias\n",
            "transformer.decoder.bbox_embed.3.layers.0.weight\n",
            "transformer.decoder.bbox_embed.3.layers.0.bias\n",
            "transformer.decoder.bbox_embed.3.layers.1.weight\n",
            "transformer.decoder.bbox_embed.3.layers.1.bias\n",
            "transformer.decoder.bbox_embed.3.layers.2.weight\n",
            "transformer.decoder.bbox_embed.3.layers.2.bias\n",
            "transformer.decoder.bbox_embed.4.layers.0.weight\n",
            "transformer.decoder.bbox_embed.4.layers.0.bias\n",
            "transformer.decoder.bbox_embed.4.layers.1.weight\n",
            "transformer.decoder.bbox_embed.4.layers.1.bias\n",
            "transformer.decoder.bbox_embed.4.layers.2.weight\n",
            "transformer.decoder.bbox_embed.4.layers.2.bias\n",
            "transformer.decoder.bbox_embed.5.layers.0.weight\n",
            "transformer.decoder.bbox_embed.5.layers.0.bias\n",
            "transformer.decoder.bbox_embed.5.layers.1.weight\n",
            "transformer.decoder.bbox_embed.5.layers.1.bias\n",
            "transformer.decoder.bbox_embed.5.layers.2.weight\n",
            "transformer.decoder.bbox_embed.5.layers.2.bias\n",
            "transformer.reference_points.weight\n",
            "transformer.reference_points.bias\n",
            "class_embed.0.weight\n",
            "class_embed.0.bias\n",
            "class_embed.1.weight\n",
            "class_embed.1.bias\n",
            "class_embed.2.weight\n",
            "class_embed.2.bias\n",
            "class_embed.3.weight\n",
            "class_embed.3.bias\n",
            "class_embed.4.weight\n",
            "class_embed.4.bias\n",
            "class_embed.5.weight\n",
            "class_embed.5.bias\n",
            "query_embed.weight\n",
            "input_proj.0.0.weight\n",
            "input_proj.0.0.bias\n",
            "input_proj.0.1.weight\n",
            "input_proj.0.1.bias\n",
            "input_proj.1.0.weight\n",
            "input_proj.1.0.bias\n",
            "input_proj.1.1.weight\n",
            "input_proj.1.1.bias\n",
            "input_proj.2.0.weight\n",
            "input_proj.2.0.bias\n",
            "input_proj.2.1.weight\n",
            "input_proj.2.1.bias\n",
            "input_proj.3.0.weight\n",
            "input_proj.3.0.bias\n",
            "input_proj.3.1.weight\n",
            "input_proj.3.1.bias\n",
            "backbone.0.body.conv1.weight\n",
            "backbone.0.body.layer1.0.conv1.weight\n",
            "backbone.0.body.layer1.0.conv2.weight\n",
            "backbone.0.body.layer1.0.conv3.weight\n",
            "backbone.0.body.layer1.0.downsample.0.weight\n",
            "backbone.0.body.layer1.1.conv1.weight\n",
            "backbone.0.body.layer1.1.conv2.weight\n",
            "backbone.0.body.layer1.1.conv3.weight\n",
            "backbone.0.body.layer1.2.conv1.weight\n",
            "backbone.0.body.layer1.2.conv2.weight\n",
            "backbone.0.body.layer1.2.conv3.weight\n",
            "backbone.0.body.layer2.0.conv1.weight\n",
            "backbone.0.body.layer2.0.conv2.weight\n",
            "backbone.0.body.layer2.0.conv3.weight\n",
            "backbone.0.body.layer2.0.downsample.0.weight\n",
            "backbone.0.body.layer2.1.conv1.weight\n",
            "backbone.0.body.layer2.1.conv2.weight\n",
            "backbone.0.body.layer2.1.conv3.weight\n",
            "backbone.0.body.layer2.2.conv1.weight\n",
            "backbone.0.body.layer2.2.conv2.weight\n",
            "backbone.0.body.layer2.2.conv3.weight\n",
            "backbone.0.body.layer2.3.conv1.weight\n",
            "backbone.0.body.layer2.3.conv2.weight\n",
            "backbone.0.body.layer2.3.conv3.weight\n",
            "backbone.0.body.layer3.0.conv1.weight\n",
            "backbone.0.body.layer3.0.conv2.weight\n",
            "backbone.0.body.layer3.0.conv3.weight\n",
            "backbone.0.body.layer3.0.downsample.0.weight\n",
            "backbone.0.body.layer3.1.conv1.weight\n",
            "backbone.0.body.layer3.1.conv2.weight\n",
            "backbone.0.body.layer3.1.conv3.weight\n",
            "backbone.0.body.layer3.2.conv1.weight\n",
            "backbone.0.body.layer3.2.conv2.weight\n",
            "backbone.0.body.layer3.2.conv3.weight\n",
            "backbone.0.body.layer3.3.conv1.weight\n",
            "backbone.0.body.layer3.3.conv2.weight\n",
            "backbone.0.body.layer3.3.conv3.weight\n",
            "backbone.0.body.layer3.4.conv1.weight\n",
            "backbone.0.body.layer3.4.conv2.weight\n",
            "backbone.0.body.layer3.4.conv3.weight\n",
            "backbone.0.body.layer3.5.conv1.weight\n",
            "backbone.0.body.layer3.5.conv2.weight\n",
            "backbone.0.body.layer3.5.conv3.weight\n",
            "backbone.0.body.layer4.0.conv1.weight\n",
            "backbone.0.body.layer4.0.conv2.weight\n",
            "backbone.0.body.layer4.0.conv3.weight\n",
            "backbone.0.body.layer4.0.downsample.0.weight\n",
            "backbone.0.body.layer4.1.conv1.weight\n",
            "backbone.0.body.layer4.1.conv2.weight\n",
            "backbone.0.body.layer4.1.conv3.weight\n",
            "backbone.0.body.layer4.2.conv1.weight\n",
            "backbone.0.body.layer4.2.conv2.weight\n",
            "backbone.0.body.layer4.2.conv3.weight\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Test:  [  0/417]  eta: 0:29:01  class_error: 47.37  loss: 18.8875 (18.8875)  loss_ce: 1.4302 (1.4302)  loss_bbox: 0.6762 (0.6762)  loss_giou: 1.0138 (1.0138)  loss_ce_0: 1.4759 (1.4759)  loss_bbox_0: 0.6747 (0.6747)  loss_giou_0: 1.0935 (1.0935)  loss_ce_1: 1.4888 (1.4888)  loss_bbox_1: 0.6625 (0.6625)  loss_giou_1: 1.0118 (1.0118)  loss_ce_2: 1.4569 (1.4569)  loss_bbox_2: 0.6710 (0.6710)  loss_giou_2: 1.0143 (1.0143)  loss_ce_3: 1.4124 (1.4124)  loss_bbox_3: 0.6803 (0.6803)  loss_giou_3: 1.0136 (1.0136)  loss_ce_4: 1.4220 (1.4220)  loss_bbox_4: 0.6761 (0.6761)  loss_giou_4: 1.0135 (1.0135)  loss_ce_unscaled: 0.7151 (0.7151)  class_error_unscaled: 47.3684 (47.3684)  loss_bbox_unscaled: 0.1352 (0.1352)  loss_giou_unscaled: 0.5069 (0.5069)  cardinality_error_unscaled: 291.2500 (291.2500)  loss_ce_0_unscaled: 0.7379 (0.7379)  loss_bbox_0_unscaled: 0.1349 (0.1349)  loss_giou_0_unscaled: 0.5467 (0.5467)  cardinality_error_0_unscaled: 291.7500 (291.7500)  loss_ce_1_unscaled: 0.7444 (0.7444)  loss_bbox_1_unscaled: 0.1325 (0.1325)  loss_giou_1_unscaled: 0.5059 (0.5059)  cardinality_error_1_unscaled: 290.9167 (290.9167)  loss_ce_2_unscaled: 0.7285 (0.7285)  loss_bbox_2_unscaled: 0.1342 (0.1342)  loss_giou_2_unscaled: 0.5072 (0.5072)  cardinality_error_2_unscaled: 288.7500 (288.7500)  loss_ce_3_unscaled: 0.7062 (0.7062)  loss_bbox_3_unscaled: 0.1361 (0.1361)  loss_giou_3_unscaled: 0.5068 (0.5068)  cardinality_error_3_unscaled: 290.5833 (290.5833)  loss_ce_4_unscaled: 0.7110 (0.7110)  loss_bbox_4_unscaled: 0.1352 (0.1352)  loss_giou_4_unscaled: 0.5067 (0.5067)  cardinality_error_4_unscaled: 291.5000 (291.5000)  time: 4.1772  data: 1.0573  max mem: 6678\n",
            "Test:  [ 10/417]  eta: 0:20:36  class_error: 72.73  loss: 18.8875 (19.3038)  loss_ce: 1.5814 (1.6168)  loss_bbox: 0.5980 (0.6030)  loss_giou: 0.9461 (0.9414)  loss_ce_0: 1.6378 (1.7015)  loss_bbox_0: 0.6429 (0.6306)  loss_giou_0: 1.0329 (1.0202)  loss_ce_1: 1.5712 (1.6758)  loss_bbox_1: 0.6217 (0.6005)  loss_giou_1: 0.9600 (0.9561)  loss_ce_2: 1.5874 (1.6756)  loss_bbox_2: 0.5974 (0.6041)  loss_giou_2: 0.9534 (0.9423)  loss_ce_3: 1.5504 (1.6230)  loss_bbox_3: 0.6001 (0.6029)  loss_giou_3: 0.9481 (0.9404)  loss_ce_4: 1.5598 (1.6284)  loss_bbox_4: 0.5892 (0.6005)  loss_giou_4: 0.9442 (0.9407)  loss_ce_unscaled: 0.7907 (0.8084)  class_error_unscaled: 57.1429 (59.1393)  loss_bbox_unscaled: 0.1196 (0.1206)  loss_giou_unscaled: 0.4730 (0.4707)  cardinality_error_unscaled: 291.5000 (291.3712)  loss_ce_0_unscaled: 0.8189 (0.8508)  loss_bbox_0_unscaled: 0.1286 (0.1261)  loss_giou_0_unscaled: 0.5165 (0.5101)  cardinality_error_0_unscaled: 291.5000 (291.0227)  loss_ce_1_unscaled: 0.7856 (0.8379)  loss_bbox_1_unscaled: 0.1243 (0.1201)  loss_giou_1_unscaled: 0.4800 (0.4780)  cardinality_error_1_unscaled: 291.6667 (290.9621)  loss_ce_2_unscaled: 0.7937 (0.8378)  loss_bbox_2_unscaled: 0.1195 (0.1208)  loss_giou_2_unscaled: 0.4767 (0.4711)  cardinality_error_2_unscaled: 290.4167 (290.0303)  loss_ce_3_unscaled: 0.7752 (0.8115)  loss_bbox_3_unscaled: 0.1200 (0.1206)  loss_giou_3_unscaled: 0.4740 (0.4702)  cardinality_error_3_unscaled: 291.3333 (291.1591)  loss_ce_4_unscaled: 0.7799 (0.8142)  loss_bbox_4_unscaled: 0.1178 (0.1201)  loss_giou_4_unscaled: 0.4721 (0.4703)  cardinality_error_4_unscaled: 291.5000 (291.4318)  time: 3.0390  data: 0.1246  max mem: 7374\n",
            "Test:  [ 20/417]  eta: 0:19:43  class_error: 54.26  loss: 17.2413 (18.2635)  loss_ce: 1.5405 (1.5558)  loss_bbox: 0.4980 (0.5453)  loss_giou: 0.8141 (0.8850)  loss_ce_0: 1.6239 (1.6437)  loss_bbox_0: 0.5208 (0.5803)  loss_giou_0: 0.9336 (0.9691)  loss_ce_1: 1.5664 (1.6140)  loss_bbox_1: 0.4904 (0.5440)  loss_giou_1: 0.8567 (0.8991)  loss_ce_2: 1.5737 (1.6186)  loss_bbox_2: 0.5094 (0.5459)  loss_giou_2: 0.8307 (0.8902)  loss_ce_3: 1.5164 (1.5572)  loss_bbox_3: 0.4908 (0.5456)  loss_giou_3: 0.8138 (0.8839)  loss_ce_4: 1.5362 (1.5587)  loss_bbox_4: 0.4908 (0.5424)  loss_giou_4: 0.8138 (0.8848)  loss_ce_unscaled: 0.7703 (0.7779)  class_error_unscaled: 54.2553 (55.8313)  loss_bbox_unscaled: 0.0996 (0.1091)  loss_giou_unscaled: 0.4070 (0.4425)  cardinality_error_unscaled: 291.5000 (291.1468)  loss_ce_0_unscaled: 0.8119 (0.8218)  loss_bbox_0_unscaled: 0.1042 (0.1161)  loss_giou_0_unscaled: 0.4668 (0.4846)  cardinality_error_0_unscaled: 291.4167 (290.9762)  loss_ce_1_unscaled: 0.7832 (0.8070)  loss_bbox_1_unscaled: 0.0981 (0.1088)  loss_giou_1_unscaled: 0.4284 (0.4495)  cardinality_error_1_unscaled: 291.3333 (290.8095)  loss_ce_2_unscaled: 0.7868 (0.8093)  loss_bbox_2_unscaled: 0.1019 (0.1092)  loss_giou_2_unscaled: 0.4154 (0.4451)  cardinality_error_2_unscaled: 290.1667 (289.7024)  loss_ce_3_unscaled: 0.7582 (0.7786)  loss_bbox_3_unscaled: 0.0982 (0.1091)  loss_giou_3_unscaled: 0.4069 (0.4419)  cardinality_error_3_unscaled: 290.6667 (290.9484)  loss_ce_4_unscaled: 0.7681 (0.7793)  loss_bbox_4_unscaled: 0.0982 (0.1085)  loss_giou_4_unscaled: 0.4069 (0.4424)  cardinality_error_4_unscaled: 291.0000 (291.1667)  time: 2.9215  data: 0.0308  max mem: 7755\n",
            "Test:  [ 30/417]  eta: 0:19:21  class_error: 52.11  loss: 16.6947 (17.5377)  loss_ce: 1.4463 (1.4948)  loss_bbox: 0.4544 (0.4921)  loss_giou: 0.7776 (0.8753)  loss_ce_0: 1.5202 (1.5829)  loss_bbox_0: 0.4814 (0.5316)  loss_giou_0: 0.9336 (0.9729)  loss_ce_1: 1.5080 (1.5533)  loss_bbox_1: 0.4592 (0.4941)  loss_giou_1: 0.8130 (0.8922)  loss_ce_2: 1.5039 (1.5527)  loss_bbox_2: 0.4525 (0.4933)  loss_giou_2: 0.7848 (0.8804)  loss_ce_3: 1.4222 (1.4925)  loss_bbox_3: 0.4532 (0.4928)  loss_giou_3: 0.7808 (0.8746)  loss_ce_4: 1.4539 (1.4961)  loss_bbox_4: 0.4533 (0.4904)  loss_giou_4: 0.7782 (0.8755)  loss_ce_unscaled: 0.7232 (0.7474)  class_error_unscaled: 50.0000 (53.7962)  loss_bbox_unscaled: 0.0909 (0.0984)  loss_giou_unscaled: 0.3888 (0.4377)  cardinality_error_unscaled: 289.6667 (290.5484)  loss_ce_0_unscaled: 0.7601 (0.7915)  loss_bbox_0_unscaled: 0.0963 (0.1063)  loss_giou_0_unscaled: 0.4668 (0.4865)  cardinality_error_0_unscaled: 291.0000 (290.5565)  loss_ce_1_unscaled: 0.7540 (0.7767)  loss_bbox_1_unscaled: 0.0918 (0.0988)  loss_giou_1_unscaled: 0.4065 (0.4461)  cardinality_error_1_unscaled: 290.1667 (290.3548)  loss_ce_2_unscaled: 0.7519 (0.7763)  loss_bbox_2_unscaled: 0.0905 (0.0987)  loss_giou_2_unscaled: 0.3924 (0.4402)  cardinality_error_2_unscaled: 289.3333 (289.3253)  loss_ce_3_unscaled: 0.7111 (0.7463)  loss_bbox_3_unscaled: 0.0906 (0.0986)  loss_giou_3_unscaled: 0.3904 (0.4373)  cardinality_error_3_unscaled: 290.0000 (290.4812)  loss_ce_4_unscaled: 0.7270 (0.7480)  loss_bbox_4_unscaled: 0.0907 (0.0981)  loss_giou_4_unscaled: 0.3891 (0.4378)  cardinality_error_4_unscaled: 290.0000 (290.6936)  time: 2.9819  data: 0.0309  max mem: 8137\n",
            "Test:  [ 40/417]  eta: 0:19:07  class_error: 57.14  loss: 16.6723 (17.6311)  loss_ce: 1.4518 (1.4944)  loss_bbox: 0.4544 (0.5046)  loss_giou: 0.8380 (0.8774)  loss_ce_0: 1.5928 (1.5901)  loss_bbox_0: 0.4814 (0.5474)  loss_giou_0: 0.9686 (0.9741)  loss_ce_1: 1.5111 (1.5569)  loss_bbox_1: 0.4572 (0.5087)  loss_giou_1: 0.8518 (0.8919)  loss_ce_2: 1.5039 (1.5499)  loss_bbox_2: 0.4525 (0.5052)  loss_giou_2: 0.8469 (0.8825)  loss_ce_3: 1.4222 (1.4901)  loss_bbox_3: 0.4532 (0.5054)  loss_giou_3: 0.8409 (0.8766)  loss_ce_4: 1.4649 (1.4948)  loss_bbox_4: 0.4533 (0.5033)  loss_giou_4: 0.8388 (0.8777)  loss_ce_unscaled: 0.7259 (0.7472)  class_error_unscaled: 50.6667 (53.2894)  loss_bbox_unscaled: 0.0909 (0.1009)  loss_giou_unscaled: 0.4190 (0.4387)  cardinality_error_unscaled: 289.6667 (290.5955)  loss_ce_0_unscaled: 0.7964 (0.7950)  loss_bbox_0_unscaled: 0.0963 (0.1095)  loss_giou_0_unscaled: 0.4843 (0.4871)  cardinality_error_0_unscaled: 291.0000 (290.5041)  loss_ce_1_unscaled: 0.7556 (0.7785)  loss_bbox_1_unscaled: 0.0914 (0.1017)  loss_giou_1_unscaled: 0.4259 (0.4460)  cardinality_error_1_unscaled: 291.0000 (290.4207)  loss_ce_2_unscaled: 0.7519 (0.7750)  loss_bbox_2_unscaled: 0.0905 (0.1010)  loss_giou_2_unscaled: 0.4235 (0.4412)  cardinality_error_2_unscaled: 289.3333 (289.3720)  loss_ce_3_unscaled: 0.7111 (0.7450)  loss_bbox_3_unscaled: 0.0906 (0.1011)  loss_giou_3_unscaled: 0.4205 (0.4383)  cardinality_error_3_unscaled: 290.1667 (290.5081)  loss_ce_4_unscaled: 0.7324 (0.7474)  loss_bbox_4_unscaled: 0.0907 (0.1007)  loss_giou_4_unscaled: 0.4194 (0.4389)  cardinality_error_4_unscaled: 290.7500 (290.8272)  time: 3.1093  data: 0.0321  max mem: 8137\n",
            "Test:  [ 50/417]  eta: 0:18:40  class_error: 52.22  loss: 19.1002 (17.9364)  loss_ce: 1.5792 (1.5148)  loss_bbox: 0.5805 (0.5205)  loss_giou: 0.9578 (0.8958)  loss_ce_0: 1.6685 (1.6133)  loss_bbox_0: 0.5860 (0.5572)  loss_giou_0: 1.0316 (0.9837)  loss_ce_1: 1.6420 (1.5754)  loss_bbox_1: 0.5644 (0.5234)  loss_giou_1: 0.9545 (0.9078)  loss_ce_2: 1.6352 (1.5688)  loss_bbox_2: 0.5569 (0.5192)  loss_giou_2: 0.9683 (0.8997)  loss_ce_3: 1.5867 (1.5112)  loss_bbox_3: 0.5658 (0.5206)  loss_giou_3: 0.9577 (0.8947)  loss_ce_4: 1.5731 (1.5161)  loss_bbox_4: 0.5706 (0.5194)  loss_giou_4: 0.9577 (0.8949)  loss_ce_unscaled: 0.7896 (0.7574)  class_error_unscaled: 55.8140 (54.1295)  loss_bbox_unscaled: 0.1161 (0.1041)  loss_giou_unscaled: 0.4789 (0.4479)  cardinality_error_unscaled: 288.6667 (290.1585)  loss_ce_0_unscaled: 0.8342 (0.8067)  loss_bbox_0_unscaled: 0.1172 (0.1114)  loss_giou_0_unscaled: 0.5158 (0.4918)  cardinality_error_0_unscaled: 288.9167 (290.0850)  loss_ce_1_unscaled: 0.8210 (0.7877)  loss_bbox_1_unscaled: 0.1129 (0.1047)  loss_giou_1_unscaled: 0.4772 (0.4539)  cardinality_error_1_unscaled: 288.6667 (289.9951)  loss_ce_2_unscaled: 0.8176 (0.7844)  loss_bbox_2_unscaled: 0.1114 (0.1038)  loss_giou_2_unscaled: 0.4841 (0.4498)  cardinality_error_2_unscaled: 287.3333 (288.7533)  loss_ce_3_unscaled: 0.7933 (0.7556)  loss_bbox_3_unscaled: 0.1132 (0.1041)  loss_giou_3_unscaled: 0.4788 (0.4473)  cardinality_error_3_unscaled: 288.8333 (290.0278)  loss_ce_4_unscaled: 0.7865 (0.7581)  loss_bbox_4_unscaled: 0.1141 (0.1039)  loss_giou_4_unscaled: 0.4788 (0.4474)  cardinality_error_4_unscaled: 289.2500 (290.3660)  time: 3.1341  data: 0.0322  max mem: 8138\n",
            "Test:  [ 60/417]  eta: 0:18:11  class_error: 61.36  loss: 18.6373 (18.1809)  loss_ce: 1.5214 (1.5387)  loss_bbox: 0.5340 (0.5241)  loss_giou: 0.9835 (0.9099)  loss_ce_0: 1.5905 (1.6334)  loss_bbox_0: 0.5860 (0.5639)  loss_giou_0: 1.0526 (0.9969)  loss_ce_1: 1.5296 (1.5949)  loss_bbox_1: 0.5464 (0.5278)  loss_giou_1: 1.0024 (0.9223)  loss_ce_2: 1.5861 (1.5927)  loss_bbox_2: 0.5332 (0.5234)  loss_giou_2: 0.9915 (0.9139)  loss_ce_3: 1.5354 (1.5349)  loss_bbox_3: 0.5329 (0.5245)  loss_giou_3: 0.9849 (0.9086)  loss_ce_4: 1.5151 (1.5382)  loss_bbox_4: 0.5267 (0.5238)  loss_giou_4: 0.9927 (0.9090)  loss_ce_unscaled: 0.7607 (0.7694)  class_error_unscaled: 61.2903 (55.4403)  loss_bbox_unscaled: 0.1068 (0.1048)  loss_giou_unscaled: 0.4918 (0.4550)  cardinality_error_unscaled: 289.4167 (290.1571)  loss_ce_0_unscaled: 0.7953 (0.8167)  loss_bbox_0_unscaled: 0.1172 (0.1128)  loss_giou_0_unscaled: 0.5263 (0.4984)  cardinality_error_0_unscaled: 289.9167 (290.1680)  loss_ce_1_unscaled: 0.7648 (0.7975)  loss_bbox_1_unscaled: 0.1093 (0.1056)  loss_giou_1_unscaled: 0.5012 (0.4612)  cardinality_error_1_unscaled: 290.0833 (290.0478)  loss_ce_2_unscaled: 0.7930 (0.7963)  loss_bbox_2_unscaled: 0.1066 (0.1047)  loss_giou_2_unscaled: 0.4958 (0.4570)  cardinality_error_2_unscaled: 287.5000 (288.7937)  loss_ce_3_unscaled: 0.7677 (0.7674)  loss_bbox_3_unscaled: 0.1066 (0.1049)  loss_giou_3_unscaled: 0.4925 (0.4543)  cardinality_error_3_unscaled: 288.5000 (290.0601)  loss_ce_4_unscaled: 0.7575 (0.7691)  loss_bbox_4_unscaled: 0.1053 (0.1048)  loss_giou_4_unscaled: 0.4963 (0.4545)  cardinality_error_4_unscaled: 290.5000 (290.4139)  time: 3.0892  data: 0.0322  max mem: 8138\n",
            "Test:  [ 70/417]  eta: 0:17:49  class_error: 53.76  loss: 17.5596 (18.0163)  loss_ce: 1.5214 (1.5210)  loss_bbox: 0.5015 (0.5203)  loss_giou: 0.9052 (0.9017)  loss_ce_0: 1.5809 (1.6173)  loss_bbox_0: 0.5573 (0.5608)  loss_giou_0: 1.0025 (0.9904)  loss_ce_1: 1.5296 (1.5802)  loss_bbox_1: 0.5182 (0.5244)  loss_giou_1: 0.9293 (0.9147)  loss_ce_2: 1.5635 (1.5766)  loss_bbox_2: 0.5023 (0.5196)  loss_giou_2: 0.9023 (0.9062)  loss_ce_3: 1.5201 (1.5186)  loss_bbox_3: 0.5085 (0.5202)  loss_giou_3: 0.9071 (0.9011)  loss_ce_4: 1.4827 (1.5230)  loss_bbox_4: 0.5087 (0.5193)  loss_giou_4: 0.8988 (0.9009)  loss_ce_unscaled: 0.7607 (0.7605)  class_error_unscaled: 53.5211 (54.2402)  loss_bbox_unscaled: 0.1003 (0.1041)  loss_giou_unscaled: 0.4526 (0.4508)  cardinality_error_unscaled: 290.0833 (290.1737)  loss_ce_0_unscaled: 0.7904 (0.8086)  loss_bbox_0_unscaled: 0.1115 (0.1122)  loss_giou_0_unscaled: 0.5013 (0.4952)  cardinality_error_0_unscaled: 290.5000 (290.2418)  loss_ce_1_unscaled: 0.7648 (0.7901)  loss_bbox_1_unscaled: 0.1036 (0.1049)  loss_giou_1_unscaled: 0.4647 (0.4573)  cardinality_error_1_unscaled: 290.4167 (290.1315)  loss_ce_2_unscaled: 0.7817 (0.7883)  loss_bbox_2_unscaled: 0.1005 (0.1039)  loss_giou_2_unscaled: 0.4512 (0.4531)  cardinality_error_2_unscaled: 289.5833 (288.8357)  loss_ce_3_unscaled: 0.7600 (0.7593)  loss_bbox_3_unscaled: 0.1017 (0.1040)  loss_giou_3_unscaled: 0.4535 (0.4505)  cardinality_error_3_unscaled: 290.5000 (290.0986)  loss_ce_4_unscaled: 0.7414 (0.7615)  loss_bbox_4_unscaled: 0.1017 (0.1039)  loss_giou_4_unscaled: 0.4494 (0.4505)  cardinality_error_4_unscaled: 290.6667 (290.4225)  time: 3.1569  data: 0.0321  max mem: 8138\n",
            "Test:  [ 80/417]  eta: 0:17:14  class_error: 67.19  loss: 17.3967 (17.9145)  loss_ce: 1.4557 (1.5184)  loss_bbox: 0.4818 (0.5178)  loss_giou: 0.8353 (0.8879)  loss_ce_0: 1.5323 (1.6135)  loss_bbox_0: 0.5467 (0.5610)  loss_giou_0: 0.9474 (0.9810)  loss_ce_1: 1.4994 (1.5780)  loss_bbox_1: 0.4976 (0.5231)  loss_giou_1: 0.8885 (0.9034)  loss_ce_2: 1.4831 (1.5740)  loss_bbox_2: 0.4947 (0.5177)  loss_giou_2: 0.8759 (0.8927)  loss_ce_3: 1.4582 (1.5152)  loss_bbox_3: 0.4816 (0.5177)  loss_giou_3: 0.8595 (0.8877)  loss_ce_4: 1.4759 (1.5214)  loss_bbox_4: 0.4820 (0.5167)  loss_giou_4: 0.8490 (0.8872)  loss_ce_unscaled: 0.7279 (0.7592)  class_error_unscaled: 51.6129 (54.0827)  loss_bbox_unscaled: 0.0964 (0.1036)  loss_giou_unscaled: 0.4177 (0.4439)  cardinality_error_unscaled: 290.0833 (290.4084)  loss_ce_0_unscaled: 0.7661 (0.8068)  loss_bbox_0_unscaled: 0.1093 (0.1122)  loss_giou_0_unscaled: 0.4737 (0.4905)  cardinality_error_0_unscaled: 291.2500 (290.3838)  loss_ce_1_unscaled: 0.7497 (0.7890)  loss_bbox_1_unscaled: 0.0995 (0.1046)  loss_giou_1_unscaled: 0.4442 (0.4517)  cardinality_error_1_unscaled: 290.5833 (290.3539)  loss_ce_2_unscaled: 0.7416 (0.7870)  loss_bbox_2_unscaled: 0.0989 (0.1035)  loss_giou_2_unscaled: 0.4380 (0.4464)  cardinality_error_2_unscaled: 289.7500 (289.0535)  loss_ce_3_unscaled: 0.7291 (0.7576)  loss_bbox_3_unscaled: 0.0963 (0.1035)  loss_giou_3_unscaled: 0.4298 (0.4438)  cardinality_error_3_unscaled: 290.7500 (290.2798)  loss_ce_4_unscaled: 0.7379 (0.7607)  loss_bbox_4_unscaled: 0.0964 (0.1033)  loss_giou_4_unscaled: 0.4245 (0.4436)  cardinality_error_4_unscaled: 290.6667 (290.5761)  time: 3.1071  data: 0.0312  max mem: 8138\n",
            "Test:  [ 90/417]  eta: 0:16:43  class_error: 38.39  loss: 17.3086 (17.9179)  loss_ce: 1.5258 (1.5209)  loss_bbox: 0.4856 (0.5140)  loss_giou: 0.8473 (0.8903)  loss_ce_0: 1.5610 (1.6137)  loss_bbox_0: 0.5408 (0.5579)  loss_giou_0: 0.9504 (0.9839)  loss_ce_1: 1.5516 (1.5785)  loss_bbox_1: 0.5069 (0.5193)  loss_giou_1: 0.8682 (0.9059)  loss_ce_2: 1.5724 (1.5764)  loss_bbox_2: 0.4940 (0.5140)  loss_giou_2: 0.8652 (0.8945)  loss_ce_3: 1.5599 (1.5175)  loss_bbox_3: 0.4888 (0.5140)  loss_giou_3: 0.8637 (0.8895)  loss_ce_4: 1.5739 (1.5246)  loss_bbox_4: 0.4953 (0.5133)  loss_giou_4: 0.8490 (0.8895)  loss_ce_unscaled: 0.7629 (0.7604)  class_error_unscaled: 54.5455 (54.1214)  loss_bbox_unscaled: 0.0971 (0.1028)  loss_giou_unscaled: 0.4236 (0.4451)  cardinality_error_unscaled: 291.3333 (290.3718)  loss_ce_0_unscaled: 0.7805 (0.8068)  loss_bbox_0_unscaled: 0.1082 (0.1116)  loss_giou_0_unscaled: 0.4752 (0.4920)  cardinality_error_0_unscaled: 291.1667 (290.3251)  loss_ce_1_unscaled: 0.7758 (0.7893)  loss_bbox_1_unscaled: 0.1014 (0.1039)  loss_giou_1_unscaled: 0.4341 (0.4530)  cardinality_error_1_unscaled: 291.0000 (290.3095)  loss_ce_2_unscaled: 0.7862 (0.7882)  loss_bbox_2_unscaled: 0.0988 (0.1028)  loss_giou_2_unscaled: 0.4326 (0.4472)  cardinality_error_2_unscaled: 290.0000 (289.0623)  loss_ce_3_unscaled: 0.7799 (0.7588)  loss_bbox_3_unscaled: 0.0978 (0.1028)  loss_giou_3_unscaled: 0.4319 (0.4448)  cardinality_error_3_unscaled: 291.1667 (290.2720)  loss_ce_4_unscaled: 0.7870 (0.7623)  loss_bbox_4_unscaled: 0.0991 (0.1027)  loss_giou_4_unscaled: 0.4245 (0.4447)  cardinality_error_4_unscaled: 291.6667 (290.5788)  time: 3.0230  data: 0.0317  max mem: 8138\n",
            "Test:  [100/417]  eta: 0:16:10  class_error: 67.61  loss: 18.4932 (18.0468)  loss_ce: 1.6454 (1.5345)  loss_bbox: 0.4856 (0.5175)  loss_giou: 0.8716 (0.8933)  loss_ce_0: 1.6717 (1.6294)  loss_bbox_0: 0.5408 (0.5607)  loss_giou_0: 0.9876 (0.9879)  loss_ce_1: 1.6808 (1.5933)  loss_bbox_1: 0.5069 (0.5234)  loss_giou_1: 0.8765 (0.9105)  loss_ce_2: 1.6966 (1.5919)  loss_bbox_2: 0.4940 (0.5176)  loss_giou_2: 0.8652 (0.8976)  loss_ce_3: 1.6144 (1.5314)  loss_bbox_3: 0.4888 (0.5174)  loss_giou_3: 0.8637 (0.8926)  loss_ce_4: 1.6252 (1.5386)  loss_bbox_4: 0.4953 (0.5167)  loss_giou_4: 0.8708 (0.8925)  loss_ce_unscaled: 0.8227 (0.7673)  class_error_unscaled: 56.1224 (54.4357)  loss_bbox_unscaled: 0.0971 (0.1035)  loss_giou_unscaled: 0.4358 (0.4467)  cardinality_error_unscaled: 291.0833 (290.4035)  loss_ce_0_unscaled: 0.8358 (0.8147)  loss_bbox_0_unscaled: 0.1082 (0.1121)  loss_giou_0_unscaled: 0.4938 (0.4940)  cardinality_error_0_unscaled: 291.0833 (290.2954)  loss_ce_1_unscaled: 0.8404 (0.7967)  loss_bbox_1_unscaled: 0.1014 (0.1047)  loss_giou_1_unscaled: 0.4383 (0.4552)  cardinality_error_1_unscaled: 290.7500 (290.2723)  loss_ce_2_unscaled: 0.8483 (0.7959)  loss_bbox_2_unscaled: 0.0988 (0.1035)  loss_giou_2_unscaled: 0.4326 (0.4488)  cardinality_error_2_unscaled: 290.0000 (289.0611)  loss_ce_3_unscaled: 0.8072 (0.7657)  loss_bbox_3_unscaled: 0.0978 (0.1035)  loss_giou_3_unscaled: 0.4319 (0.4463)  cardinality_error_3_unscaled: 291.0000 (290.2451)  loss_ce_4_unscaled: 0.8126 (0.7693)  loss_bbox_4_unscaled: 0.0991 (0.1033)  loss_giou_4_unscaled: 0.4354 (0.4462)  cardinality_error_4_unscaled: 291.0833 (290.5842)  time: 3.0287  data: 0.0313  max mem: 8138\n",
            "Test:  [110/417]  eta: 0:15:38  class_error: 41.90  loss: 19.7275 (18.2508)  loss_ce: 1.7557 (1.5471)  loss_bbox: 0.5551 (0.5280)  loss_giou: 0.9384 (0.9046)  loss_ce_0: 1.8427 (1.6421)  loss_bbox_0: 0.6356 (0.5705)  loss_giou_0: 1.0856 (0.9979)  loss_ce_1: 1.8106 (1.6060)  loss_bbox_1: 0.5837 (0.5338)  loss_giou_1: 0.9872 (0.9212)  loss_ce_2: 1.8096 (1.6044)  loss_bbox_2: 0.5590 (0.5283)  loss_giou_2: 0.9553 (0.9092)  loss_ce_3: 1.7262 (1.5434)  loss_bbox_3: 0.5576 (0.5280)  loss_giou_3: 0.9420 (0.9041)  loss_ce_4: 1.7206 (1.5512)  loss_bbox_4: 0.5527 (0.5271)  loss_giou_4: 0.9386 (0.9038)  loss_ce_unscaled: 0.8778 (0.7735)  class_error_unscaled: 63.1579 (54.9621)  loss_bbox_unscaled: 0.1110 (0.1056)  loss_giou_unscaled: 0.4692 (0.4523)  cardinality_error_unscaled: 291.0833 (290.4415)  loss_ce_0_unscaled: 0.9214 (0.8210)  loss_bbox_0_unscaled: 0.1271 (0.1141)  loss_giou_0_unscaled: 0.5428 (0.4990)  cardinality_error_0_unscaled: 291.0000 (290.3393)  loss_ce_1_unscaled: 0.9053 (0.8030)  loss_bbox_1_unscaled: 0.1167 (0.1068)  loss_giou_1_unscaled: 0.4936 (0.4606)  cardinality_error_1_unscaled: 290.7500 (290.3123)  loss_ce_2_unscaled: 0.9048 (0.8022)  loss_bbox_2_unscaled: 0.1118 (0.1057)  loss_giou_2_unscaled: 0.4776 (0.4546)  cardinality_error_2_unscaled: 289.6667 (289.1156)  loss_ce_3_unscaled: 0.8631 (0.7717)  loss_bbox_3_unscaled: 0.1115 (0.1056)  loss_giou_3_unscaled: 0.4710 (0.4521)  cardinality_error_3_unscaled: 291.0000 (290.3063)  loss_ce_4_unscaled: 0.8603 (0.7756)  loss_bbox_4_unscaled: 0.1105 (0.1054)  loss_giou_4_unscaled: 0.4693 (0.4519)  cardinality_error_4_unscaled: 291.2500 (290.6306)  time: 3.0025  data: 0.0311  max mem: 8138\n",
            "Test:  [120/417]  eta: 0:15:03  class_error: 54.55  loss: 19.8305 (18.2694)  loss_ce: 1.7538 (1.5508)  loss_bbox: 0.5075 (0.5268)  loss_giou: 1.0084 (0.9067)  loss_ce_0: 1.8205 (1.6423)  loss_bbox_0: 0.5515 (0.5687)  loss_giou_0: 1.0855 (0.9989)  loss_ce_1: 1.7869 (1.6077)  loss_bbox_1: 0.5212 (0.5325)  loss_giou_1: 1.0290 (0.9232)  loss_ce_2: 1.7691 (1.6066)  loss_bbox_2: 0.5184 (0.5275)  loss_giou_2: 1.0092 (0.9113)  loss_ce_3: 1.7262 (1.5457)  loss_bbox_3: 0.5185 (0.5271)  loss_giou_3: 1.0140 (0.9065)  loss_ce_4: 1.7206 (1.5548)  loss_bbox_4: 0.5148 (0.5262)  loss_giou_4: 1.0131 (0.9060)  loss_ce_unscaled: 0.8769 (0.7754)  class_error_unscaled: 63.1579 (55.0655)  loss_bbox_unscaled: 0.1015 (0.1054)  loss_giou_unscaled: 0.5042 (0.4534)  cardinality_error_unscaled: 290.1667 (290.3134)  loss_ce_0_unscaled: 0.9103 (0.8212)  loss_bbox_0_unscaled: 0.1103 (0.1137)  loss_giou_0_unscaled: 0.5428 (0.4995)  cardinality_error_0_unscaled: 290.0833 (290.1715)  loss_ce_1_unscaled: 0.8935 (0.8038)  loss_bbox_1_unscaled: 0.1042 (0.1065)  loss_giou_1_unscaled: 0.5145 (0.4616)  cardinality_error_1_unscaled: 290.2500 (290.1384)  loss_ce_2_unscaled: 0.8846 (0.8033)  loss_bbox_2_unscaled: 0.1037 (0.1055)  loss_giou_2_unscaled: 0.5046 (0.4556)  cardinality_error_2_unscaled: 289.5833 (288.9566)  loss_ce_3_unscaled: 0.8631 (0.7729)  loss_bbox_3_unscaled: 0.1037 (0.1054)  loss_giou_3_unscaled: 0.5070 (0.4532)  cardinality_error_3_unscaled: 290.6667 (290.1667)  loss_ce_4_unscaled: 0.8603 (0.7774)  loss_bbox_4_unscaled: 0.1030 (0.1052)  loss_giou_4_unscaled: 0.5065 (0.4530)  cardinality_error_4_unscaled: 290.4167 (290.4787)  time: 2.9490  data: 0.0307  max mem: 8138\n",
            "Test:  [130/417]  eta: 0:14:31  class_error: 55.74  loss: 18.8918 (18.2487)  loss_ce: 1.6247 (1.5509)  loss_bbox: 0.4901 (0.5256)  loss_giou: 0.9402 (0.9052)  loss_ce_0: 1.6364 (1.6413)  loss_bbox_0: 0.5419 (0.5679)  loss_giou_0: 1.0018 (0.9972)  loss_ce_1: 1.6259 (1.6063)  loss_bbox_1: 0.5066 (0.5314)  loss_giou_1: 0.9191 (0.9209)  loss_ce_2: 1.6170 (1.6048)  loss_bbox_2: 0.5180 (0.5263)  loss_giou_2: 0.9365 (0.9098)  loss_ce_3: 1.5965 (1.5454)  loss_bbox_3: 0.5185 (0.5260)  loss_giou_3: 0.9374 (0.9051)  loss_ce_4: 1.6281 (1.5547)  loss_bbox_4: 0.5148 (0.5253)  loss_giou_4: 0.9309 (0.9045)  loss_ce_unscaled: 0.8123 (0.7755)  class_error_unscaled: 55.6962 (55.1658)  loss_bbox_unscaled: 0.0980 (0.1051)  loss_giou_unscaled: 0.4701 (0.4526)  cardinality_error_unscaled: 288.4167 (290.2214)  loss_ce_0_unscaled: 0.8182 (0.8206)  loss_bbox_0_unscaled: 0.1084 (0.1136)  loss_giou_0_unscaled: 0.5009 (0.4986)  cardinality_error_0_unscaled: 289.6667 (290.0922)  loss_ce_1_unscaled: 0.8130 (0.8032)  loss_bbox_1_unscaled: 0.1013 (0.1063)  loss_giou_1_unscaled: 0.4596 (0.4605)  cardinality_error_1_unscaled: 289.5000 (290.0439)  loss_ce_2_unscaled: 0.8085 (0.8024)  loss_bbox_2_unscaled: 0.1036 (0.1053)  loss_giou_2_unscaled: 0.4683 (0.4549)  cardinality_error_2_unscaled: 287.9167 (288.8174)  loss_ce_3_unscaled: 0.7982 (0.7727)  loss_bbox_3_unscaled: 0.1037 (0.1052)  loss_giou_3_unscaled: 0.4687 (0.4525)  cardinality_error_3_unscaled: 289.5833 (290.0732)  loss_ce_4_unscaled: 0.8140 (0.7773)  loss_bbox_4_unscaled: 0.1030 (0.1051)  loss_giou_4_unscaled: 0.4655 (0.4522)  cardinality_error_4_unscaled: 289.8333 (290.3741)  time: 2.9104  data: 0.0295  max mem: 8138\n",
            "Test:  [140/417]  eta: 0:14:01  class_error: 66.00  loss: 18.8918 (18.3063)  loss_ce: 1.5821 (1.5574)  loss_bbox: 0.5309 (0.5261)  loss_giou: 0.8957 (0.9082)  loss_ce_0: 1.6412 (1.6477)  loss_bbox_0: 0.5876 (0.5681)  loss_giou_0: 0.9917 (1.0000)  loss_ce_1: 1.6022 (1.6132)  loss_bbox_1: 0.5283 (0.5314)  loss_giou_1: 0.9100 (0.9237)  loss_ce_2: 1.6014 (1.6104)  loss_bbox_2: 0.5395 (0.5271)  loss_giou_2: 0.9021 (0.9129)  loss_ce_3: 1.5671 (1.5514)  loss_bbox_3: 0.5281 (0.5266)  loss_giou_3: 0.9082 (0.9082)  loss_ce_4: 1.5854 (1.5610)  loss_bbox_4: 0.5329 (0.5255)  loss_giou_4: 0.8990 (0.9076)  loss_ce_unscaled: 0.7910 (0.7787)  class_error_unscaled: 56.1404 (55.4661)  loss_bbox_unscaled: 0.1062 (0.1052)  loss_giou_unscaled: 0.4479 (0.4541)  cardinality_error_unscaled: 290.4167 (290.2045)  loss_ce_0_unscaled: 0.8206 (0.8239)  loss_bbox_0_unscaled: 0.1175 (0.1136)  loss_giou_0_unscaled: 0.4958 (0.5000)  cardinality_error_0_unscaled: 290.2500 (290.0396)  loss_ce_1_unscaled: 0.8011 (0.8066)  loss_bbox_1_unscaled: 0.1057 (0.1063)  loss_giou_1_unscaled: 0.4550 (0.4618)  cardinality_error_1_unscaled: 289.6667 (289.9994)  loss_ce_2_unscaled: 0.8007 (0.8052)  loss_bbox_2_unscaled: 0.1079 (0.1054)  loss_giou_2_unscaled: 0.4510 (0.4564)  cardinality_error_2_unscaled: 288.7500 (288.7571)  loss_ce_3_unscaled: 0.7836 (0.7757)  loss_bbox_3_unscaled: 0.1056 (0.1053)  loss_giou_3_unscaled: 0.4541 (0.4541)  cardinality_error_3_unscaled: 289.7500 (289.9947)  loss_ce_4_unscaled: 0.7927 (0.7805)  loss_bbox_4_unscaled: 0.1066 (0.1051)  loss_giou_4_unscaled: 0.4495 (0.4538)  cardinality_error_4_unscaled: 290.0833 (290.3186)  time: 3.0071  data: 0.0311  max mem: 8138\n",
            "Test:  [150/417]  eta: 0:13:30  class_error: 50.00  loss: 19.0236 (18.2596)  loss_ce: 1.4978 (1.5528)  loss_bbox: 0.5556 (0.5254)  loss_giou: 0.9011 (0.9054)  loss_ce_0: 1.6340 (1.6423)  loss_bbox_0: 0.5884 (0.5692)  loss_giou_0: 0.9906 (0.9977)  loss_ce_1: 1.5833 (1.6084)  loss_bbox_1: 0.5535 (0.5307)  loss_giou_1: 0.9100 (0.9206)  loss_ce_2: 1.5632 (1.6063)  loss_bbox_2: 0.5478 (0.5264)  loss_giou_2: 0.9005 (0.9097)  loss_ce_3: 1.5145 (1.5471)  loss_bbox_3: 0.5505 (0.5260)  loss_giou_3: 0.9029 (0.9053)  loss_ce_4: 1.5164 (1.5567)  loss_bbox_4: 0.5450 (0.5247)  loss_giou_4: 0.9110 (0.9050)  loss_ce_unscaled: 0.7489 (0.7764)  class_error_unscaled: 55.0459 (55.1902)  loss_bbox_unscaled: 0.1111 (0.1051)  loss_giou_unscaled: 0.4505 (0.4527)  cardinality_error_unscaled: 290.0833 (290.1247)  loss_ce_0_unscaled: 0.8170 (0.8211)  loss_bbox_0_unscaled: 0.1177 (0.1138)  loss_giou_0_unscaled: 0.4953 (0.4989)  cardinality_error_0_unscaled: 289.3333 (289.9553)  loss_ce_1_unscaled: 0.7916 (0.8042)  loss_bbox_1_unscaled: 0.1107 (0.1061)  loss_giou_1_unscaled: 0.4550 (0.4603)  cardinality_error_1_unscaled: 290.0000 (289.9608)  loss_ce_2_unscaled: 0.7816 (0.8031)  loss_bbox_2_unscaled: 0.1096 (0.1053)  loss_giou_2_unscaled: 0.4503 (0.4549)  cardinality_error_2_unscaled: 287.9167 (288.7064)  loss_ce_3_unscaled: 0.7573 (0.7735)  loss_bbox_3_unscaled: 0.1101 (0.1052)  loss_giou_3_unscaled: 0.4514 (0.4526)  cardinality_error_3_unscaled: 288.8333 (289.9250)  loss_ce_4_unscaled: 0.7582 (0.7783)  loss_bbox_4_unscaled: 0.1090 (0.1049)  loss_giou_4_unscaled: 0.4555 (0.4525)  cardinality_error_4_unscaled: 290.0000 (290.2379)  time: 3.0321  data: 0.0330  max mem: 8138\n",
            "Test:  [160/417]  eta: 0:12:59  class_error: 44.00  loss: 18.3500 (18.1760)  loss_ce: 1.4435 (1.5464)  loss_bbox: 0.4749 (0.5212)  loss_giou: 0.8912 (0.9021)  loss_ce_0: 1.5336 (1.6351)  loss_bbox_0: 0.5279 (0.5647)  loss_giou_0: 0.9692 (0.9947)  loss_ce_1: 1.5200 (1.6027)  loss_bbox_1: 0.4885 (0.5263)  loss_giou_1: 0.8863 (0.9171)  loss_ce_2: 1.5033 (1.5997)  loss_bbox_2: 0.4669 (0.5221)  loss_giou_2: 0.8918 (0.9068)  loss_ce_3: 1.4696 (1.5410)  loss_bbox_3: 0.4740 (0.5215)  loss_giou_3: 0.8872 (0.9019)  loss_ce_4: 1.4699 (1.5504)  loss_bbox_4: 0.4702 (0.5204)  loss_giou_4: 0.8871 (0.9017)  loss_ce_unscaled: 0.7218 (0.7732)  class_error_unscaled: 50.0000 (54.9143)  loss_bbox_unscaled: 0.0950 (0.1042)  loss_giou_unscaled: 0.4456 (0.4511)  cardinality_error_unscaled: 290.7500 (290.2226)  loss_ce_0_unscaled: 0.7668 (0.8175)  loss_bbox_0_unscaled: 0.1056 (0.1129)  loss_giou_0_unscaled: 0.4846 (0.4974)  cardinality_error_0_unscaled: 290.0833 (290.0575)  loss_ce_1_unscaled: 0.7600 (0.8014)  loss_bbox_1_unscaled: 0.0977 (0.1053)  loss_giou_1_unscaled: 0.4431 (0.4585)  cardinality_error_1_unscaled: 290.4167 (290.0642)  loss_ce_2_unscaled: 0.7517 (0.7999)  loss_bbox_2_unscaled: 0.0934 (0.1044)  loss_giou_2_unscaled: 0.4459 (0.4534)  cardinality_error_2_unscaled: 289.3333 (288.8230)  loss_ce_3_unscaled: 0.7348 (0.7705)  loss_bbox_3_unscaled: 0.0948 (0.1043)  loss_giou_3_unscaled: 0.4436 (0.4509)  cardinality_error_3_unscaled: 290.8333 (290.0311)  loss_ce_4_unscaled: 0.7349 (0.7752)  loss_bbox_4_unscaled: 0.0940 (0.1041)  loss_giou_4_unscaled: 0.4436 (0.4509)  cardinality_error_4_unscaled: 290.5000 (290.3385)  time: 2.9848  data: 0.0318  max mem: 8138\n",
            "Test:  [170/417]  eta: 0:12:28  class_error: 56.18  loss: 18.3707 (18.1971)  loss_ce: 1.5317 (1.5455)  loss_bbox: 0.4770 (0.5231)  loss_giou: 0.9126 (0.9051)  loss_ce_0: 1.5908 (1.6335)  loss_bbox_0: 0.5112 (0.5669)  loss_giou_0: 1.0004 (0.9971)  loss_ce_1: 1.5674 (1.6016)  loss_bbox_1: 0.4882 (0.5281)  loss_giou_1: 0.9340 (0.9199)  loss_ce_2: 1.5897 (1.5982)  loss_bbox_2: 0.4770 (0.5242)  loss_giou_2: 0.9067 (0.9096)  loss_ce_3: 1.5066 (1.5395)  loss_bbox_3: 0.4767 (0.5236)  loss_giou_3: 0.9141 (0.9049)  loss_ce_4: 1.5246 (1.5491)  loss_bbox_4: 0.4823 (0.5223)  loss_giou_4: 0.9144 (0.9048)  loss_ce_unscaled: 0.7658 (0.7727)  class_error_unscaled: 53.9683 (54.8984)  loss_bbox_unscaled: 0.0954 (0.1046)  loss_giou_unscaled: 0.4563 (0.4525)  cardinality_error_unscaled: 291.1667 (290.1433)  loss_ce_0_unscaled: 0.7954 (0.8167)  loss_bbox_0_unscaled: 0.1022 (0.1134)  loss_giou_0_unscaled: 0.5002 (0.4986)  cardinality_error_0_unscaled: 291.0000 (289.9888)  loss_ce_1_unscaled: 0.7837 (0.8008)  loss_bbox_1_unscaled: 0.0976 (0.1056)  loss_giou_1_unscaled: 0.4670 (0.4600)  cardinality_error_1_unscaled: 290.3333 (289.9834)  loss_ce_2_unscaled: 0.7949 (0.7991)  loss_bbox_2_unscaled: 0.0954 (0.1048)  loss_giou_2_unscaled: 0.4533 (0.4548)  cardinality_error_2_unscaled: 289.4167 (288.7573)  loss_ce_3_unscaled: 0.7533 (0.7697)  loss_bbox_3_unscaled: 0.0953 (0.1047)  loss_giou_3_unscaled: 0.4571 (0.4525)  cardinality_error_3_unscaled: 290.8333 (289.9586)  loss_ce_4_unscaled: 0.7623 (0.7746)  loss_bbox_4_unscaled: 0.0965 (0.1045)  loss_giou_4_unscaled: 0.4572 (0.4524)  cardinality_error_4_unscaled: 291.2500 (290.2505)  time: 2.9965  data: 0.0302  max mem: 8138\n",
            "Test:  [180/417]  eta: 0:11:57  class_error: 45.16  loss: 16.4874 (18.0646)  loss_ce: 1.4120 (1.5336)  loss_bbox: 0.4772 (0.5198)  loss_giou: 0.8187 (0.8975)  loss_ce_0: 1.4795 (1.6227)  loss_bbox_0: 0.5112 (0.5641)  loss_giou_0: 0.9398 (0.9900)  loss_ce_1: 1.4206 (1.5894)  loss_bbox_1: 0.4804 (0.5251)  loss_giou_1: 0.8716 (0.9126)  loss_ce_2: 1.4218 (1.5866)  loss_bbox_2: 0.4725 (0.5208)  loss_giou_2: 0.8395 (0.9022)  loss_ce_3: 1.4033 (1.5287)  loss_bbox_3: 0.4706 (0.5201)  loss_giou_3: 0.8314 (0.8975)  loss_ce_4: 1.4028 (1.5377)  loss_bbox_4: 0.4718 (0.5189)  loss_giou_4: 0.8186 (0.8973)  loss_ce_unscaled: 0.7060 (0.7668)  class_error_unscaled: 51.5625 (54.4098)  loss_bbox_unscaled: 0.0954 (0.1040)  loss_giou_unscaled: 0.4093 (0.4488)  cardinality_error_unscaled: 290.4167 (290.0875)  loss_ce_0_unscaled: 0.7398 (0.8114)  loss_bbox_0_unscaled: 0.1022 (0.1128)  loss_giou_0_unscaled: 0.4699 (0.4950)  cardinality_error_0_unscaled: 290.5000 (289.9282)  loss_ce_1_unscaled: 0.7103 (0.7947)  loss_bbox_1_unscaled: 0.0961 (0.1050)  loss_giou_1_unscaled: 0.4358 (0.4563)  cardinality_error_1_unscaled: 290.2500 (289.9452)  loss_ce_2_unscaled: 0.7109 (0.7933)  loss_bbox_2_unscaled: 0.0945 (0.1042)  loss_giou_2_unscaled: 0.4198 (0.4511)  cardinality_error_2_unscaled: 287.8333 (288.6621)  loss_ce_3_unscaled: 0.7017 (0.7644)  loss_bbox_3_unscaled: 0.0941 (0.1040)  loss_giou_3_unscaled: 0.4157 (0.4487)  cardinality_error_3_unscaled: 290.0833 (289.8927)  loss_ce_4_unscaled: 0.7014 (0.7688)  loss_bbox_4_unscaled: 0.0944 (0.1038)  loss_giou_4_unscaled: 0.4093 (0.4486)  cardinality_error_4_unscaled: 290.2500 (290.1846)  time: 2.9998  data: 0.0298  max mem: 8138\n",
            "Test:  [190/417]  eta: 0:11:26  class_error: 52.50  loss: 16.9914 (18.1730)  loss_ce: 1.5668 (1.5437)  loss_bbox: 0.4838 (0.5238)  loss_giou: 0.8187 (0.9018)  loss_ce_0: 1.6314 (1.6335)  loss_bbox_0: 0.5190 (0.5682)  loss_giou_0: 0.9398 (0.9932)  loss_ce_1: 1.5863 (1.5991)  loss_bbox_1: 0.4944 (0.5293)  loss_giou_1: 0.8689 (0.9166)  loss_ce_2: 1.5928 (1.5968)  loss_bbox_2: 0.4847 (0.5247)  loss_giou_2: 0.8395 (0.9065)  loss_ce_3: 1.5165 (1.5380)  loss_bbox_3: 0.4806 (0.5242)  loss_giou_3: 0.8314 (0.9017)  loss_ce_4: 1.5496 (1.5475)  loss_bbox_4: 0.4724 (0.5228)  loss_giou_4: 0.8186 (0.9015)  loss_ce_unscaled: 0.7834 (0.7719)  class_error_unscaled: 52.0833 (54.7714)  loss_bbox_unscaled: 0.0968 (0.1048)  loss_giou_unscaled: 0.4093 (0.4509)  cardinality_error_unscaled: 290.1667 (290.1078)  loss_ce_0_unscaled: 0.8157 (0.8167)  loss_bbox_0_unscaled: 0.1038 (0.1136)  loss_giou_0_unscaled: 0.4699 (0.4966)  cardinality_error_0_unscaled: 290.0833 (289.9197)  loss_ce_1_unscaled: 0.7932 (0.7996)  loss_bbox_1_unscaled: 0.0989 (0.1059)  loss_giou_1_unscaled: 0.4345 (0.4583)  cardinality_error_1_unscaled: 290.2500 (289.9472)  loss_ce_2_unscaled: 0.7964 (0.7984)  loss_bbox_2_unscaled: 0.0969 (0.1049)  loss_giou_2_unscaled: 0.4198 (0.4533)  cardinality_error_2_unscaled: 287.6667 (288.6265)  loss_ce_3_unscaled: 0.7583 (0.7690)  loss_bbox_3_unscaled: 0.0961 (0.1048)  loss_giou_3_unscaled: 0.4157 (0.4508)  cardinality_error_3_unscaled: 290.0833 (289.9001)  loss_ce_4_unscaled: 0.7748 (0.7738)  loss_bbox_4_unscaled: 0.0945 (0.1046)  loss_giou_4_unscaled: 0.4093 (0.4508)  cardinality_error_4_unscaled: 290.7500 (290.2199)  time: 2.9860  data: 0.0308  max mem: 8138\n",
            "Test:  [200/417]  eta: 0:10:55  class_error: 72.73  loss: 19.2592 (18.1483)  loss_ce: 1.5719 (1.5393)  loss_bbox: 0.5725 (0.5252)  loss_giou: 0.8899 (0.9012)  loss_ce_0: 1.7444 (1.6292)  loss_bbox_0: 0.6010 (0.5691)  loss_giou_0: 0.9549 (0.9921)  loss_ce_1: 1.6382 (1.5943)  loss_bbox_1: 0.5670 (0.5312)  loss_giou_1: 0.8855 (0.9157)  loss_ce_2: 1.6422 (1.5918)  loss_bbox_2: 0.5790 (0.5264)  loss_giou_2: 0.8718 (0.9056)  loss_ce_3: 1.5487 (1.5331)  loss_bbox_3: 0.5770 (0.5256)  loss_giou_3: 0.8793 (0.9010)  loss_ce_4: 1.5876 (1.5426)  loss_bbox_4: 0.5735 (0.5242)  loss_giou_4: 0.8852 (0.9009)  loss_ce_unscaled: 0.7860 (0.7696)  class_error_unscaled: 53.0303 (54.5514)  loss_bbox_unscaled: 0.1145 (0.1050)  loss_giou_unscaled: 0.4449 (0.4506)  cardinality_error_unscaled: 290.2500 (290.0879)  loss_ce_0_unscaled: 0.8722 (0.8146)  loss_bbox_0_unscaled: 0.1202 (0.1138)  loss_giou_0_unscaled: 0.4775 (0.4960)  cardinality_error_0_unscaled: 290.5833 (289.8852)  loss_ce_1_unscaled: 0.8191 (0.7971)  loss_bbox_1_unscaled: 0.1134 (0.1062)  loss_giou_1_unscaled: 0.4428 (0.4578)  cardinality_error_1_unscaled: 290.2500 (289.9167)  loss_ce_2_unscaled: 0.8211 (0.7959)  loss_bbox_2_unscaled: 0.1158 (0.1053)  loss_giou_2_unscaled: 0.4359 (0.4528)  cardinality_error_2_unscaled: 287.7500 (288.5891)  loss_ce_3_unscaled: 0.7743 (0.7666)  loss_bbox_3_unscaled: 0.1154 (0.1051)  loss_giou_3_unscaled: 0.4397 (0.4505)  cardinality_error_3_unscaled: 290.4167 (289.8727)  loss_ce_4_unscaled: 0.7938 (0.7713)  loss_bbox_4_unscaled: 0.1147 (0.1048)  loss_giou_4_unscaled: 0.4426 (0.4504)  cardinality_error_4_unscaled: 291.0000 (290.2119)  time: 2.9497  data: 0.0305  max mem: 8138\n",
            "Test:  [210/417]  eta: 0:10:24  class_error: 60.81  loss: 19.7134 (18.1980)  loss_ce: 1.5160 (1.5433)  loss_bbox: 0.5103 (0.5250)  loss_giou: 0.9020 (0.9055)  loss_ce_0: 1.6035 (1.6330)  loss_bbox_0: 0.5509 (0.5694)  loss_giou_0: 0.9996 (0.9965)  loss_ce_1: 1.5701 (1.5979)  loss_bbox_1: 0.5125 (0.5315)  loss_giou_1: 0.9292 (0.9204)  loss_ce_2: 1.5795 (1.5955)  loss_bbox_2: 0.5100 (0.5263)  loss_giou_2: 0.9057 (0.9103)  loss_ce_3: 1.5111 (1.5373)  loss_bbox_3: 0.5083 (0.5252)  loss_giou_3: 0.9053 (0.9052)  loss_ce_4: 1.5302 (1.5463)  loss_bbox_4: 0.5130 (0.5239)  loss_giou_4: 0.9072 (0.9053)  loss_ce_unscaled: 0.7580 (0.7717)  class_error_unscaled: 55.1020 (54.7399)  loss_bbox_unscaled: 0.1021 (0.1050)  loss_giou_unscaled: 0.4510 (0.4527)  cardinality_error_unscaled: 290.5000 (290.0912)  loss_ce_0_unscaled: 0.8017 (0.8165)  loss_bbox_0_unscaled: 0.1102 (0.1139)  loss_giou_0_unscaled: 0.4998 (0.4982)  cardinality_error_0_unscaled: 290.0833 (289.8306)  loss_ce_1_unscaled: 0.7851 (0.7990)  loss_bbox_1_unscaled: 0.1025 (0.1063)  loss_giou_1_unscaled: 0.4646 (0.4602)  cardinality_error_1_unscaled: 290.0000 (289.8851)  loss_ce_2_unscaled: 0.7897 (0.7978)  loss_bbox_2_unscaled: 0.1020 (0.1053)  loss_giou_2_unscaled: 0.4529 (0.4552)  cardinality_error_2_unscaled: 288.5000 (288.5407)  loss_ce_3_unscaled: 0.7555 (0.7687)  loss_bbox_3_unscaled: 0.1017 (0.1050)  loss_giou_3_unscaled: 0.4527 (0.4526)  cardinality_error_3_unscaled: 290.5833 (289.8559)  loss_ce_4_unscaled: 0.7651 (0.7732)  loss_bbox_4_unscaled: 0.1026 (0.1048)  loss_giou_4_unscaled: 0.4536 (0.4526)  cardinality_error_4_unscaled: 290.5000 (290.1995)  time: 2.9300  data: 0.0296  max mem: 8138\n",
            "Test:  [220/417]  eta: 0:09:54  class_error: 55.14  loss: 19.7810 (18.2104)  loss_ce: 1.5944 (1.5434)  loss_bbox: 0.5088 (0.5247)  loss_giou: 0.9497 (0.9082)  loss_ce_0: 1.7601 (1.6326)  loss_bbox_0: 0.5239 (0.5689)  loss_giou_0: 1.0149 (0.9988)  loss_ce_1: 1.7197 (1.5972)  loss_bbox_1: 0.5000 (0.5313)  loss_giou_1: 0.9725 (0.9230)  loss_ce_2: 1.6468 (1.5950)  loss_bbox_2: 0.5100 (0.5260)  loss_giou_2: 0.9598 (0.9130)  loss_ce_3: 1.6023 (1.5372)  loss_bbox_3: 0.5083 (0.5250)  loss_giou_3: 0.9487 (0.9081)  loss_ce_4: 1.6160 (1.5465)  loss_bbox_4: 0.5130 (0.5236)  loss_giou_4: 0.9474 (0.9079)  loss_ce_unscaled: 0.7972 (0.7717)  class_error_unscaled: 56.2500 (54.7074)  loss_bbox_unscaled: 0.1018 (0.1049)  loss_giou_unscaled: 0.4749 (0.4541)  cardinality_error_unscaled: 290.4167 (290.0400)  loss_ce_0_unscaled: 0.8800 (0.8163)  loss_bbox_0_unscaled: 0.1048 (0.1138)  loss_giou_0_unscaled: 0.5075 (0.4994)  cardinality_error_0_unscaled: 290.0833 (289.8002)  loss_ce_1_unscaled: 0.8599 (0.7986)  loss_bbox_1_unscaled: 0.1000 (0.1063)  loss_giou_1_unscaled: 0.4863 (0.4615)  cardinality_error_1_unscaled: 290.0000 (289.8303)  loss_ce_2_unscaled: 0.8234 (0.7975)  loss_bbox_2_unscaled: 0.1020 (0.1052)  loss_giou_2_unscaled: 0.4799 (0.4565)  cardinality_error_2_unscaled: 288.5000 (288.4868)  loss_ce_3_unscaled: 0.8012 (0.7686)  loss_bbox_3_unscaled: 0.1017 (0.1050)  loss_giou_3_unscaled: 0.4743 (0.4541)  cardinality_error_3_unscaled: 290.1667 (289.8122)  loss_ce_4_unscaled: 0.8080 (0.7732)  loss_bbox_4_unscaled: 0.1026 (0.1047)  loss_giou_4_unscaled: 0.4737 (0.4539)  cardinality_error_4_unscaled: 290.3333 (290.1523)  time: 2.9959  data: 0.0319  max mem: 8138\n",
            "Test:  [230/417]  eta: 0:09:23  class_error: 58.75  loss: 17.7575 (18.2005)  loss_ce: 1.4793 (1.5440)  loss_bbox: 0.4371 (0.5219)  loss_giou: 0.8971 (0.9082)  loss_ce_0: 1.5909 (1.6340)  loss_bbox_0: 0.4864 (0.5669)  loss_giou_0: 1.0006 (0.9991)  loss_ce_1: 1.5299 (1.5986)  loss_bbox_1: 0.4342 (0.5288)  loss_giou_1: 0.9115 (0.9226)  loss_ce_2: 1.5234 (1.5963)  loss_bbox_2: 0.4322 (0.5233)  loss_giou_2: 0.9138 (0.9128)  loss_ce_3: 1.4772 (1.5380)  loss_bbox_3: 0.4372 (0.5223)  loss_giou_3: 0.9008 (0.9080)  loss_ce_4: 1.5069 (1.5471)  loss_bbox_4: 0.4366 (0.5209)  loss_giou_4: 0.8962 (0.9078)  loss_ce_unscaled: 0.7397 (0.7720)  class_error_unscaled: 52.5253 (54.6305)  loss_bbox_unscaled: 0.0874 (0.1044)  loss_giou_unscaled: 0.4485 (0.4541)  cardinality_error_unscaled: 290.0833 (290.0238)  loss_ce_0_unscaled: 0.7955 (0.8170)  loss_bbox_0_unscaled: 0.0973 (0.1134)  loss_giou_0_unscaled: 0.5003 (0.4996)  cardinality_error_0_unscaled: 290.1667 (289.7302)  loss_ce_1_unscaled: 0.7650 (0.7993)  loss_bbox_1_unscaled: 0.0868 (0.1058)  loss_giou_1_unscaled: 0.4558 (0.4613)  cardinality_error_1_unscaled: 290.0833 (289.7857)  loss_ce_2_unscaled: 0.7617 (0.7981)  loss_bbox_2_unscaled: 0.0864 (0.1047)  loss_giou_2_unscaled: 0.4569 (0.4564)  cardinality_error_2_unscaled: 289.2500 (288.4275)  loss_ce_3_unscaled: 0.7386 (0.7690)  loss_bbox_3_unscaled: 0.0874 (0.1045)  loss_giou_3_unscaled: 0.4504 (0.4540)  cardinality_error_3_unscaled: 289.8333 (289.7587)  loss_ce_4_unscaled: 0.7535 (0.7736)  loss_bbox_4_unscaled: 0.0873 (0.1042)  loss_giou_4_unscaled: 0.4481 (0.4539)  cardinality_error_4_unscaled: 290.2500 (290.1180)  time: 2.9545  data: 0.0317  max mem: 8138\n",
            "Test:  [240/417]  eta: 0:08:53  class_error: 63.11  loss: 16.9357 (18.1552)  loss_ce: 1.4276 (1.5390)  loss_bbox: 0.4417 (0.5217)  loss_giou: 0.8522 (0.9054)  loss_ce_0: 1.5909 (1.6287)  loss_bbox_0: 0.5225 (0.5671)  loss_giou_0: 0.9618 (0.9974)  loss_ce_1: 1.4904 (1.5938)  loss_bbox_1: 0.4642 (0.5288)  loss_giou_1: 0.8626 (0.9203)  loss_ce_2: 1.5234 (1.5915)  loss_bbox_2: 0.4529 (0.5230)  loss_giou_2: 0.8629 (0.9100)  loss_ce_3: 1.4772 (1.5332)  loss_bbox_3: 0.4450 (0.5222)  loss_giou_3: 0.8515 (0.9052)  loss_ce_4: 1.4376 (1.5422)  loss_bbox_4: 0.4412 (0.5208)  loss_giou_4: 0.8514 (0.9050)  loss_ce_unscaled: 0.7138 (0.7695)  class_error_unscaled: 49.4253 (54.4568)  loss_bbox_unscaled: 0.0883 (0.1043)  loss_giou_unscaled: 0.4261 (0.4527)  cardinality_error_unscaled: 290.3333 (290.0263)  loss_ce_0_unscaled: 0.7955 (0.8143)  loss_bbox_0_unscaled: 0.1045 (0.1134)  loss_giou_0_unscaled: 0.4809 (0.4987)  cardinality_error_0_unscaled: 290.0000 (289.7493)  loss_ce_1_unscaled: 0.7452 (0.7969)  loss_bbox_1_unscaled: 0.0928 (0.1058)  loss_giou_1_unscaled: 0.4313 (0.4601)  cardinality_error_1_unscaled: 290.5000 (289.7998)  loss_ce_2_unscaled: 0.7617 (0.7957)  loss_bbox_2_unscaled: 0.0906 (0.1046)  loss_giou_2_unscaled: 0.4315 (0.4550)  cardinality_error_2_unscaled: 289.0000 (288.4357)  loss_ce_3_unscaled: 0.7386 (0.7666)  loss_bbox_3_unscaled: 0.0890 (0.1044)  loss_giou_3_unscaled: 0.4257 (0.4526)  cardinality_error_3_unscaled: 289.5833 (289.7466)  loss_ce_4_unscaled: 0.7188 (0.7711)  loss_bbox_4_unscaled: 0.0882 (0.1042)  loss_giou_4_unscaled: 0.4257 (0.4525)  cardinality_error_4_unscaled: 290.1667 (290.1103)  time: 2.9433  data: 0.0308  max mem: 8138\n",
            "Test:  [250/417]  eta: 0:08:23  class_error: 33.96  loss: 16.2998 (18.1070)  loss_ce: 1.3884 (1.5356)  loss_bbox: 0.4534 (0.5196)  loss_giou: 0.8012 (0.9029)  loss_ce_0: 1.4636 (1.6249)  loss_bbox_0: 0.5225 (0.5650)  loss_giou_0: 0.9046 (0.9953)  loss_ce_1: 1.4385 (1.5905)  loss_bbox_1: 0.4810 (0.5266)  loss_giou_1: 0.8004 (0.9177)  loss_ce_2: 1.4606 (1.5880)  loss_bbox_2: 0.4571 (0.5207)  loss_giou_2: 0.8017 (0.9076)  loss_ce_3: 1.3813 (1.5294)  loss_bbox_3: 0.4576 (0.5201)  loss_giou_3: 0.8027 (0.9029)  loss_ce_4: 1.4098 (1.5388)  loss_bbox_4: 0.4570 (0.5187)  loss_giou_4: 0.7984 (0.9026)  loss_ce_unscaled: 0.6942 (0.7678)  class_error_unscaled: 47.6636 (54.3439)  loss_bbox_unscaled: 0.0907 (0.1039)  loss_giou_unscaled: 0.4006 (0.4515)  cardinality_error_unscaled: 291.0833 (290.0946)  loss_ce_0_unscaled: 0.7318 (0.8125)  loss_bbox_0_unscaled: 0.1045 (0.1130)  loss_giou_0_unscaled: 0.4523 (0.4976)  cardinality_error_0_unscaled: 290.4167 (289.8171)  loss_ce_1_unscaled: 0.7192 (0.7953)  loss_bbox_1_unscaled: 0.0962 (0.1053)  loss_giou_1_unscaled: 0.4002 (0.4589)  cardinality_error_1_unscaled: 290.6667 (289.8619)  loss_ce_2_unscaled: 0.7303 (0.7940)  loss_bbox_2_unscaled: 0.0914 (0.1041)  loss_giou_2_unscaled: 0.4008 (0.4538)  cardinality_error_2_unscaled: 289.3333 (288.4937)  loss_ce_3_unscaled: 0.6906 (0.7647)  loss_bbox_3_unscaled: 0.0915 (0.1040)  loss_giou_3_unscaled: 0.4013 (0.4514)  cardinality_error_3_unscaled: 291.0000 (289.8387)  loss_ce_4_unscaled: 0.7049 (0.7694)  loss_bbox_4_unscaled: 0.0914 (0.1037)  loss_giou_4_unscaled: 0.3992 (0.4513)  cardinality_error_4_unscaled: 291.1667 (290.1823)  time: 3.0450  data: 0.0317  max mem: 8138\n",
            "Test:  [260/417]  eta: 0:07:52  class_error: 77.14  loss: 17.1382 (18.1216)  loss_ce: 1.5474 (1.5373)  loss_bbox: 0.5098 (0.5206)  loss_giou: 0.8455 (0.9022)  loss_ce_0: 1.6673 (1.6267)  loss_bbox_0: 0.5610 (0.5664)  loss_giou_0: 0.9246 (0.9949)  loss_ce_1: 1.6308 (1.5927)  loss_bbox_1: 0.5068 (0.5279)  loss_giou_1: 0.8378 (0.9176)  loss_ce_2: 1.6101 (1.5900)  loss_bbox_2: 0.5050 (0.5217)  loss_giou_2: 0.8295 (0.9070)  loss_ce_3: 1.5671 (1.5313)  loss_bbox_3: 0.5068 (0.5211)  loss_giou_3: 0.8451 (0.9022)  loss_ce_4: 1.5493 (1.5405)  loss_bbox_4: 0.5009 (0.5197)  loss_giou_4: 0.8453 (0.9019)  loss_ce_unscaled: 0.7737 (0.7687)  class_error_unscaled: 53.9474 (54.4429)  loss_bbox_unscaled: 0.1020 (0.1041)  loss_giou_unscaled: 0.4228 (0.4511)  cardinality_error_unscaled: 291.9167 (290.1300)  loss_ce_0_unscaled: 0.8336 (0.8133)  loss_bbox_0_unscaled: 0.1122 (0.1133)  loss_giou_0_unscaled: 0.4623 (0.4974)  cardinality_error_0_unscaled: 291.1667 (289.8289)  loss_ce_1_unscaled: 0.8154 (0.7963)  loss_bbox_1_unscaled: 0.1014 (0.1056)  loss_giou_1_unscaled: 0.4189 (0.4588)  cardinality_error_1_unscaled: 291.3333 (289.8879)  loss_ce_2_unscaled: 0.8051 (0.7950)  loss_bbox_2_unscaled: 0.1010 (0.1043)  loss_giou_2_unscaled: 0.4147 (0.4535)  cardinality_error_2_unscaled: 289.8333 (288.5128)  loss_ce_3_unscaled: 0.7836 (0.7657)  loss_bbox_3_unscaled: 0.1014 (0.1042)  loss_giou_3_unscaled: 0.4225 (0.4511)  cardinality_error_3_unscaled: 291.6667 (289.8598)  loss_ce_4_unscaled: 0.7747 (0.7702)  loss_bbox_4_unscaled: 0.1002 (0.1039)  loss_giou_4_unscaled: 0.4227 (0.4509)  cardinality_error_4_unscaled: 292.0833 (290.2187)  time: 3.0170  data: 0.0312  max mem: 8138\n",
            "Test:  [270/417]  eta: 0:07:23  class_error: 39.74  loss: 17.1382 (18.0484)  loss_ce: 1.5290 (1.5315)  loss_bbox: 0.5113 (0.5185)  loss_giou: 0.7770 (0.8978)  loss_ce_0: 1.6465 (1.6211)  loss_bbox_0: 0.5385 (0.5641)  loss_giou_0: 0.8515 (0.9907)  loss_ce_1: 1.5941 (1.5869)  loss_bbox_1: 0.5205 (0.5258)  loss_giou_1: 0.7770 (0.9131)  loss_ce_2: 1.6211 (1.5845)  loss_bbox_2: 0.5126 (0.5197)  loss_giou_2: 0.7681 (0.9024)  loss_ce_3: 1.5391 (1.5257)  loss_bbox_3: 0.5109 (0.5190)  loss_giou_3: 0.7728 (0.8977)  loss_ce_4: 1.5221 (1.5347)  loss_bbox_4: 0.5139 (0.5176)  loss_giou_4: 0.7648 (0.8975)  loss_ce_unscaled: 0.7645 (0.7657)  class_error_unscaled: 53.1915 (54.1659)  loss_bbox_unscaled: 0.1023 (0.1037)  loss_giou_unscaled: 0.3885 (0.4489)  cardinality_error_unscaled: 290.6667 (290.0517)  loss_ce_0_unscaled: 0.8233 (0.8106)  loss_bbox_0_unscaled: 0.1077 (0.1128)  loss_giou_0_unscaled: 0.4258 (0.4954)  cardinality_error_0_unscaled: 290.6667 (289.7549)  loss_ce_1_unscaled: 0.7971 (0.7935)  loss_bbox_1_unscaled: 0.1041 (0.1052)  loss_giou_1_unscaled: 0.3885 (0.4565)  cardinality_error_1_unscaled: 290.5833 (289.8094)  loss_ce_2_unscaled: 0.8105 (0.7922)  loss_bbox_2_unscaled: 0.1025 (0.1039)  loss_giou_2_unscaled: 0.3841 (0.4512)  cardinality_error_2_unscaled: 288.9167 (288.3770)  loss_ce_3_unscaled: 0.7695 (0.7628)  loss_bbox_3_unscaled: 0.1022 (0.1038)  loss_giou_3_unscaled: 0.3864 (0.4489)  cardinality_error_3_unscaled: 289.7500 (289.7737)  loss_ce_4_unscaled: 0.7610 (0.7673)  loss_bbox_4_unscaled: 0.1028 (0.1035)  loss_giou_4_unscaled: 0.3824 (0.4488)  cardinality_error_4_unscaled: 290.4167 (290.1461)  time: 3.0978  data: 0.0320  max mem: 8138\n",
            "Test:  [280/417]  eta: 0:06:53  class_error: 58.96  loss: 17.1328 (18.0541)  loss_ce: 1.4115 (1.5310)  loss_bbox: 0.5014 (0.5181)  loss_giou: 0.8813 (0.9000)  loss_ce_0: 1.4744 (1.6200)  loss_bbox_0: 0.5304 (0.5633)  loss_giou_0: 1.0010 (0.9930)  loss_ce_1: 1.4318 (1.5861)  loss_bbox_1: 0.4927 (0.5250)  loss_giou_1: 0.8852 (0.9152)  loss_ce_2: 1.4606 (1.5837)  loss_bbox_2: 0.4964 (0.5192)  loss_giou_2: 0.8910 (0.9045)  loss_ce_3: 1.4111 (1.5253)  loss_bbox_3: 0.4949 (0.5186)  loss_giou_3: 0.8839 (0.8999)  loss_ce_4: 1.3984 (1.5343)  loss_bbox_4: 0.4968 (0.5172)  loss_giou_4: 0.8836 (0.8997)  loss_ce_unscaled: 0.7058 (0.7655)  class_error_unscaled: 48.8189 (54.0720)  loss_bbox_unscaled: 0.1003 (0.1036)  loss_giou_unscaled: 0.4407 (0.4500)  cardinality_error_unscaled: 288.7500 (289.9997)  loss_ce_0_unscaled: 0.7372 (0.8100)  loss_bbox_0_unscaled: 0.1061 (0.1127)  loss_giou_0_unscaled: 0.5005 (0.4965)  cardinality_error_0_unscaled: 288.8333 (289.6785)  loss_ce_1_unscaled: 0.7159 (0.7930)  loss_bbox_1_unscaled: 0.0985 (0.1050)  loss_giou_1_unscaled: 0.4426 (0.4576)  cardinality_error_1_unscaled: 288.6667 (289.7553)  loss_ce_2_unscaled: 0.7303 (0.7919)  loss_bbox_2_unscaled: 0.0993 (0.1038)  loss_giou_2_unscaled: 0.4455 (0.4523)  cardinality_error_2_unscaled: 288.3333 (288.3479)  loss_ce_3_unscaled: 0.7056 (0.7627)  loss_bbox_3_unscaled: 0.0990 (0.1037)  loss_giou_3_unscaled: 0.4420 (0.4499)  cardinality_error_3_unscaled: 288.8333 (289.7153)  loss_ce_4_unscaled: 0.6992 (0.7671)  loss_bbox_4_unscaled: 0.0994 (0.1034)  loss_giou_4_unscaled: 0.4418 (0.4498)  cardinality_error_4_unscaled: 289.0000 (290.0961)  time: 3.0542  data: 0.0326  max mem: 8138\n",
            "Test:  [290/417]  eta: 0:06:22  class_error: 36.36  loss: 18.1308 (18.0819)  loss_ce: 1.4115 (1.5327)  loss_bbox: 0.5114 (0.5209)  loss_giou: 0.9606 (0.9003)  loss_ce_0: 1.5166 (1.6226)  loss_bbox_0: 0.5500 (0.5654)  loss_giou_0: 1.0165 (0.9929)  loss_ce_1: 1.4573 (1.5876)  loss_bbox_1: 0.5172 (0.5274)  loss_giou_1: 0.9623 (0.9156)  loss_ce_2: 1.4606 (1.5852)  loss_bbox_2: 0.5102 (0.5220)  loss_giou_2: 0.9601 (0.9049)  loss_ce_3: 1.4116 (1.5267)  loss_bbox_3: 0.5116 (0.5214)  loss_giou_3: 0.9573 (0.9003)  loss_ce_4: 1.4161 (1.5359)  loss_bbox_4: 0.5137 (0.5200)  loss_giou_4: 0.9583 (0.9001)  loss_ce_unscaled: 0.7058 (0.7664)  class_error_unscaled: 52.3810 (54.1050)  loss_bbox_unscaled: 0.1023 (0.1042)  loss_giou_unscaled: 0.4803 (0.4502)  cardinality_error_unscaled: 289.9167 (290.0278)  loss_ce_0_unscaled: 0.7583 (0.8113)  loss_bbox_0_unscaled: 0.1100 (0.1131)  loss_giou_0_unscaled: 0.5083 (0.4964)  cardinality_error_0_unscaled: 289.6667 (289.7202)  loss_ce_1_unscaled: 0.7287 (0.7938)  loss_bbox_1_unscaled: 0.1034 (0.1055)  loss_giou_1_unscaled: 0.4812 (0.4578)  cardinality_error_1_unscaled: 290.6667 (289.8133)  loss_ce_2_unscaled: 0.7303 (0.7926)  loss_bbox_2_unscaled: 0.1020 (0.1044)  loss_giou_2_unscaled: 0.4800 (0.4524)  cardinality_error_2_unscaled: 288.6667 (288.3783)  loss_ce_3_unscaled: 0.7058 (0.7633)  loss_bbox_3_unscaled: 0.1023 (0.1043)  loss_giou_3_unscaled: 0.4787 (0.4501)  cardinality_error_3_unscaled: 289.8333 (289.7709)  loss_ce_4_unscaled: 0.7081 (0.7680)  loss_bbox_4_unscaled: 0.1027 (0.1040)  loss_giou_4_unscaled: 0.4792 (0.4500)  cardinality_error_4_unscaled: 290.3333 (290.1355)  time: 2.9227  data: 0.0331  max mem: 8138\n",
            "Test:  [300/417]  eta: 0:05:52  class_error: 48.08  loss: 18.1818 (18.1115)  loss_ce: 1.6391 (1.5342)  loss_bbox: 0.4895 (0.5219)  loss_giou: 0.9341 (0.9030)  loss_ce_0: 1.6865 (1.6235)  loss_bbox_0: 0.5548 (0.5664)  loss_giou_0: 0.9997 (0.9950)  loss_ce_1: 1.6728 (1.5889)  loss_bbox_1: 0.5097 (0.5283)  loss_giou_1: 0.9500 (0.9183)  loss_ce_2: 1.6707 (1.5867)  loss_bbox_2: 0.4967 (0.5230)  loss_giou_2: 0.9388 (0.9075)  loss_ce_3: 1.6343 (1.5284)  loss_bbox_3: 0.4951 (0.5224)  loss_giou_3: 0.9380 (0.9029)  loss_ce_4: 1.6613 (1.5376)  loss_bbox_4: 0.4952 (0.5210)  loss_giou_4: 0.9295 (0.9025)  loss_ce_unscaled: 0.8195 (0.7671)  class_error_unscaled: 55.0000 (54.1201)  loss_bbox_unscaled: 0.0979 (0.1044)  loss_giou_unscaled: 0.4670 (0.4515)  cardinality_error_unscaled: 290.3333 (290.0161)  loss_ce_0_unscaled: 0.8432 (0.8118)  loss_bbox_0_unscaled: 0.1110 (0.1133)  loss_giou_0_unscaled: 0.4998 (0.4975)  cardinality_error_0_unscaled: 290.0833 (289.6766)  loss_ce_1_unscaled: 0.8364 (0.7945)  loss_bbox_1_unscaled: 0.1019 (0.1057)  loss_giou_1_unscaled: 0.4750 (0.4591)  cardinality_error_1_unscaled: 290.5000 (289.7561)  loss_ce_2_unscaled: 0.8354 (0.7934)  loss_bbox_2_unscaled: 0.0993 (0.1046)  loss_giou_2_unscaled: 0.4694 (0.4538)  cardinality_error_2_unscaled: 287.9167 (288.3054)  loss_ce_3_unscaled: 0.8172 (0.7642)  loss_bbox_3_unscaled: 0.0990 (0.1045)  loss_giou_3_unscaled: 0.4690 (0.4514)  cardinality_error_3_unscaled: 289.9167 (289.7173)  loss_ce_4_unscaled: 0.8306 (0.7688)  loss_bbox_4_unscaled: 0.0990 (0.1042)  loss_giou_4_unscaled: 0.4647 (0.4513)  cardinality_error_4_unscaled: 290.5833 (290.1268)  time: 2.9538  data: 0.0332  max mem: 8138\n",
            "Test:  [310/417]  eta: 0:05:22  class_error: 78.26  loss: 17.4478 (18.1082)  loss_ce: 1.6096 (1.5367)  loss_bbox: 0.4416 (0.5204)  loss_giou: 0.8577 (0.9014)  loss_ce_0: 1.6367 (1.6256)  loss_bbox_0: 0.5390 (0.5655)  loss_giou_0: 0.9802 (0.9940)  loss_ce_1: 1.5997 (1.5906)  loss_bbox_1: 0.4684 (0.5271)  loss_giou_1: 0.8899 (0.9172)  loss_ce_2: 1.5954 (1.5889)  loss_bbox_2: 0.4415 (0.5213)  loss_giou_2: 0.8533 (0.9059)  loss_ce_3: 1.5630 (1.5305)  loss_bbox_3: 0.4483 (0.5210)  loss_giou_3: 0.8515 (0.9013)  loss_ce_4: 1.6109 (1.5404)  loss_bbox_4: 0.4447 (0.5196)  loss_giou_4: 0.8517 (0.9009)  loss_ce_unscaled: 0.8048 (0.7683)  class_error_unscaled: 57.0175 (54.2052)  loss_bbox_unscaled: 0.0883 (0.1041)  loss_giou_unscaled: 0.4288 (0.4507)  cardinality_error_unscaled: 291.0000 (290.0643)  loss_ce_0_unscaled: 0.8184 (0.8128)  loss_bbox_0_unscaled: 0.1078 (0.1131)  loss_giou_0_unscaled: 0.4901 (0.4970)  cardinality_error_0_unscaled: 290.1667 (289.7160)  loss_ce_1_unscaled: 0.7998 (0.7953)  loss_bbox_1_unscaled: 0.0937 (0.1054)  loss_giou_1_unscaled: 0.4450 (0.4586)  cardinality_error_1_unscaled: 290.1667 (289.7956)  loss_ce_2_unscaled: 0.7977 (0.7945)  loss_bbox_2_unscaled: 0.0883 (0.1043)  loss_giou_2_unscaled: 0.4266 (0.4530)  cardinality_error_2_unscaled: 287.9167 (288.3575)  loss_ce_3_unscaled: 0.7815 (0.7652)  loss_bbox_3_unscaled: 0.0897 (0.1042)  loss_giou_3_unscaled: 0.4258 (0.4506)  cardinality_error_3_unscaled: 290.1667 (289.7733)  loss_ce_4_unscaled: 0.8055 (0.7702)  loss_bbox_4_unscaled: 0.0889 (0.1039)  loss_giou_4_unscaled: 0.4259 (0.4505)  cardinality_error_4_unscaled: 291.0000 (290.1787)  time: 2.9922  data: 0.0321  max mem: 8138\n",
            "Test:  [320/417]  eta: 0:04:51  class_error: 40.30  loss: 17.0192 (18.0982)  loss_ce: 1.5227 (1.5352)  loss_bbox: 0.4402 (0.5202)  loss_giou: 0.8504 (0.9013)  loss_ce_0: 1.6162 (1.6245)  loss_bbox_0: 0.5064 (0.5652)  loss_giou_0: 0.9561 (0.9941)  loss_ce_1: 1.5565 (1.5896)  loss_bbox_1: 0.4672 (0.5268)  loss_giou_1: 0.8730 (0.9169)  loss_ce_2: 1.5835 (1.5874)  loss_bbox_2: 0.4394 (0.5210)  loss_giou_2: 0.8478 (0.9059)  loss_ce_3: 1.4907 (1.5290)  loss_bbox_3: 0.4430 (0.5209)  loss_giou_3: 0.8487 (0.9011)  loss_ce_4: 1.5271 (1.5390)  loss_bbox_4: 0.4358 (0.5194)  loss_giou_4: 0.8479 (0.9008)  loss_ce_unscaled: 0.7613 (0.7676)  class_error_unscaled: 50.0000 (54.1113)  loss_bbox_unscaled: 0.0880 (0.1040)  loss_giou_unscaled: 0.4252 (0.4507)  cardinality_error_unscaled: 291.5000 (290.0602)  loss_ce_0_unscaled: 0.8081 (0.8122)  loss_bbox_0_unscaled: 0.1013 (0.1130)  loss_giou_0_unscaled: 0.4781 (0.4971)  cardinality_error_0_unscaled: 290.1667 (289.7316)  loss_ce_1_unscaled: 0.7783 (0.7948)  loss_bbox_1_unscaled: 0.0934 (0.1054)  loss_giou_1_unscaled: 0.4365 (0.4585)  cardinality_error_1_unscaled: 291.0000 (289.7944)  loss_ce_2_unscaled: 0.7918 (0.7937)  loss_bbox_2_unscaled: 0.0879 (0.1042)  loss_giou_2_unscaled: 0.4239 (0.4529)  cardinality_error_2_unscaled: 289.8333 (288.3837)  loss_ce_3_unscaled: 0.7453 (0.7645)  loss_bbox_3_unscaled: 0.0886 (0.1042)  loss_giou_3_unscaled: 0.4243 (0.4505)  cardinality_error_3_unscaled: 291.4167 (289.7864)  loss_ce_4_unscaled: 0.7635 (0.7695)  loss_bbox_4_unscaled: 0.0872 (0.1039)  loss_giou_4_unscaled: 0.4239 (0.4504)  cardinality_error_4_unscaled: 291.0833 (290.1739)  time: 2.9651  data: 0.0314  max mem: 8138\n",
            "Test:  [330/417]  eta: 0:04:21  class_error: 69.14  loss: 17.3326 (18.1036)  loss_ce: 1.3917 (1.5337)  loss_bbox: 0.5370 (0.5226)  loss_giou: 0.8677 (0.9009)  loss_ce_0: 1.4625 (1.6228)  loss_bbox_0: 0.5907 (0.5681)  loss_giou_0: 0.9853 (0.9941)  loss_ce_1: 1.4488 (1.5890)  loss_bbox_1: 0.5446 (0.5293)  loss_giou_1: 0.8877 (0.9165)  loss_ce_2: 1.4168 (1.5865)  loss_bbox_2: 0.5395 (0.5234)  loss_giou_2: 0.8744 (0.9054)  loss_ce_3: 1.3879 (1.5277)  loss_bbox_3: 0.5386 (0.5234)  loss_giou_3: 0.8654 (0.9005)  loss_ce_4: 1.3890 (1.5377)  loss_bbox_4: 0.5367 (0.5219)  loss_giou_4: 0.8654 (0.9003)  loss_ce_unscaled: 0.6958 (0.7669)  class_error_unscaled: 46.4912 (54.0388)  loss_bbox_unscaled: 0.1074 (0.1045)  loss_giou_unscaled: 0.4339 (0.4504)  cardinality_error_unscaled: 290.5000 (290.1191)  loss_ce_0_unscaled: 0.7312 (0.8114)  loss_bbox_0_unscaled: 0.1181 (0.1136)  loss_giou_0_unscaled: 0.4927 (0.4971)  cardinality_error_0_unscaled: 290.1667 (289.7963)  loss_ce_1_unscaled: 0.7244 (0.7945)  loss_bbox_1_unscaled: 0.1089 (0.1059)  loss_giou_1_unscaled: 0.4439 (0.4583)  cardinality_error_1_unscaled: 289.8333 (289.8457)  loss_ce_2_unscaled: 0.7084 (0.7932)  loss_bbox_2_unscaled: 0.1079 (0.1047)  loss_giou_2_unscaled: 0.4372 (0.4527)  cardinality_error_2_unscaled: 289.6667 (288.4597)  loss_ce_3_unscaled: 0.6940 (0.7638)  loss_bbox_3_unscaled: 0.1077 (0.1047)  loss_giou_3_unscaled: 0.4327 (0.4503)  cardinality_error_3_unscaled: 290.4167 (289.8417)  loss_ce_4_unscaled: 0.6945 (0.7688)  loss_bbox_4_unscaled: 0.1073 (0.1044)  loss_giou_4_unscaled: 0.4327 (0.4501)  cardinality_error_4_unscaled: 290.9167 (290.2352)  time: 2.8417  data: 0.0295  max mem: 8138\n",
            "Test:  [340/417]  eta: 0:03:50  class_error: 35.78  loss: 16.8152 (18.0670)  loss_ce: 1.3219 (1.5291)  loss_bbox: 0.5411 (0.5227)  loss_giou: 0.8677 (0.8994)  loss_ce_0: 1.4100 (1.6180)  loss_bbox_0: 0.5904 (0.5680)  loss_giou_0: 0.9853 (0.9930)  loss_ce_1: 1.3883 (1.5841)  loss_bbox_1: 0.5517 (0.5292)  loss_giou_1: 0.8975 (0.9152)  loss_ce_2: 1.3899 (1.5818)  loss_bbox_2: 0.5395 (0.5232)  loss_giou_2: 0.8744 (0.9038)  loss_ce_3: 1.3249 (1.5233)  loss_bbox_3: 0.5373 (0.5233)  loss_giou_3: 0.8654 (0.8990)  loss_ce_4: 1.3272 (1.5331)  loss_bbox_4: 0.5353 (0.5218)  loss_giou_4: 0.8654 (0.8988)  loss_ce_unscaled: 0.6609 (0.7645)  class_error_unscaled: 43.9024 (53.7859)  loss_bbox_unscaled: 0.1082 (0.1045)  loss_giou_unscaled: 0.4339 (0.4497)  cardinality_error_unscaled: 290.1667 (290.0780)  loss_ce_0_unscaled: 0.7050 (0.8090)  loss_bbox_0_unscaled: 0.1181 (0.1136)  loss_giou_0_unscaled: 0.4927 (0.4965)  cardinality_error_0_unscaled: 289.6667 (289.7688)  loss_ce_1_unscaled: 0.6941 (0.7920)  loss_bbox_1_unscaled: 0.1103 (0.1058)  loss_giou_1_unscaled: 0.4487 (0.4576)  cardinality_error_1_unscaled: 290.0833 (289.8104)  loss_ce_2_unscaled: 0.6949 (0.7909)  loss_bbox_2_unscaled: 0.1079 (0.1046)  loss_giou_2_unscaled: 0.4372 (0.4519)  cardinality_error_2_unscaled: 288.1667 (288.4394)  loss_ce_3_unscaled: 0.6624 (0.7617)  loss_bbox_3_unscaled: 0.1075 (0.1047)  loss_giou_3_unscaled: 0.4327 (0.4495)  cardinality_error_3_unscaled: 289.8333 (289.8236)  loss_ce_4_unscaled: 0.6636 (0.7665)  loss_bbox_4_unscaled: 0.1071 (0.1044)  loss_giou_4_unscaled: 0.4327 (0.4494)  cardinality_error_4_unscaled: 290.1667 (290.2036)  time: 2.8242  data: 0.0286  max mem: 8138\n",
            "Test:  [350/417]  eta: 0:03:20  class_error: 38.60  loss: 16.1010 (18.0268)  loss_ce: 1.3025 (1.5237)  loss_bbox: 0.5013 (0.5222)  loss_giou: 0.8649 (0.8988)  loss_ce_0: 1.3539 (1.6120)  loss_bbox_0: 0.5291 (0.5674)  loss_giou_0: 0.9198 (0.9926)  loss_ce_1: 1.3217 (1.5782)  loss_bbox_1: 0.5014 (0.5288)  loss_giou_1: 0.8975 (0.9146)  loss_ce_2: 1.3385 (1.5762)  loss_bbox_2: 0.4947 (0.5227)  loss_giou_2: 0.8648 (0.9031)  loss_ce_3: 1.3145 (1.5181)  loss_bbox_3: 0.4943 (0.5229)  loss_giou_3: 0.8557 (0.8983)  loss_ce_4: 1.3106 (1.5277)  loss_bbox_4: 0.5022 (0.5213)  loss_giou_4: 0.8626 (0.8981)  loss_ce_unscaled: 0.6512 (0.7619)  class_error_unscaled: 43.3121 (53.5569)  loss_bbox_unscaled: 0.1003 (0.1044)  loss_giou_unscaled: 0.4325 (0.4494)  cardinality_error_unscaled: 290.5833 (290.1000)  loss_ce_0_unscaled: 0.6769 (0.8060)  loss_bbox_0_unscaled: 0.1058 (0.1135)  loss_giou_0_unscaled: 0.4599 (0.4963)  cardinality_error_0_unscaled: 289.6667 (289.8068)  loss_ce_1_unscaled: 0.6608 (0.7891)  loss_bbox_1_unscaled: 0.1003 (0.1058)  loss_giou_1_unscaled: 0.4487 (0.4573)  cardinality_error_1_unscaled: 290.0833 (289.8331)  loss_ce_2_unscaled: 0.6692 (0.7881)  loss_bbox_2_unscaled: 0.0989 (0.1045)  loss_giou_2_unscaled: 0.4324 (0.4516)  cardinality_error_2_unscaled: 288.7500 (288.4739)  loss_ce_3_unscaled: 0.6572 (0.7590)  loss_bbox_3_unscaled: 0.0989 (0.1046)  loss_giou_3_unscaled: 0.4279 (0.4491)  cardinality_error_3_unscaled: 290.5000 (289.8493)  loss_ce_4_unscaled: 0.6553 (0.7638)  loss_bbox_4_unscaled: 0.1004 (0.1043)  loss_giou_4_unscaled: 0.4313 (0.4491)  cardinality_error_4_unscaled: 290.5000 (290.2362)  time: 2.9053  data: 0.0292  max mem: 8138\n",
            "Test:  [360/417]  eta: 0:02:50  class_error: 62.50  loss: 16.4587 (18.0081)  loss_ce: 1.3859 (1.5212)  loss_bbox: 0.4933 (0.5218)  loss_giou: 0.8779 (0.8986)  loss_ce_0: 1.4844 (1.6097)  loss_bbox_0: 0.5333 (0.5669)  loss_giou_0: 0.9482 (0.9925)  loss_ce_1: 1.4302 (1.5758)  loss_bbox_1: 0.4998 (0.5282)  loss_giou_1: 0.8993 (0.9144)  loss_ce_2: 1.4430 (1.5737)  loss_bbox_2: 0.4897 (0.5222)  loss_giou_2: 0.8698 (0.9030)  loss_ce_3: 1.3643 (1.5154)  loss_bbox_3: 0.4938 (0.5224)  loss_giou_3: 0.8766 (0.8981)  loss_ce_4: 1.3979 (1.5254)  loss_bbox_4: 0.4903 (0.5209)  loss_giou_4: 0.8795 (0.8978)  loss_ce_unscaled: 0.6929 (0.7606)  class_error_unscaled: 47.8632 (53.5321)  loss_bbox_unscaled: 0.0987 (0.1044)  loss_giou_unscaled: 0.4389 (0.4493)  cardinality_error_unscaled: 289.9167 (290.0508)  loss_ce_0_unscaled: 0.7422 (0.8048)  loss_bbox_0_unscaled: 0.1067 (0.1134)  loss_giou_0_unscaled: 0.4741 (0.4963)  cardinality_error_0_unscaled: 290.4167 (289.7809)  loss_ce_1_unscaled: 0.7151 (0.7879)  loss_bbox_1_unscaled: 0.1000 (0.1056)  loss_giou_1_unscaled: 0.4497 (0.4572)  cardinality_error_1_unscaled: 289.1667 (289.7876)  loss_ce_2_unscaled: 0.7215 (0.7869)  loss_bbox_2_unscaled: 0.0979 (0.1044)  loss_giou_2_unscaled: 0.4349 (0.4515)  cardinality_error_2_unscaled: 288.7500 (288.4248)  loss_ce_3_unscaled: 0.6822 (0.7577)  loss_bbox_3_unscaled: 0.0988 (0.1045)  loss_giou_3_unscaled: 0.4383 (0.4490)  cardinality_error_3_unscaled: 289.9167 (289.8209)  loss_ce_4_unscaled: 0.6989 (0.7627)  loss_bbox_4_unscaled: 0.0981 (0.1042)  loss_giou_4_unscaled: 0.4397 (0.4489)  cardinality_error_4_unscaled: 290.5833 (290.2064)  time: 3.0169  data: 0.0315  max mem: 8138\n",
            "Test:  [370/417]  eta: 0:02:20  class_error: 52.78  loss: 18.2926 (18.0478)  loss_ce: 1.5071 (1.5254)  loss_bbox: 0.5221 (0.5233)  loss_giou: 0.9224 (0.8994)  loss_ce_0: 1.5980 (1.6133)  loss_bbox_0: 0.5847 (0.5689)  loss_giou_0: 1.0120 (0.9937)  loss_ce_1: 1.5932 (1.5804)  loss_bbox_1: 0.5265 (0.5298)  loss_giou_1: 0.9345 (0.9152)  loss_ce_2: 1.5821 (1.5784)  loss_bbox_2: 0.5327 (0.5237)  loss_giou_2: 0.9283 (0.9037)  loss_ce_3: 1.5331 (1.5196)  loss_bbox_3: 0.5190 (0.5238)  loss_giou_3: 0.9134 (0.8989)  loss_ce_4: 1.5282 (1.5296)  loss_bbox_4: 0.5203 (0.5223)  loss_giou_4: 0.9156 (0.8986)  loss_ce_unscaled: 0.7535 (0.7627)  class_error_unscaled: 55.7692 (53.6490)  loss_bbox_unscaled: 0.1044 (0.1047)  loss_giou_unscaled: 0.4612 (0.4497)  cardinality_error_unscaled: 289.8333 (290.0400)  loss_ce_0_unscaled: 0.7990 (0.8066)  loss_bbox_0_unscaled: 0.1169 (0.1138)  loss_giou_0_unscaled: 0.5060 (0.4968)  cardinality_error_0_unscaled: 289.5833 (289.7545)  loss_ce_1_unscaled: 0.7966 (0.7902)  loss_bbox_1_unscaled: 0.1053 (0.1060)  loss_giou_1_unscaled: 0.4672 (0.4576)  cardinality_error_1_unscaled: 289.0000 (289.7763)  loss_ce_2_unscaled: 0.7911 (0.7892)  loss_bbox_2_unscaled: 0.1065 (0.1047)  loss_giou_2_unscaled: 0.4641 (0.4518)  cardinality_error_2_unscaled: 288.4167 (288.4194)  loss_ce_3_unscaled: 0.7665 (0.7598)  loss_bbox_3_unscaled: 0.1038 (0.1048)  loss_giou_3_unscaled: 0.4567 (0.4494)  cardinality_error_3_unscaled: 289.9167 (289.8102)  loss_ce_4_unscaled: 0.7641 (0.7648)  loss_bbox_4_unscaled: 0.1041 (0.1045)  loss_giou_4_unscaled: 0.4578 (0.4493)  cardinality_error_4_unscaled: 289.7500 (290.1988)  time: 3.0076  data: 0.0323  max mem: 8138\n",
            "Test:  [380/417]  eta: 0:01:50  class_error: 30.61  loss: 18.6433 (18.0462)  loss_ce: 1.5525 (1.5250)  loss_bbox: 0.4936 (0.5229)  loss_giou: 0.9224 (0.9001)  loss_ce_0: 1.6092 (1.6126)  loss_bbox_0: 0.5851 (0.5682)  loss_giou_0: 1.0147 (0.9942)  loss_ce_1: 1.6270 (1.5804)  loss_bbox_1: 0.5265 (0.5293)  loss_giou_1: 0.9399 (0.9157)  loss_ce_2: 1.6064 (1.5779)  loss_bbox_2: 0.5037 (0.5232)  loss_giou_2: 0.9283 (0.9043)  loss_ce_3: 1.5470 (1.5192)  loss_bbox_3: 0.4973 (0.5234)  loss_giou_3: 0.9179 (0.8995)  loss_ce_4: 1.5440 (1.5293)  loss_bbox_4: 0.4929 (0.5218)  loss_giou_4: 0.9203 (0.8993)  loss_ce_unscaled: 0.7762 (0.7625)  class_error_unscaled: 55.5556 (53.6087)  loss_bbox_unscaled: 0.0987 (0.1046)  loss_giou_unscaled: 0.4612 (0.4500)  cardinality_error_unscaled: 290.6667 (290.0623)  loss_ce_0_unscaled: 0.8046 (0.8063)  loss_bbox_0_unscaled: 0.1170 (0.1136)  loss_giou_0_unscaled: 0.5073 (0.4971)  cardinality_error_0_unscaled: 290.5000 (289.7905)  loss_ce_1_unscaled: 0.8135 (0.7902)  loss_bbox_1_unscaled: 0.1053 (0.1059)  loss_giou_1_unscaled: 0.4699 (0.4579)  cardinality_error_1_unscaled: 291.3333 (289.8036)  loss_ce_2_unscaled: 0.8032 (0.7890)  loss_bbox_2_unscaled: 0.1007 (0.1046)  loss_giou_2_unscaled: 0.4641 (0.4521)  cardinality_error_2_unscaled: 289.8333 (288.4554)  loss_ce_3_unscaled: 0.7735 (0.7596)  loss_bbox_3_unscaled: 0.0995 (0.1047)  loss_giou_3_unscaled: 0.4590 (0.4498)  cardinality_error_3_unscaled: 290.5833 (289.8417)  loss_ce_4_unscaled: 0.7720 (0.7646)  loss_bbox_4_unscaled: 0.0986 (0.1044)  loss_giou_4_unscaled: 0.4601 (0.4496)  cardinality_error_4_unscaled: 291.8333 (290.2292)  time: 3.0076  data: 0.0311  max mem: 8138\n",
            "Test:  [390/417]  eta: 0:01:20  class_error: 49.43  loss: 16.7399 (18.0264)  loss_ce: 1.4837 (1.5236)  loss_bbox: 0.4801 (0.5214)  loss_giou: 0.8750 (0.8996)  loss_ce_0: 1.5399 (1.6111)  loss_bbox_0: 0.5095 (0.5669)  loss_giou_0: 0.9647 (0.9942)  loss_ce_1: 1.5335 (1.5784)  loss_bbox_1: 0.4621 (0.5280)  loss_giou_1: 0.8880 (0.9156)  loss_ce_2: 1.5034 (1.5761)  loss_bbox_2: 0.4650 (0.5218)  loss_giou_2: 0.8766 (0.9038)  loss_ce_3: 1.4706 (1.5178)  loss_bbox_3: 0.4744 (0.5220)  loss_giou_3: 0.8706 (0.8991)  loss_ce_4: 1.4976 (1.5279)  loss_bbox_4: 0.4686 (0.5203)  loss_giou_4: 0.8753 (0.8987)  loss_ce_unscaled: 0.7418 (0.7618)  class_error_unscaled: 50.7246 (53.5236)  loss_bbox_unscaled: 0.0960 (0.1043)  loss_giou_unscaled: 0.4375 (0.4498)  cardinality_error_unscaled: 291.5833 (290.0902)  loss_ce_0_unscaled: 0.7699 (0.8055)  loss_bbox_0_unscaled: 0.1019 (0.1134)  loss_giou_0_unscaled: 0.4824 (0.4971)  cardinality_error_0_unscaled: 290.5833 (289.8105)  loss_ce_1_unscaled: 0.7667 (0.7892)  loss_bbox_1_unscaled: 0.0924 (0.1056)  loss_giou_1_unscaled: 0.4440 (0.4578)  cardinality_error_1_unscaled: 291.5000 (289.8216)  loss_ce_2_unscaled: 0.7517 (0.7881)  loss_bbox_2_unscaled: 0.0930 (0.1044)  loss_giou_2_unscaled: 0.4383 (0.4519)  cardinality_error_2_unscaled: 290.8333 (288.4887)  loss_ce_3_unscaled: 0.7353 (0.7589)  loss_bbox_3_unscaled: 0.0949 (0.1044)  loss_giou_3_unscaled: 0.4353 (0.4495)  cardinality_error_3_unscaled: 292.1667 (289.8651)  loss_ce_4_unscaled: 0.7488 (0.7639)  loss_bbox_4_unscaled: 0.0937 (0.1041)  loss_giou_4_unscaled: 0.4377 (0.4494)  cardinality_error_4_unscaled: 291.8333 (290.2528)  time: 2.9685  data: 0.0299  max mem: 8138\n",
            "Test:  [400/417]  eta: 0:00:50  class_error: 55.21  loss: 16.7399 (18.0198)  loss_ce: 1.5073 (1.5236)  loss_bbox: 0.4654 (0.5211)  loss_giou: 0.8750 (0.8989)  loss_ce_0: 1.5399 (1.6108)  loss_bbox_0: 0.5127 (0.5667)  loss_giou_0: 0.9647 (0.9937)  loss_ce_1: 1.5194 (1.5781)  loss_bbox_1: 0.4894 (0.5278)  loss_giou_1: 0.8879 (0.9150)  loss_ce_2: 1.5560 (1.5758)  loss_bbox_2: 0.4623 (0.5215)  loss_giou_2: 0.8781 (0.9031)  loss_ce_3: 1.5151 (1.5179)  loss_bbox_3: 0.4662 (0.5216)  loss_giou_3: 0.8706 (0.8984)  loss_ce_4: 1.4958 (1.5277)  loss_bbox_4: 0.4584 (0.5200)  loss_giou_4: 0.8753 (0.8981)  loss_ce_unscaled: 0.7536 (0.7618)  class_error_unscaled: 51.0204 (53.5176)  loss_bbox_unscaled: 0.0931 (0.1042)  loss_giou_unscaled: 0.4375 (0.4494)  cardinality_error_unscaled: 290.7500 (290.1023)  loss_ce_0_unscaled: 0.7699 (0.8054)  loss_bbox_0_unscaled: 0.1025 (0.1133)  loss_giou_0_unscaled: 0.4824 (0.4969)  cardinality_error_0_unscaled: 290.4167 (289.8198)  loss_ce_1_unscaled: 0.7597 (0.7890)  loss_bbox_1_unscaled: 0.0979 (0.1056)  loss_giou_1_unscaled: 0.4439 (0.4575)  cardinality_error_1_unscaled: 290.1667 (289.8238)  loss_ce_2_unscaled: 0.7780 (0.7879)  loss_bbox_2_unscaled: 0.0925 (0.1043)  loss_giou_2_unscaled: 0.4390 (0.4516)  cardinality_error_2_unscaled: 289.2500 (288.4747)  loss_ce_3_unscaled: 0.7575 (0.7590)  loss_bbox_3_unscaled: 0.0932 (0.1043)  loss_giou_3_unscaled: 0.4353 (0.4492)  cardinality_error_3_unscaled: 290.5833 (289.8651)  loss_ce_4_unscaled: 0.7479 (0.7639)  loss_bbox_4_unscaled: 0.0917 (0.1040)  loss_giou_4_unscaled: 0.4377 (0.4490)  cardinality_error_4_unscaled: 290.6667 (290.2531)  time: 2.8639  data: 0.0300  max mem: 8138\n",
            "Test:  [410/417]  eta: 0:00:20  class_error: 50.83  loss: 18.2719 (18.0519)  loss_ce: 1.5572 (1.5262)  loss_bbox: 0.5035 (0.5214)  loss_giou: 0.9351 (0.9016)  loss_ce_0: 1.6496 (1.6124)  loss_bbox_0: 0.5606 (0.5672)  loss_giou_0: 1.0303 (0.9961)  loss_ce_1: 1.5723 (1.5803)  loss_bbox_1: 0.5219 (0.5284)  loss_giou_1: 0.9545 (0.9176)  loss_ce_2: 1.6031 (1.5782)  loss_bbox_2: 0.5101 (0.5219)  loss_giou_2: 0.9364 (0.9058)  loss_ce_3: 1.5284 (1.5203)  loss_bbox_3: 0.5020 (0.5220)  loss_giou_3: 0.9155 (0.9010)  loss_ce_4: 1.5593 (1.5304)  loss_bbox_4: 0.5018 (0.5203)  loss_giou_4: 0.9194 (0.9008)  loss_ce_unscaled: 0.7786 (0.7631)  class_error_unscaled: 52.0548 (53.6019)  loss_bbox_unscaled: 0.1007 (0.1043)  loss_giou_unscaled: 0.4676 (0.4508)  cardinality_error_unscaled: 290.7500 (290.1162)  loss_ce_0_unscaled: 0.8248 (0.8062)  loss_bbox_0_unscaled: 0.1121 (0.1134)  loss_giou_0_unscaled: 0.5152 (0.4981)  cardinality_error_0_unscaled: 290.4167 (289.8313)  loss_ce_1_unscaled: 0.7862 (0.7901)  loss_bbox_1_unscaled: 0.1044 (0.1057)  loss_giou_1_unscaled: 0.4772 (0.4588)  cardinality_error_1_unscaled: 290.1667 (289.8295)  loss_ce_2_unscaled: 0.8016 (0.7891)  loss_bbox_2_unscaled: 0.1020 (0.1044)  loss_giou_2_unscaled: 0.4682 (0.4529)  cardinality_error_2_unscaled: 289.2500 (288.4854)  loss_ce_3_unscaled: 0.7642 (0.7602)  loss_bbox_3_unscaled: 0.1004 (0.1044)  loss_giou_3_unscaled: 0.4577 (0.4505)  cardinality_error_3_unscaled: 290.0833 (289.8613)  loss_ce_4_unscaled: 0.7797 (0.7652)  loss_bbox_4_unscaled: 0.1004 (0.1041)  loss_giou_4_unscaled: 0.4597 (0.4504)  cardinality_error_4_unscaled: 290.4167 (290.2531)  time: 2.8598  data: 0.0308  max mem: 8138\n",
            "Test:  [416/417]  eta: 0:00:02  class_error: 36.59  loss: 17.8883 (18.0211)  loss_ce: 1.4936 (1.5238)  loss_bbox: 0.4875 (0.5203)  loss_giou: 0.9137 (0.8999)  loss_ce_0: 1.5606 (1.6103)  loss_bbox_0: 0.5430 (0.5663)  loss_giou_0: 0.9972 (0.9944)  loss_ce_1: 1.5652 (1.5781)  loss_bbox_1: 0.5113 (0.5272)  loss_giou_1: 0.9113 (0.9158)  loss_ce_2: 1.5364 (1.5760)  loss_bbox_2: 0.4918 (0.5207)  loss_giou_2: 0.9222 (0.9040)  loss_ce_3: 1.4837 (1.5179)  loss_bbox_3: 0.4885 (0.5209)  loss_giou_3: 0.9155 (0.8993)  loss_ce_4: 1.5186 (1.5280)  loss_bbox_4: 0.4844 (0.5192)  loss_giou_4: 0.9137 (0.8990)  loss_ce_unscaled: 0.7468 (0.7619)  class_error_unscaled: 50.8333 (53.5564)  loss_bbox_unscaled: 0.0975 (0.1041)  loss_giou_unscaled: 0.4569 (0.4500)  cardinality_error_unscaled: 291.3333 (290.1401)  loss_ce_0_unscaled: 0.7803 (0.8051)  loss_bbox_0_unscaled: 0.1086 (0.1133)  loss_giou_0_unscaled: 0.4986 (0.4972)  cardinality_error_0_unscaled: 290.7500 (289.8644)  loss_ce_1_unscaled: 0.7826 (0.7890)  loss_bbox_1_unscaled: 0.1023 (0.1054)  loss_giou_1_unscaled: 0.4556 (0.4579)  cardinality_error_1_unscaled: 290.6667 (289.8573)  loss_ce_2_unscaled: 0.7682 (0.7880)  loss_bbox_2_unscaled: 0.0984 (0.1041)  loss_giou_2_unscaled: 0.4611 (0.4520)  cardinality_error_2_unscaled: 289.4167 (288.5114)  loss_ce_3_unscaled: 0.7419 (0.7590)  loss_bbox_3_unscaled: 0.0977 (0.1042)  loss_giou_3_unscaled: 0.4577 (0.4496)  cardinality_error_3_unscaled: 291.0833 (289.8945)  loss_ce_4_unscaled: 0.7593 (0.7640)  loss_bbox_4_unscaled: 0.0969 (0.1038)  loss_giou_4_unscaled: 0.4569 (0.4495)  cardinality_error_4_unscaled: 290.9167 (290.2812)  time: 2.8512  data: 0.0304  max mem: 8138\n",
            "Test: Total time: 0:20:45 (2.9867 s / it)\n",
            "Averaged stats: class_error: 36.59  loss: 17.8883 (18.0211)  loss_ce: 1.4936 (1.5238)  loss_bbox: 0.4875 (0.5203)  loss_giou: 0.9137 (0.8999)  loss_ce_0: 1.5606 (1.6103)  loss_bbox_0: 0.5430 (0.5663)  loss_giou_0: 0.9972 (0.9944)  loss_ce_1: 1.5652 (1.5781)  loss_bbox_1: 0.5113 (0.5272)  loss_giou_1: 0.9113 (0.9158)  loss_ce_2: 1.5364 (1.5760)  loss_bbox_2: 0.4918 (0.5207)  loss_giou_2: 0.9222 (0.9040)  loss_ce_3: 1.4837 (1.5179)  loss_bbox_3: 0.4885 (0.5209)  loss_giou_3: 0.9155 (0.8993)  loss_ce_4: 1.5186 (1.5280)  loss_bbox_4: 0.4844 (0.5192)  loss_giou_4: 0.9137 (0.8990)  loss_ce_unscaled: 0.7468 (0.7619)  class_error_unscaled: 50.8333 (53.5564)  loss_bbox_unscaled: 0.0975 (0.1041)  loss_giou_unscaled: 0.4569 (0.4500)  cardinality_error_unscaled: 291.3333 (290.1401)  loss_ce_0_unscaled: 0.7803 (0.8051)  loss_bbox_0_unscaled: 0.1086 (0.1133)  loss_giou_0_unscaled: 0.4986 (0.4972)  cardinality_error_0_unscaled: 290.7500 (289.8644)  loss_ce_1_unscaled: 0.7826 (0.7890)  loss_bbox_1_unscaled: 0.1023 (0.1054)  loss_giou_1_unscaled: 0.4556 (0.4579)  cardinality_error_1_unscaled: 290.6667 (289.8573)  loss_ce_2_unscaled: 0.7682 (0.7880)  loss_bbox_2_unscaled: 0.0984 (0.1041)  loss_giou_2_unscaled: 0.4611 (0.4520)  cardinality_error_2_unscaled: 289.4167 (288.5114)  loss_ce_3_unscaled: 0.7419 (0.7590)  loss_bbox_3_unscaled: 0.0977 (0.1042)  loss_giou_3_unscaled: 0.4577 (0.4496)  cardinality_error_3_unscaled: 291.0833 (289.8945)  loss_ce_4_unscaled: 0.7593 (0.7640)  loss_bbox_4_unscaled: 0.0969 (0.1038)  loss_giou_4_unscaled: 0.4569 (0.4495)  cardinality_error_4_unscaled: 290.9167 (290.2812)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=13.35s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.265\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.388\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.286\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.143\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.284\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.379\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.255\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.414\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.266\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.463\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Insights and discussions"
      ],
      "metadata": {
        "id": "F8Waqb404hwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Contribution\n",
        "- The project pioneers a replay scheme in class-incremental learning, specifically tailored for object detection, filling a significant gap in current research.\n",
        "\n",
        "- Our project arranges the replay buffer based on training loss, enhancing the efficiency of knowledge retrieval and potentially inspiring future machine learning developments.\n",
        "\n",
        "- The proposed circular training strategy addresses the data quantity asymmetry between new samples and the replay buffer, offering a solution to class imbalance.\n",
        "\n",
        "### Limitation\n",
        "- More Detailed experiement can be performed.\n",
        "  - For example, Task1-70, Task2-10\n",
        "\n",
        "\n",
        "### Future Research\n",
        "- Implement this approach in *open-world settings*\n",
        "  - Not divided datset, Add some samples subsequently.\n",
        "  - can give a substantial impact in areas like autonomous driving and robotics.\n"
      ],
      "metadata": {
        "id": "V1FlRckm47rh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference"
      ],
      "metadata": {
        "id": "RZrzBWDX49Xv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- https://www.researchgate.net/figure/Distribution-of-the-numbers-of-individual-categories-in-the-MS-COCO-dataset-The-dataset_fig1_368788377\n",
        "- https://www.researchgate.net/figure/Sample-images-from-the-COCO-dataset_fig3_344601010\n",
        "- https://github.com/fcdl94/MMA\n",
        "- Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. iCaRL: Incremental classifier and representation learning. In CVPR, 2017\n",
        "- Jeng-Lun Shieh, Qazi Mazhar ul Haq, Muhamad Amirul Haq, Said Karam, Peter Chon dro, De-Qin Gao, and Shanq-Jang Ruan. Continual learning strategy in one-stage obj ect detection framework based on experience replay for autonomous driving vehicle. Sensors, 2020\n",
        "- Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni, and Lei Zhang. Dn-detr: Acc elerate detr training by introducing query denoising. In CVPR, 2022\n",
        "- Konstantin Shmelkov, Cordelia Schmid, and Karteek Alahari. Incremental learning of o bject detectors without catastrophic forgetting. In ICCV, 2017\n",
        "- Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. ICLR, 2020\n",
        "- Manoj Acharya, Tyler L Hayes, and Christopher Kanan. Rodeo: Replay for online obje ct detection. BMVC, 2020\n",
        "- Xialei Liu, Hao Yang, Avinash Ravichandran, Rahul Bhotika, and Stefano Soatto. Multi- task incremental learning for object detection. arXiv, 2022\n"
      ],
      "metadata": {
        "id": "PkGseWt94-9v"
      }
    }
  ]
}